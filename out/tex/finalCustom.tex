%
\documentclass[paper=A4,twoside=true,openright,parskip=full,chapterprefix=true,headings=normal,bibliography=totoc,listof=totoc,titlepage=on,captions=tableabove,draft=false,british]{scrreprt}%

% **************************************************
% Debug LaTeX Information
% **************************************************
%\listfiles

% **************************************************
% Information and Commands for Reuse
% **************************************************

\input{src/res/format.tex}

\newcommand{\thesisTitle}{Concepts in Parallel Programming}
\newcommand{\thesisTitleGerman}{Konzepte in Paralleler Programmierung}
\newcommand{\thesisSubtitle}{Arrows for Parallel Computation}
\newcommand{\thesisSubtitleGerman}{}
\newcommand{\thesisName}{Martin Braun (1249080)}
\newcommand{\thesisSubject}{Master Thesis}
\newcommand{\thesisDate}{07.09.2018}
\newcommand{\thesisUniversity}{\protect{University of Bayreuth}}
\newcommand{\thesisUniversityDepartment}{AI5 -- Visual Computing}
\newcommand{\thesisUniversityDepartmentGerman}{AI5 - Graphische Datenverarbeitung}
\newcommand{\thesisUniversityCity}{Bayreuth}

% **************************************************
% Load and Configure Packages
% **************************************************
\usepackage[section]{placeins}
\makeatletter
\AtBeginDocument{%
  \expandafter\renewcommand\expandafter\subsection\expandafter{%
    \expandafter\@fb@secFB\subsection
  }%
}
\makeatother
\usepackage[utf8]{inputenc}		% defines file's character encoding
\usepackage[english]{babel}     % babel system, adjust the language of the content
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{listings}

\usepackage[					% clean thesis style
	figuresep=colon,%
	sansserif=false,%
	hangfigurecaption=false,%
	hangsection=true,%
	hangsubsection=true,%
	colorize=bw,%
	colortheme=bluemagenta,%
	bibfile=src/res/references.bib,%
]{cleanthesis}


\makeatletter
\DeclareOldFontCommand{\rm}{\normalfont\rmfamily}{\mathrm}
\DeclareOldFontCommand{\sf}{\normalfont\sffamily}{\mathsf}
\DeclareOldFontCommand{\tt}{\normalfont\ttfamily}{\mathtt}
\DeclareOldFontCommand{\bf}{\normalfont\bfseries}{\mathbf}
\DeclareOldFontCommand{\it}{\normalfont\itshape}{\mathit}
\DeclareOldFontCommand{\sl}{\normalfont\slshape}{\@nomath\sl}
\DeclareOldFontCommand{\sc}{\normalfont\scshape}{\@nomath\sc}
\makeatother

\renewcommand{\cfttoctitlefont}{\thesischapterfont}
\renewcommand{\cftlottitlefont}{\thesischapterfont}
\renewcommand{\cftloftitlefont}{\thesischapterfont}
\renewcommand{\cftchapfont}{\rmfamily\bfseries}
\renewcommand{\cftsecfont}{\rmfamily}
\renewcommand{\cftsubsecfont}{\rmfamily}
\renewcommand{\cfttabfont}{\rmfamily}
\renewcommand{\cftfigfont}{\rmfamily}

\providecommand{\tightlist}{%
    \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\hypersetup{					% setup the hyperref-package options
	pdftitle={\thesisTitle},	% 	- title (PDF meta)
	pdfsubject={\thesisSubject},% 	- subject (PDF meta)
	pdfauthor={\thesisName},	% 	- author (PDF meta)
	plainpages=false,			% 	-
	colorlinks=false,			% 	- colorize links?
	pdfborder={0 0 0},			% 	-
	breaklinks=true,			% 	- allow line break inside links
	bookmarksnumbered=true,		%
	bookmarksopen=true			%
}

%fuck it. quotations did not work properly. do it manually now.
\renewcommand{\enquote}[1]{{``}#1{''}}

\usepackage{float}

% BEGIN
\usepackage{xparse}
\let\oldquote\quote
\let\endoldquote\endquote

\RenewDocumentEnvironment{quote}{}{%
    \oldquote
    \itshape
    \setlength{\rightmargin}{\leftmargin}
    {\Large\textbf{``}}
	{\hspace{-0.7em}}
}
{%
	{\hspace{-0.7em}}
	{\Large\textbf{''}}
    \endoldquote
}
% END

% BEGIN
% fix for Figures below footnotes
\usepackage[bottom]{footmisc}

\makeatletter
\renewcommand{\fps@figure}{tbp}
\renewcommand{\fps@table}{tbp}
\makeatother

\toks0{\ifvoid\footins\else\suppressfloats[b]\fi}
\output\expandafter{\the\toks0\the\output}
% END

% **************************************************
% Document CONTENT
% **************************************************
\begin{document}


% --------------------------
% rename document parts
% --------------------------
%\renewcaptionname{english}{\figurename}{Fig.}
%\renewcaptionname{english}{\tablename}{Tab.}

% --------------------------
% Front matter
% --------------------------
\pagenumbering{roman}			% roman page numbing (invisible for empty page style)
\pagestyle{empty}				% no header or footers

\begin{titlepage}
\pdfbookmark[0]{Cover}{Cover}
\flushright
\hfill
\vfill
{\huge\thesisTitle \par}
{\LARGE\thesisSubtitle \par}
\rule[5pt]{\textwidth}{.4pt} \par
{\Large\thesisName} \par
{First supervisor: \Large Dr.~Oleg Lobachev} \par
{Second supervisor: \Large Prof.~Dr.~Thomas Rauber} \par

\vfill
\vspace*{7cm}
%
\begin{minipage}{2cm}
\includegraphics[height=1cm]{src/img/ai5logo.pdf}
\end{minipage}
%
\begin{minipage}{5cm}
\vspace{16pt}
\thesisUniversityDepartment \\
\thesisUniversity \\
\end{minipage}
\hfill
\begin{minipage}{5cm}
\vspace*{18pt}
\begin{flushright}
\textit{\large\thesisDate}
\end{flushright}
\end{minipage}
%
\end{titlepage}
\cleardoublepage

\pagestyle{plain}
\begin{abstract}
Arrows are a general functional interface for computation and an
alternative to Monads for API design. In contrast to Monad-based
parallelism, we explore the use of Arrows for specifying generalised
parallelism. Specifically, we define an Arrow-based language and
implement it using multiple parallel Haskells. As each parallel
computation is an Arrow, such parallel Arrows (PArrows) can be readily
composed and transformed. To allow for more sophisticated communication
schemes between computation nodes in distributed systems, we utilise the
concept of Futures to wrap direct communication. Addressing the recent
trends in cloud computing, we also explore the possibility for a cloud
based implementation of this newly created DSL. To show that PArrows
have similar expressive power as existing parallel languages, we
implement several algorithmic skeletons and four benchmarks. The
Benchmarks show that our framework does not induce any notable
performance overhead. We conclude that Arrows have considerable
potential for composing parallel programs and more specifically for
programs that have to be executed with multiple different parallel
language implementations.
\end{abstract}
\cleardoublepage

\begin{titlepage}
\pdfbookmark[0]{CoverGerman}{CoverGerman}
\flushright
\hfill
\vfill
{\huge\thesisTitleGerman \par}
% do this here, lazy.
{\LARGE Arrows für Parallele Berechnungen \par}
\rule[5pt]{\textwidth}{.4pt} \par
{\Large\thesisName} \par
{1. Beurteilender: \Large Dr.~Oleg Lobachev} \par
{2. Beurteilender: \Large Prof.~Dr.~Thomas Rauber} \par

\vfill
\vspace*{7cm}
%
\begin{minipage}{2cm}
\includegraphics[height=1cm]{src/img/ai5logo.pdf}
\end{minipage}
%
\begin{minipage}{7cm}
\vspace{16pt}
\thesisUniversityDepartmentGerman \\
Universität Bayreuth \\
\end{minipage}
\hfill
\begin{minipage}{3cm}
\vspace*{18pt}
\begin{flushright}
\textit{\large\thesisDate}
\end{flushright}
\end{minipage}
%
\end{titlepage}
\cleardoublepage

\pagestyle{plain}
\begin{abstract}
Arrows sind eine allgemeine funktionale Schnittstelle für Berechnungen und eine Alternative zu Monaden zum Design von APIs.
Im Gegensatz zu auf Monaden basierender Parallelität wird hier die Verwendung von Arrows zur Spezifizierung von Parallelität untersucht.
Konkret wird eine Arrow-basierte Sprache definiert und mit mehreren parallelen Haskells implementiert.
Da jede parallele Berechnung ein Arrow ist, können solche parallele Arrows (PArrows) leicht zusammengesetzt und transformiert werden.
Damit komplexere Kommunikationsschemata zwischen Berechnungsknoten in verteilten Systemen einfacher handzuhaben sind, wird das Konzept der Futures verwendet,
um direkte Kommunikation zu abstrahieren.
Des Weiteren wird, um den aktuellen Trends im Cloud Computing Rechnung zu tragen, die Möglichkeit einer Cloud-basierten Implementierung dieser
neu geschaffenen DSL untersucht.
Um zu zeigen, dass PArrows eine ähnliche Ausdruckskraft wie bestehende parallele Sprachen haben,
werden mehrere algorithmische Skelette und vier Benchmarks implementiert.
Die Benchmarks zeigen, dass PArrows keinen nennenswerten Performance-Overhead verursachen.
Arrows haben ein beträchtliches Potenzial für das Schreiben paralleler Programme und speziell auch für solche,
welche auf mehreren unterschiedlichen parallelen Sprachimplementierungen ausgeführt werden sollen.
\end{abstract}
\cleardoublepage

\pagestyle{empty}
\begin{center}
\topskip0pt
\vspace*{\fill}
\textit{To my parents and Maren.}
\vspace*{\fill}
\end{center}

\cleardoublepage

\chapter*{Acknowledgements}
\thispagestyle{empty}

First of all, I would like to thank Oleg Lobachev for encouraging me
to work on this thesis in the first place.
Without his initial idea and guidance I would
not have been able to complete this work or the underlying paper,
which he also co-authored.

Secondly, I would like to thank the my other co-author
Phil Trinder for his contributions to the paper as well as providing me access to the
Glasgow Beowulf cluster.

Furthermore, I would like to thank Andreas Braun, Monika Braun, Julian Neuberger and
Maren Gruber for proof-reading and giving me their invaluable input.

Last but not least, I thank my family for their support and especially my parents
for helping me become the person I am today.

\cleardoublepage

{
\setcounter{tocdepth}{2}
\addtocontents{toc}{\protect\thispagestyle{empty}}
\pagenumbering{gobble}
\tableofcontents
}

\cleardoublepage
%

% --------------------------
% Body matter
% --------------------------
\pagenumbering{arabic}			% arabic page numbering
\setcounter{page}{1}			% set page counter
\pagestyle{maincontentstyle} 	% fancy header and footer

% pandoc-xnos: cleveref fakery
\newcommand{\plusnamesingular}{}
\newcommand{\starnamesingular}{}
\newcommand{\xrefname}[1]{\protect\renewcommand{\plusnamesingular}{#1}}
\newcommand{\Xrefname}[1]{\protect\renewcommand{\starnamesingular}{#1}}
\providecommand{\cref}{\plusnamesingular~\ref}
\providecommand{\Cref}{\starnamesingular~\ref}
\providecommand{\crefformat}[2]{}
\providecommand{\Crefformat}[2]{}

% pandoc-xnos: cleveref formatting
\crefformat{equation}{eq.~#2#1#3}
\Crefformat{equation}{Equation~#2#1#3}
\crefformat{figure}{fig.~#2#1#3}
\Crefformat{figure}{Figure~#2#1#3}

\hypertarget{introduction}{%
\chapter{Introduction}\label{introduction}}

\label{sec:introduction}

In recent years, functional programming has been on the rise as
functional languages like Haskell, Scala, Lisp or their derivatives have
seen an increase in popularity. Furthermore, imperative languages are
adopting features originally coming from functional languages. So, even
Java, which is generally not associated with introducing new features
quickly, has officially embraced at least some functional concepts such
as Lambdas and the functional interfaces in the standard library. Many
new concepts such as the streaming API rely heavily on these new-to-Java
ideas. Other languages such as C++, C\# or Python show an even greater
influence of functional paradigms as they all have been seen improving
their support for a functional style of programming, even though they
can not be considered pure functional languages by any means.

This rise in popularity does not come from nowhere. The core benefit of
functional programming, its modularity, allows programmers to write
concise programs in a clear and structured way.

Functional languages, coming from an academic environment, historically
also have a long history of being used for experimenting with novel
programming paradigms. Among these is the use of functional languages
for parallel programming.

In Haskell, which we focus on in this thesis, there already exist
several ways to write parallel programs. One approach that has not been
explored in depth yet, is to represent parallel computations with
Arrows. Nonetheless, they seem a natural fit since they can be thought
of as a more general function arrow (\ensuremath{\to }) and serve as a general
interface to computations while not being as restrictive as Monads
(Hughes, \protect\hyperlink{ref-HughesArrows}{2000}).

This is why in this thesis we will explain how a parallel Haskell based
on parallel Arrows (PArrows) can be achieved. We do not want to
re-invent parallelism, though, as we only provide an Arrow based type
class hosting an Arrow combinator \ensuremath{\Varid{parEvalN}\mathbin{::}[\mskip1.5mu \Varid{arr}\;\Varid{a}\;\Varid{b}\mskip1.5mu]\to \Varid{arr}\;[\mskip1.5mu \Varid{a}\mskip1.5mu]\;[\mskip1.5mu \Varid{b}\mskip1.5mu]}
that turns a list of Arrows into a singular parallel Arrow. We use this
class as an interface to wrap around existing parallel Haskells instead
of introducing yet another new low-level parallel backend.

As backends, we choose three of the most important parallel Haskells:
GpH, or more specifically -- its Multicore SMP implementation, (Trinder
et al., \protect\hyperlink{ref-Trinder1996}{1996},
\protect\hyperlink{ref-Trinder1998a}{1998}) for its simplicity, the
classic \ensuremath{\Conid{Par}} Monad (Foltzer et al.,
\protect\hyperlink{ref-Foltzer:2012:MPC:2398856.2364562}{2012}; Marlow
et al., \protect\hyperlink{ref-par-monad}{2011}) to represent a monadic
DSL, and Eden (Loogen, \protect\hyperlink{ref-Loogen2012}{2012}; Loogen
et al., \protect\hyperlink{ref-eden}{2005}) as a distributed memory
parallel Haskell targeting clusters running MPI or PVM. These languages
represent orthogonal approaches. Some use a Monad, even if only for the
internal representation while others introduce additional language
constructs.

While alternatives such as HdpH (Maier et al.,
\protect\hyperlink{ref-Maier:2014:HDS:2775050.2633363}{2014}; Stewart,
\protect\hyperlink{ref-stewart_maier_trinder_2016}{2016}, a Template
Haskell-based parallel Haskell for distributed memory) and LVish (Kuper
et al., \protect\hyperlink{ref-Kuper:2014:TPE:2666356.2594312}{2014}, a
\ensuremath{\Conid{Par}} extension with focus on communication) exist, these are not
chosen. The former does not differ from the original \ensuremath{\Conid{Par}} Monad with
regard to how we would have used it in this thesis, while the latter (at
least in its current form) does not suit a wrapping approach such as the
one presented here due to its heavy reliance on Template
Haskell\footnote{A Haskell extension that allows to generate code.}. We
will nonetheless experiment with a potential Cloud Haskell (Epstein et
al., \protect\hyperlink{ref-Epstein:2011:THC:2096148.2034690}{2011})
backend to explore the possibility of someday enabling the DSL to
natively work in modern cloud based clusters.

With the shallow--embedded DSL based on Arrows we define in this thesis
we do however not only aim to define a novel parallel programming
interface that allows for arbitrary Arrow types to be parallelised -- we
also aim to tame the zoo of parallel Haskells. Even more so, we want to
provide a common, low-penalty programming interface that is general by
allowing to switch the parallel implementations at will.

\hypertarget{structure}{%
\subsubsection{Structure}\label{structure}}

This thesis is structured as follows. In Chapter \ref{sec:related-work},
we discuss related work. Chapter \ref{sec:background} covers the
background of this thesis including an introduction to functional
programming, a Monad tutorial to finally introduce the concepts of
Arrows. In Chapter \ref{sec:parallel-arrows} we define the
shallow-embedded DSL based on Arrows (PArrows) together with some first
basic extensions and \ensuremath{\Varid{map}}-based skeletons. Chapter
\ref{sec:further-development} develops the PArrows API further by
introducing the concept of \ensuremath{\Conid{Future}}s and by giving the definitions of
some topology skeletons. We describe an experimental Cloud Haskell based
backend in Chapter \ref{sec:cloudHaskellExperiment}. Chapter
\ref{sec:benchmarks} evaluates the performance of the PArrows DSL.
Chapter \ref{sec:outlook} gives and outlook and concludes.

\hypertarget{related-work}{%
\chapter{Related Work}\label{related-work}}

\label{sec:related-work}

In this chapter, we will discuss work related to this thesis. We start
with work on parallel Haskells in Chapter
\ref{sec:relWorkParallelHaskells}. Next, we discuss research on
algorithmic skeletons in Chapter \ref{sec:relWorkAlgorithmicSkels}.
Then, we go over previous work in the field of Arrows in Chapter
\ref{sec:relWorkArrows}. Finally, we explain how this thesis is related
to previous work done on PArrows during the exploration of the subject
in Chapter \ref{sec:earlierwork}.

\hypertarget{parallel-haskells}{%
\section{Parallel Haskells}\label{parallel-haskells}}

\label{sec:relWorkParallelHaskells}

The non-strict semantics of Haskell, and the fact that reduction
encapsulates computations as closures, makes it relatively easy to
define alternate execution strategies. A range of approaches has been
explored, including data parallelism (Chakravarty et al.,
\protect\hyperlink{ref-Chakravarty2007}{2007}; Keller et al.,
\protect\hyperlink{ref-Keller:2010:RSP:1932681.1863582}{2010}),
GPU-based approaches (Mainland and Morrisett,
\protect\hyperlink{ref-Mainland:2010:NEC:2088456.1863533}{2010};
Svensson, \protect\hyperlink{ref-obsidian-phd}{2011}) and software
transactional memory (Harris et al.,
\protect\hyperlink{ref-Harris:2005:CMT:1065944.1065952}{2005}; Perfumo
et al., \protect\hyperlink{ref-Perfumo:2008:LST:1366230.1366241}{2008}).
The Haskell--GPU bridge Accelerate (Chakravarty et al.,
\protect\hyperlink{ref-Chakravarty:2011:AHA:1926354.1926358}{2011};
Clifton-Everest et al., \protect\hyperlink{ref-CMCK14}{2014}; McDonell
et al., \protect\hyperlink{ref-McDonell:2015:TRC:2887747.2804313}{2015})
is completely orthogonal to our approach as it is by nature focused on
parallel execution of singular functions on \enquote{Arrays} of data
instead of parallel execution of multiple functions with their
respective inputs. A good survey of parallel Haskells can be found in
Marlow (\protect\hyperlink{ref-marlow2013parallel}{2013}).

Our PArrows implementation uses three task parallel languages as
backends: the GpH (Trinder et al.,
\protect\hyperlink{ref-Trinder1996}{1996},
\protect\hyperlink{ref-Trinder1998a}{1998}) parallel Haskell dialect and
its multicore version (Marlow et al.,
\protect\hyperlink{ref-Marlow2009}{2009}), the \ensuremath{\Conid{Par}} Monad (Foltzer et
al., \protect\hyperlink{ref-Foltzer:2012:MPC:2398856.2364562}{2012};
Marlow et al., \protect\hyperlink{ref-par-monad}{2011}), and Eden
(Loogen, \protect\hyperlink{ref-Loogen2012}{2012}; Loogen et al.,
\protect\hyperlink{ref-eden}{2005}). These languages are under active
development, for example a combined shared and distributed memory
implementation of GpH is available (Aljabri et al.,
\protect\hyperlink{ref-Aljabri2015}{2015},
\protect\hyperlink{ref-Aljabri:2013:DIG:2620678.2620682}{2014}).
Research on Eden includes low-level implementation (Berthold,
\protect\hyperlink{ref-berthold_loidl_hammond_2016}{2016},
\protect\hyperlink{ref-JostThesis}{2008}), skeleton composition
(Dieterle et al.,
\protect\hyperlink{ref-dieterle_horstmeyer_loogen_berthold_2016}{2016}),
communication (Dieterle et al.,
\protect\hyperlink{ref-Dieterle2010}{2010}\protect\hyperlink{ref-Dieterle2010}{b}),
and generation of process networks (Horstmeyer and Loogen,
\protect\hyperlink{ref-Horstmeyer2013}{2013}). The definitions of new
Eden skeletons are a specific focus (Berthold and Loogen,
\protect\hyperlink{ref-Eden:PARCO05}{2006}; Berthold et al.,
\protect\hyperlink{ref-Berthold2009-mr}{2009}\protect\hyperlink{ref-Berthold2009-mr}{b},
\protect\hyperlink{ref-Berthold2009-fft}{2009}\protect\hyperlink{ref-Berthold2009-fft}{c};
Dieterle et al.,
\protect\hyperlink{ref-dieterle2010skeleton}{2010}\protect\hyperlink{ref-dieterle2010skeleton}{a},
\protect\hyperlink{ref-Dieterle2013}{2013}; Encina et al.,
\protect\hyperlink{ref-delaEncina2011}{2011}; Hammond et al.,
\protect\hyperlink{ref-doi:10.1142ux2fS0129626403001380}{2003}; Janjic
et al., \protect\hyperlink{ref-janjic2013space}{2013}).

Other task parallel Haskells related to Eden, GpH, and the \ensuremath{\Conid{Par}} Monad
include: HdpH (Maier et al.,
\protect\hyperlink{ref-Maier:2014:HDS:2775050.2633363}{2014}; Stewart,
\protect\hyperlink{ref-stewart_maier_trinder_2016}{2016}) is an
extension of \ensuremath{\Conid{Par}} Monad to heterogeneous clusters. LVish (Kuper et al.,
\protect\hyperlink{ref-Kuper:2014:TPE:2666356.2594312}{2014}) is a
communication-centred extension of the \ensuremath{\Conid{Par}} Monad and is based on
monotonically growing data structures.

Cloud Haskell (Epstein et al.,
\protect\hyperlink{ref-Epstein:2011:THC:2096148.2034690}{2011}) which we
use for an experimental backend is a domain specific language for cloud
computing but can also be used for parallel computation.

\hypertarget{algorithmic-skeletons}{%
\section{Algorithmic skeletons}\label{algorithmic-skeletons}}

\label{sec:relWorkAlgorithmicSkels}

Algorithmic skeletons as a concept to abstract the general idea of
different classes of algorithms were introduced by Cole
(\protect\hyperlink{ref-Cole1989}{1989}). Early publications on this
topic include Danelutto et al.
(\protect\hyperlink{ref-DANELUTTO1992205}{1992}), Darlington et al.
(\protect\hyperlink{ref-darlington1993parallel}{1993}), Botorog and
Kuchen (\protect\hyperlink{ref-botorog1996efficient}{1996}), Lengauer et
al. (\protect\hyperlink{ref-Lengauer1997}{1997}) and Gorlatch
(\protect\hyperlink{ref-Gorlatch1998}{1998}). Rabhi and Gorlatch
(\protect\hyperlink{ref-SkeletonBook}{2003}) consolidated early reports
on high-level programming approaches. Types of algorithmic skeletons
include \ensuremath{\Varid{map}}-, \ensuremath{\Varid{fold}}-, and \ensuremath{\Varid{scan}}-based parallel programming patterns,
special applications such as divide-and-conquer or topological
skeletons.

The \ensuremath{\Varid{farm}} skeleton (Hey, \protect\hyperlink{ref-Hey1990185}{1990}; Peña
and Rubio, \protect\hyperlink{ref-Eden:PPDP01}{2001}; Poldner and
Kuchen, \protect\hyperlink{ref-Kuchen05}{2005}) is a statically
task-balanced parallel \ensuremath{\Varid{map}}. When tasks' durations cannot be foreseen,
a dynamic load balancing (\ensuremath{\Varid{workpool}}) brings a lot of improvement
{[}Rudolph et al.
(\protect\hyperlink{ref-Rudolph:1991:SLB:113379.113401}{1991}); Hammond
et al. (\protect\hyperlink{ref-doi:10.1142ux2fS0129626403001380}{2003});
Hippold and Rünger (\protect\hyperlink{ref-Hippold2006}{2006}); Berthold
et al. (\protect\hyperlink{ref-PADL08HMWS}{2008}); Marlow2009{]}. For
special tasks, \ensuremath{\Varid{workpool}} skeletons can be extended with dynamic task
creation (Brown and Hammond,
\protect\hyperlink{ref-brown2010ever}{2010}; Dinan et al.,
\protect\hyperlink{ref-Dinan:2009:SWS:1654059.1654113}{2009}; Priebe,
\protect\hyperlink{ref-WPEuropar06}{2006}). Efficient load-balancing
schemes for \ensuremath{\Varid{workpool}}s are subject of research (Acar et al.,
\protect\hyperlink{ref-Acar:2000:DLW:341800.341801}{2000}; Blumofe and
Leiserson, \protect\hyperlink{ref-Blumofe:1999:SMC:324133.324234}{1999};
Chase and Lev,
\protect\hyperlink{ref-Chase:2005:DCW:1073970.1073974}{2005}; Michael et
al., \protect\hyperlink{ref-Michael:2009:IWS:1594835.1504186}{2009};
Nieuwpoort et al.,
\protect\hyperlink{ref-vanNieuwpoort:2001:ELB:568014.379563}{2001};
Olivier and Prins, \protect\hyperlink{ref-4625841}{2008}).

The \ensuremath{\Varid{fold}} (or \ensuremath{\Varid{reduce}}) skeleton was implemented in various skeleton
libraries (Buono et al., \protect\hyperlink{ref-BUONO20102095}{2010};
Dastgeer et al.,
\protect\hyperlink{ref-Dastgeer:2011:ASM:1984693.1984697}{2011};
Karasawa and Iwasaki, \protect\hyperlink{ref-5361825}{2009}; Kuchen,
\protect\hyperlink{ref-Kuchen2002}{2002}), as was its inverse, \ensuremath{\Varid{scan}}
(Bischof and Gorlatch, \protect\hyperlink{ref-Bischof2002}{2002}; Harris
et al., \protect\hyperlink{ref-harris2007parallel}{2007}). Google's
\ensuremath{\Varid{map}}--\ensuremath{\Varid{reduce}} (Dean and Ghemawat,
\protect\hyperlink{ref-Dean:2008:MSD:1327452.1327492}{2008},
\protect\hyperlink{ref-Dean:2010:MFD:1629175.1629198}{2010}) is more
special than just a composition of the two skeletons (Berthold et al.,
\protect\hyperlink{ref-Berthold2009-mr}{2009}\protect\hyperlink{ref-Berthold2009-mr}{b};
Lämmel, \protect\hyperlink{ref-LAMMEL20081}{2008}).

The effort is ongoing, including topological skeletons (Berthold and
Loogen, \protect\hyperlink{ref-Eden:PARCO05}{2006}), special-purpose
skeletons for computer algebra (Berthold et al.,
\protect\hyperlink{ref-Berthold2009-fft}{2009}\protect\hyperlink{ref-Berthold2009-fft}{c};
Janjic et al., \protect\hyperlink{ref-janjic2013space}{2013}; Lobachev,
\protect\hyperlink{ref-lobachev-phd}{2011},
\protect\hyperlink{ref-Lobachev2012}{2012}), and iteration skeletons
(Dieterle et al., \protect\hyperlink{ref-Dieterle2013}{2013}). The idea
of Linton et al. (\protect\hyperlink{ref-scscp}{2010}) is to use a
parallel Haskell to orchestrate further software systems to run in
parallel. Dieterle et al.
(\protect\hyperlink{ref-dieterle_horstmeyer_loogen_berthold_2016}{2016})
compare the composition of skeletons to stable process networks.

We implement some of these algorithmic skeletons in Chapters
\ref{sec:extending-interface} and \ref{sec:topology-skeletons}, namely
various parallel \ensuremath{\Varid{map}} variants as well as the topological skeletons
\ensuremath{\Varid{pipe}}, \ensuremath{\Varid{ring}} and \ensuremath{\Varid{torus}}.

\hypertarget{arrows}{%
\section{Arrows}\label{arrows}}

\label{sec:relWorkArrows}

Arrows were introduced by Hughes
(\protect\hyperlink{ref-HughesArrows}{2000}) as a less restrictive
alternative to Monads. In essence, they are a generalised function arrow
\ensuremath{\to }. Hughes (\protect\hyperlink{ref-Hughes2005}{2005}) presents a
tutorial on Arrows. Jacobs
(\protect\hyperlink{ref-jacobs_heunen_hasuo_2009}{2009}), Lindley et al.
(\protect\hyperlink{ref-LINDLEY201197}{2011}), Atkey
(\protect\hyperlink{ref-ATKEY201119}{2011}) develop the theoretical
background of Arrows. (Paterson,
\protect\hyperlink{ref-Paterson:2001:NNA:507669.507664}{2001})
introduced a new notation for Arrows. Arrows have applications in
information flow research (Li and Zdancewic,
\protect\hyperlink{ref-1648705}{2006},
\protect\hyperlink{ref-LI20101974}{2010}; Russo et al.,
\protect\hyperlink{ref-Russo:2008:LLI:1411286.1411289}{2008}),
invertible programming (Alimarine et al.,
\protect\hyperlink{ref-Alimarine:2005:BAA:1088348.1088357}{2005}), and
quantum computer simulation (Vizzotto,
\protect\hyperlink{ref-vizzotto_altenkirch_sabry_2006}{2006}). But the
probably most prominent application of Arrows is Arrow-based functional
reactive programming, AFRP (Czaplicki and Chong,
\protect\hyperlink{ref-Czaplicki:2013:AFR:2499370.2462161}{2013}; Hudak
et al., \protect\hyperlink{ref-Hudak2003}{2003}; Nilsson et al.,
\protect\hyperlink{ref-Nilsson:2002:FRP:581690.581695}{2002}). (Liu et
al., \protect\hyperlink{ref-Liu:2009:CCA:1631687.1596559}{2009})
formally define a more special kind of Arrows that encapsulate the
computation more than regular Arrows do and thus enable optimisations.
Their approach would allow parallel composition, as their Arrows would
not interfere with each other in concurrent execution. In contrast, we
capture a whole parallel computation as a single entity: our main
instantiation function \ensuremath{\Varid{parEvalN}} creates a single (parallel) Arrow out
of a list of Arrows. Huang et al.
(\protect\hyperlink{ref-Huang2007}{2007}) utilise Arrows for
parallelism, but strikingly different from our approach. They use Arrows
to orchestrate several tasks in robotics. We, however, propose a general
interface for parallel programming, while remaining completely in
Haskell.

\hypertarget{arrows-in-other-languages}{%
\subsubsection{Arrows in other
languages}\label{arrows-in-other-languages}}

Although this work is centred on Haskell's implementation of Arrows, it
is applicable to any functional programming language where parallel
evaluation and Arrows can be defined. Basic definitions of PArrows are
possible in the Frege language\footnote{See the GitHub project page at
  \url{https://github.com/Frege/frege}.} -- a Haskell-like language that
compiles to Java code to then be compiled natively on the Java Virtual
Machine (JVM). However, they are beyond the scope of this work, as are
similar experiments with the Eta language\footnote{See the Eta project page at
  \url{http://eta-lang.org}.}, a new approach to Haskell on the JVM that
compiles directly to JVM bytecode.

Arrows have also been shown to be useful for better handling of typical
GUI tasks in Clean (Achten et al.,
\protect\hyperlink{ref-achten2004arrows}{2004},
\protect\hyperlink{ref-achten2007arrow}{2007}). Dagand et al.
(\protect\hyperlink{ref-Dagand:2009:ORD:1481861.1481870}{2009}) used
Arrows in OCaml in the implementation of a distributed system.

\hypertarget{earlier-work}{%
\section{Earlier work}\label{earlier-work}}

\label{sec:earlierwork}

Research on the PArrows DSL started with a Master's research project
under the supervision of Oleg Lobachev. The
results of this work\footnote{Available at \url{https://github.com/s4ke/Parrows/tree/projekt}.} included the basic idea of the interface as well as
some early basic \ensuremath{\Varid{map}}-based skeletons. We greatly expand on these
results in Chapters \ref{sec:parallelHaskells} --
\ref{sec:map-skeletons}.

Later, the paper \enquote{Arrows for Parallel Computation} (Braun et
al., \protect\hyperlink{ref-PArrowsPaper}{2018}), went further by
introducing the Future concept as well as the implementations of the
topological skeletons \ensuremath{\Varid{pipe}}, \ensuremath{\Varid{ring}} and \ensuremath{\Varid{torus}}. Chapters
\ref{sec:related-work}, \ref{sec:arrows} --
\ref{sec:further-development} as well as Chapter \ref{sec:benchmarks}
are based on this paper.

The author contributions to the paper are as follows: Conceptualisation:
MB, OL. Methodology: MB. Software: MB. Validation: MB. Formal Analysis:
MB. Investigation: MB. Resources: PT. Data Curation: MB Writing --
Original Draft: MB. Writing -- Review \& Editing: MB, OL, PT.
Visualization: MB. Supervision: OL. Corresponding author: OL

\hypertarget{background}{%
\chapter{Background}\label{background}}

\label{sec:background}

Before we delve into our novel approach for parallel programming using
Arrows, we give a short overview of all our main concepts and
technologies. We start with an introduction to functional programming
(Chapter \ref{sec:fuproHaskell}) including a short tutorial on Monads
(Chapter \ref{sec:monads}) as well as an introduction to Arrows (Chapter
\ref{sec:arrows}). Finally, we give a short introduction to the main
parallel Haskells that are used as backends for our DSL in this thesis
(Chapter \ref{sec:parallelHaskells}) - GpH, the \ensuremath{\Conid{Par}} Monad, and Eden.

\hypertarget{functional-programming}{%
\section{Functional programming}\label{functional-programming}}

\label{sec:fuproHaskell}

This chapter covers the basics of functional programming. We start by
citing Hughes
(\protect\hyperlink{ref-Hughes:1990:WFP:119830.119832}{1990}) on why
functional programming matters including a characterisation of the
concept in general (Chapter \ref{sec:whyfupro}). Then, we give a short
introduction to functional programming with Haskell (Chapter
\ref{sec:shortIntroHaskell}) and also explain the concept of Monads
(Chapter \ref{sec:monads}) which some parallel Haskells use. Finally, we
introduce Arrows and explain their type class in Haskell (Chapter
\ref{sec:arrows}).

\hypertarget{why-functional-programming-matters}{%
\subsection{Why functional programming
matters}\label{why-functional-programming-matters}}

\label{sec:whyfupro}

Hughes (\protect\hyperlink{ref-Hughes:1990:WFP:119830.119832}{1990})
describes the fundamental idea of functional programming like this:

\begin{quote}
Functional programming is so called because its fundamental operation is
the application of functions to arguments. A main program itself is
written as a function that receives the program's input as its argument
and delivers the program's output as its result. Typically the main
function is defined in terms of other functions, which in turn are
defined in terms of still more functions, until at the bottom level the
functions are language primitives.
\end{quote}

Basically, functional programs only contain logic described in terms of
functions and their compositions. Additionally, functional programming
is also often characterized as follows (Hughes,
\protect\hyperlink{ref-Hughes:1990:WFP:119830.119832}{1990}):

\begin{quote}
Functional programs contain no assignment statements, so variables, once
given a value, never change. More generally, functional programs contain
no side-effects at all. A function call can have no effect other than to
compute its result. This eliminates a major source of bugs, and also
makes the order of execution irrelevant --- since no side- effect can
change an expression's value, it can be evaluated at any time. This
relieves the programmer of the burden of prescribing the flow of
control. Since expressions can be evaluated at any time, one can freely
replace variables by their values and vice versa --- that is, programs
are ``referentially transparent''. This freedom helps make functional
programs more tractable mathematically than their conventional
counterparts.
\end{quote}

While all these are all good arguments in favour of functional
programming -- because of the elimination of programming bottlenecks --
these arguments only describe functional programming by means of what it
can not do. Hughes
(\protect\hyperlink{ref-Hughes:1990:WFP:119830.119832}{1990}) describes
his dissatisfaction with this argument as follows:

\begin{quote}
Even a functional programmer should be dissatisfied with these so-called
advantages, because they give no help in exploiting the power of
functional languages. One cannot write a program that is particularly
lacking in assignment statements, or particularly referentially
transparent. There is no yardstick of program quality here, and
therefore no ideal to aim at.
\end{quote}

To argue that there is merit in functional programming besides having
fewer error-prone features Hughes
(\protect\hyperlink{ref-Hughes:1990:WFP:119830.119832}{1990}) then goes
into detail about one of the areas where functional programming shines
and why it therefore matters -- modularity. He argues that modularity is
only possible with good glue code. This is where he sees functional
programming to be better suited because of two powerful tools:
higher-order functions and laziness.

Higher-order functions are functions that take other functions as
arguments. They usually generalize a concept (e.g.~mapping over a list,
zipping two lists, etc.) and take the passed function(s) as their
internal worker function. They provide the skeleton of the program.
Laziness here means that values are only evaluated when required. This
allows for programs to work in a producer/consumer pattern without
having to write manual interweaving code. We will explain both concepts
in greater details later in this chapter.

The focus on modularity through composability that functional languages
have, can be seen in all the definitions of Haskell functions in the
following chapters of this thesis. Also, the main functional concept
this thesis uses, Arrows, are by nature a result of the desire to
generalize modularity. We will show how to enable programs based on this
concept to benefit from parallelism by using our combinator \ensuremath{\Varid{parEvalN}}.

\hypertarget{a-short-introduction-to-haskell}{%
\subsection{A short introduction to
Haskell}\label{a-short-introduction-to-haskell}}

\label{sec:shortIntroHaskell}

Even though this thesis is called
\enquote{Concepts in Parallel Programming} and focuses on Arrows for
parallel (functional) computation, we have to first define the basic
building blocks of our programming language and show how to use them in
regular programs before we can explore \emph{parallel} programming.
Therefore, in the following chapter, we will give a short introduction
to functional programming with Haskell. While this will give a good idea
of how programming in Haskell works, this is not supposed to be a
complete tutorial on Haskell, but merely a quick overview over the most
relevant features of the language which are used in this thesis. The
following is loosely based on the book
\enquote{Learn you a haskell for great good!} (Michaelson,
\protect\hyperlink{ref-learnyouahaskell}{2013}).

\hypertarget{from-imperative-programming-to-functional-programming}{%
\subsubsection{From Imperative Programming to Functional
Programming}\label{from-imperative-programming-to-functional-programming}}

In order to ease the introduction to functional programming, we will
give a short introduction to functional programming in Haskell in this
chapter by comparing the general style of imperative C code to
functional Haskell using the example of the Fibonacci sequence.

To start off, we take a look at the following iterative implementation
of the Fibonacci sequence:

\begin{center}
\lstset{language=C}
\centering
\begin{lstlisting}
int fib( int n ) {
    int pre = 0;
    int cur = 1;
    int res = 0;
    for ( int i = 0; i < n; ++i ) {
        res = pre + cur;
        pre = cur;
        cur = res;
    }
    return cur;
}
\end{lstlisting}
\end{center}
\vspace{-1\baselineskip}

It contains assignments and a loop, which in pure\footnote{Pure code is
  code without side-effects. Assignments are side-effects.} functional
programming we can not use\footnote{It is however possible to introduce
  monadic DSLs in Haskell that mimic C style behaviour, see
  \url{https://hackage.haskell.org/package/ImperativeHaskell-2.0.0.1}.}.
If we translate this Fibonacci example into a recursive definition,
however, we get pure functional C code without any assignment
statements:

\begin{center}
\lstset{language=C}
\centering
\begin{lstlisting}
int fib( int n ) {
    if ( n <= 0 )
        return 0;
    else if ( n == 1 )
        return 1;
    else
        return fib( n - 2 ) + fib( n - 1 );
}
\end{lstlisting}
\end{center}
\vspace{-1\baselineskip}

This resembles the Haskell variant in Figure \ref{fig:fibonacciHaskell}.
We can see how the flow of the programming works even without requiring
any modifiable state. Furthermore, the Haskell implementation uses
guards (e.g. \ensuremath{\Varid{n}\leq \mathrm{0}\mathrel{=}\mathrm{0}}) which are equivalent to the conditional
statements in the C variant. They will be explained in more detail later
in this chapter.

\begin{figure}[h]
\centering
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{9}{@{}>{\hspre}l<{\hspost}@{}}%
\column{13}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{fib}\mathbin{::}\Conid{Int}\to \Conid{Int}{}\<[E]%
\\
\>[B]{}\Varid{fib}\;\Varid{n}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\mid \Varid{n}\leq \mathrm{0}\mathrel{=}\mathrm{0}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\mid \Varid{n}\equiv \mathrm{1}\mathrel{=}\mathrm{1}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\mid \Varid{otherwise}\mathrel{=}{}\<[E]%
\\
\>[5]{}\hsindent{4}{}\<[9]%
\>[9]{}(\Varid{fib}\;(\Varid{n}\mathbin{-}\mathrm{2})){}\<[E]%
\\
\>[9]{}\hsindent{4}{}\<[13]%
\>[13]{}\mathbin{+}(\Varid{fib}\;(\Varid{n}\mathbin{-}\mathrm{1})){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\caption{Standard Fibonacci in Haskell.}\label{fig:fibonacciHaskell}\end{figure}

In functional languages like Haskell we only express computations in
this matter by composition of functions (recursion in essence is also
just a composition of a function with itself). Because of this and since
we can not change the state of any associated variables, we usually also
do not have to worry about the order of execution in functional programs
and let the compiler decide how to resolve the recursive formula. In
general, we can say that in functional programming we primarily focus on
what information is required and by which transformations to compute it
instead of how we perform them and how we track the changes in
state.\footnote{From
  \url{https://docs.microsoft.com/en-us/dotnet/visual-basic/programming-guide/concepts/linq/functional-programming-vs-imperative-programming}.}

Haskell being a functional language does not mean that we do not have
the usual problem of a too small call-stack size encountered when
programming with recursion. While Haskell programs can naturally handle
much bigger call-stacks without overflowing, at some point the limit
will be reached and the program will crash. But since the class of
tail-recursive programs which all have the form


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{7}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{f}\;\Varid{x}\mathrel{=}\mathbf{if}\mathbin{<}\Varid{end}\mathbin{>}{}\<[E]%
\\
\>[B]{}\hsindent{7}{}\<[7]%
\>[7]{}\mathbf{then}\;\Varid{s}\;\Varid{x}{}\<[E]%
\\
\>[B]{}\hsindent{7}{}\<[7]%
\>[7]{}\mathbf{else}\;\Varid{f}\;(\Varid{r}\;\Varid{x}){}\<[E]%
\\[\blanklineskip]%
\>[B]{}\mbox{\onelinecomment  s and r arbitrary, but not depending on f}{}\<[E]%
\\
\>[B]{}\Varid{s}\mathrel{=}\mathbin{...}{}\<[E]%
\\
\>[B]{}\Varid{r}\mathrel{=}\mathbin{...}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

is equivalent to the class of all recursive programs (which is in turn
equivalent to all imperative programs), this is no big problem. We can
just translate our \ensuremath{\Varid{fib}} definition into a tail-recursive variant
(Figure \ref{fig:fibonacciHaskellTailRecursive}) which Haskell's
compiler is capable of automatically translating into looping machine
code.

\begin{figure}[ht]
\centering
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{9}{@{}>{\hspre}l<{\hspost}@{}}%
\column{13}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{fib}\mathbin{::}\Conid{Int}\to \Conid{Int}{}\<[E]%
\\
\>[B]{}\Varid{fib}\;\Varid{n}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\mid \Varid{n}\leq \mathrm{0}\mathrel{=}\mathrm{0}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\mid \Varid{otherwise}\mathrel{=}\Varid{fib'}\;\Varid{n}\;\mathrm{0}\;\mathrm{1}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\mathbf{where}{}\<[E]%
\\
\>[5]{}\hsindent{4}{}\<[9]%
\>[9]{}\Varid{fib'}\mathbin{::}\Conid{Int}\to \Conid{Int}\to \Conid{Int}\to \Conid{Int}{}\<[E]%
\\
\>[5]{}\hsindent{4}{}\<[9]%
\>[9]{}\Varid{fib'}\;\Varid{n}\;\Varid{prev}\;\Varid{res}{}\<[E]%
\\
\>[9]{}\hsindent{4}{}\<[13]%
\>[13]{}\mid \Varid{n}\equiv \mathrm{0}\mathrel{=}\Varid{res}{}\<[E]%
\\
\>[9]{}\hsindent{4}{}\<[13]%
\>[13]{}\mid \Varid{otherwise}\mathrel{=}\Varid{fib'}\;(\Varid{n}\mathbin{-}\mathrm{1})\;\Varid{res}\;(\Varid{res}\mathbin{+}\Varid{prev}){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\caption{Tail Recursive Fibonacci in Haskell.}\label{fig:fibonacciHaskellTailRecursive}\end{figure}

\hypertarget{functions}{%
\subsubsection{Functions}\label{functions}}

As already mentioned above, the basic building blocks of a Haskell
program are functions. We define them like this:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{f}\mathbin{::}\Conid{Int}\to \Conid{Int}\to \Conid{Int}{}\<[E]%
\\
\>[B]{}\Varid{f}\;\Varid{x}\;\Varid{y}\mathrel{=}\Varid{multiply}\;\Varid{x}\;\Varid{y}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

Here, we declared a function \ensuremath{\Varid{f}} which takes two arguments of type \ensuremath{\Conid{Int}}
and returns yet another \ensuremath{\Conid{Int}}. In the definition, we say that \ensuremath{\Varid{f}} is the
function \ensuremath{\Varid{multiply}} applied to both its arguments \ensuremath{\Varid{x}} and \ensuremath{\Varid{y}}. We define
\ensuremath{\Varid{multiply}} as


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{multiply}\mathbin{::}\Conid{Int}\to \Conid{Int}\to \Conid{Int}{}\<[E]%
\\
\>[B]{}\Varid{multiply}\;\Varid{x}\;\Varid{y}\mathrel{=}\Varid{x}\mathbin{*}\Varid{y}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

In Haskell, since \ensuremath{\Varid{f}} and \ensuremath{\Varid{multiply}} seem to be the same, we can even
write this relationship directly:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{f}\mathbin{::}\Conid{Int}\to \Conid{Int}\to \Conid{Int}{}\<[E]%
\\
\>[B]{}\Varid{f}\mathrel{=}\Varid{multiply}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

We can do so, because, in Haskell, functions can be treated just like
any other type. For example, if we want to have another function \ensuremath{\Varid{g}}
which applies \ensuremath{\Varid{f}} on two lists of integers, we can write


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{g}\mathbin{::}[\mskip1.5mu \Conid{Int}\mskip1.5mu]\to [\mskip1.5mu \Conid{Int}\mskip1.5mu]\to [\mskip1.5mu \Conid{Int}\mskip1.5mu]{}\<[E]%
\\
\>[B]{}\Varid{g}\mathrel{=}\Varid{zipWith}\;\Varid{f}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

where \ensuremath{\Varid{zipWith}} would be of type
\ensuremath{(\Conid{Int}\to \Conid{Int}\to \Conid{Int})\to [\mskip1.5mu \Conid{Int}\mskip1.5mu]\to [\mskip1.5mu \Conid{Int}\mskip1.5mu]\to [\mskip1.5mu \Conid{Int}\mskip1.5mu]}. It is common to
express calculations in such a way using higher-order functions. We will
see more of this later in this chapter.

\hypertarget{type-inference}{%
\subsubsection{Type inference}\label{type-inference}}

Taking the same example function \ensuremath{\Varid{g}} from above, it does not make sense
to be so restrictive in terms of which type to allow in such a function
since all it does is to apply some function to zip two lists.
Thankfully, in Haskell we can define functions in a completely generic
way so that we can write the actual type of zipWith as
\ensuremath{(\Varid{a}\to \Varid{b}\to \Varid{c})\to [\mskip1.5mu \Varid{a}\mskip1.5mu]\to [\mskip1.5mu \Varid{b}\mskip1.5mu]\to [\mskip1.5mu \Varid{c}\mskip1.5mu]}. This means that we zip a list
containing some \ensuremath{\Varid{a}}s with a list containing a list of \ensuremath{\Varid{b}}s with a
function \ensuremath{\Varid{a}\to \Varid{b}\to \Varid{c}} to get a list of \ensuremath{\Varid{c}}s. Only because we use this
function in the context of our function \ensuremath{\Varid{g}} it is specialized to the
\ensuremath{\Conid{Int}} form.

Type inference even allows us to define \ensuremath{\Varid{g}} without writing down the
type definition and let the compiler determine the actual type of \ensuremath{\Varid{g}}.


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{g}\mathrel{=}\Varid{zipWith}\;\Varid{f}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

While this is possible, it is generally encouraged to always specify the
type of top-level functions for better readability. Leaving out the type
specification can, however, be useful when defining nested helper
functions.

\hypertarget{function-composition-higher-order-functions-and-function-application}{%
\subsubsection{Function composition, higher-order functions, and
function
application}\label{function-composition-higher-order-functions-and-function-application}}

As we have seen, functions can be handled similar to data types in
Haskell. This way, we can for example define a function that computes a
number to the power of four as


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{toThePowerOfFour}\mathbin{::}\Conid{Int}\to \Conid{Int}{}\<[E]%
\\
\>[B]{}\Varid{toThePowerOfFour}\mathrel{=}\Varid{toThePowerOfTwo}\mathbin{\circ}\Varid{toThePowerOfTwo}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

with \ensuremath{\mathbin{\circ}} being the functional composition operator with type
\ensuremath{(\mathbin{\circ})\mathbin{::}(\Varid{b}\to \Varid{c})\to (\Varid{a}\to \Varid{b})\to (\Varid{a}\to \Varid{c})}\footnote{Note the order of the
  arguments, \ensuremath{\Varid{g}\mathbin{\circ}\Varid{f}} means to first apply \ensuremath{\Varid{f}} and then \ensuremath{\Varid{g}} and not the
  other way around.} and where \ensuremath{\Varid{toThePowerOfTwo}} is defined simply as


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{toThePowerOfTwo}\mathbin{::}\Conid{Int}\to \Conid{Int}{}\<[E]%
\\
\>[B]{}\Varid{toThePowerOfTwo}\;\Varid{x}\mathrel{=}\Varid{multiply}\;\Varid{x}\;\Varid{x}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

Another aspect of functions being similar to data types is that, in
functional programming, we frequently use higher order functions to
express calculations. We have seen this earlier with the use of
\ensuremath{\Varid{zipWith}}. Other often used higher-order functions include mapping
(\ensuremath{\Varid{map}\mathbin{::}(\Varid{a}\to \Varid{b})\to [\mskip1.5mu \Varid{a}\mskip1.5mu]\to [\mskip1.5mu \Varid{b}\mskip1.5mu]}, i.e.~convert a list of \ensuremath{\Varid{a}}s into a
list of \ensuremath{\Varid{b}}s with the given function \ensuremath{\Varid{a}\to \Varid{b}}) and folding (e.g.
\ensuremath{\Varid{foldLeft}\mathbin{::}(\Varid{b}\to \Varid{a}\to \Varid{b})\to \Varid{b}\to [\mskip1.5mu \Varid{a}\mskip1.5mu]\to \Varid{b}}, i.e.~reduce the list with
the given function \ensuremath{\Varid{b}\to \Varid{a}\to \Varid{b}} into a singular value given a starting
value of type \ensuremath{\Varid{b}}). These are often used in some kind of composition
like


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{euclidDistance}\mathbin{::}[\mskip1.5mu \Conid{Int}\mskip1.5mu]\to [\mskip1.5mu \Conid{Int}\mskip1.5mu]\to \Conid{Int}{}\<[E]%
\\
\>[B]{}\Varid{euclidDistance}\mathrel{=}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{sqrt}\mathbin{\circ}\Varid{foldLeft}\;(\mathbin{+})\;\mathrm{0}\mathbin{\circ}\Varid{map}\;(\Varid{toThePowerOfTwo})\mathbin{\circ}\Varid{zipWith}\;(\mathbin{-}){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

Note that while this could have easily been written with fewer
higher-order functions as something along the lines of
\ensuremath{\Varid{sqrt}\;(\Varid{foldLeft}\;(\mathbin{+})\;\mathrm{0}\;(\Varid{zipWith}\;(\lambda \Varid{a}\;\Varid{b}\to \Varid{toThePowerOfTwo}\;(\Varid{a}\mathbin{-}\Varid{b}))))}, it
is easy to see that the above declaration is easier to understand
because of the simple steps the computation takes. We first zip the list
of inputs with element-wise subtraction and then square this difference,
sum these results up and finally take the square root. This is something
we see a lot in Haskell code: Complex computations can be expressed with
the help of higher-order functions instead of having to write it
manually. This is not only often much shorter, but also easier to
understand for other programmers if they have to read-up on the
implementation for some reason.

Something which is also quite useful in Haskell is the function
application operator \ensuremath{(\mathbin{\$})\mathbin{::}(\Varid{a}\to \Varid{b})\to \Varid{a}\to \Varid{b}} allowing for the
application of a function \ensuremath{\Varid{a}\to \Varid{b}} to a given argument \ensuremath{\Varid{a}}. It is simply
defined as:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}(\mathbin{\$})\mathbin{::}(\Varid{a}\to \Varid{b})\to \Varid{a}\to \Varid{b}{}\<[E]%
\\
\>[B]{}\Varid{f}\mathbin{\$}\Varid{x}\mathrel{=}\Varid{f}\;\Varid{x}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

While the use-case for such an operator might not be immediately clear,
it will be, if we take a look at the following function
\ensuremath{\Varid{listApp}\mathbin{::}[\mskip1.5mu \Varid{a}\to \Varid{b}\mskip1.5mu]\to [\mskip1.5mu \Varid{a}\mskip1.5mu]\to [\mskip1.5mu \Varid{b}\mskip1.5mu]} where we take a list of functions
\ensuremath{[\mskip1.5mu \Varid{a}\to \Varid{b}\mskip1.5mu]} and apply them one-by-one with their respective input values
from the input list \ensuremath{[\mskip1.5mu \Varid{a}\mskip1.5mu]} to generate a list of results \ensuremath{\Varid{b}}:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{listApp}\mathbin{::}[\mskip1.5mu \Varid{a}\to \Varid{b}\mskip1.5mu]\to [\mskip1.5mu \Varid{a}\mskip1.5mu]\to [\mskip1.5mu \Varid{b}\mskip1.5mu]{}\<[E]%
\\
\>[B]{}\Varid{listApp}\mathrel{=}\Varid{zipWith}\;(\mathbin{\$}){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

Here, if we had not used the \ensuremath{\mathbin{\$}} operator, we would have to write
\ensuremath{\Varid{zipWith}\;(\lambda \Varid{f}\;\Varid{a}\to \Varid{f}\;\Varid{a})} which certainly seems a bit redundant.

Something the \ensuremath{\mathbin{\$}} operator is also used quite often is to write
shorter code. For example, code snippets like


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{someFunc}\mathrel{=}\Varid{f1}\;(\Varid{f2}\;\Varid{param1}\;(\Varid{f3}\;\Varid{param2}\;(\Varid{f4}\;\Varid{param3})){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

can also be written without the braces as


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{someFunc}\mathrel{=}\Varid{f1}\mathbin{\$}\Varid{f2}\;\Varid{param1}\mathbin{\$}\Varid{f3}\;\Varid{param2}\mathbin{\$}\Varid{f4}\;\Varid{param3}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

which is sometimes preferred to the brace-style, but is semantically
identical.

\hypertarget{conditional-computation}{%
\subsubsection{Conditional Computation}\label{conditional-computation}}

Haskell has different styles of dealing with conditional evaluation. We
will now show the most common variants.

The most obvious one in terms of functionality is the
\ensuremath{\mathbf{if}\mathbin{...}\mathbf{then}\mathbin{...}\mathbf{else}} construct:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{myFunc}\mathbin{::}\Conid{Int}\to \Conid{Int}{}\<[E]%
\\
\>[B]{}\Varid{myFunc}\;\Varid{x}\mathrel{=}\mathbf{if}\;\Varid{x}\mathbin{<}\mathrm{10}\;\mathbf{then}\;\Varid{x}\mathbin{*}\mathrm{2}\;\mathbf{else}\;\Varid{x}\mathbin{*}\mathrm{4}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

While having the same well-known semantics of any \ensuremath{\mathbf{if}\mathbin{...}\mathbf{then}\mathbin{...}\mathbf{else}}
like they could be found in imperative languages like e.g.~C, in
Haskell, being a functional language, the \ensuremath{\mathbf{else}} is non-optional as
expressions are required to be total.\footnote{Total in terms of
  computation, unsuccessful calculations can still be expressed with
  constructs like \ensuremath{\Conid{Maybe}\;\Varid{a}}.}

An alternative to this are guards, which make expressions easier to read
if many alternatives are involved in a function:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{myFunc}\mathbin{::}\Conid{Int}\to \Conid{Int}{}\<[E]%
\\
\>[B]{}\Varid{myFunc}\;\Varid{x}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\mid \Varid{x}\mathbin{<}\mathrm{10}\mathrel{=}\Varid{x}\mathbin{*}\mathrm{2}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\mid \Varid{x}\mathbin{<}\mathrm{12}\mathrel{=}\Varid{x}\mathbin{*}\mathrm{3}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\mid \Varid{x}\mathbin{<}\mathrm{14}\mathrel{=}\Varid{x}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\mid \Varid{x}\mathbin{>}\mathrm{18}\mathrel{\wedge}\Varid{x}\mathbin{<}\mathrm{20}\mathrel{=}\mathrm{42}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\mid \Varid{otherwise}\mathrel{=}\Varid{x}\mathbin{*}\mathrm{4}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

Yet another technique is to use pattern matching. For conditional
statements, we can use it by writing definitions of the function for
specific values, like


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{myFunc}\mathbin{::}\Conid{Int}\to \Conid{Int}{}\<[E]%
\\
\>[B]{}\Varid{myFunc}\;\mathrm{5}\mathrel{=}\mathrm{10}{}\<[E]%
\\
\>[B]{}\Varid{myFunc}\;\Varid{x}\;@\;\mathrm{10}\mathrel{=}\Varid{x}\mathbin{*}\mathrm{10}{}\<[E]%
\\
\>[B]{}\Varid{myFunc}\;\Varid{x}\mathrel{=}\Varid{x}\mathbin{*}\mathrm{2}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

where the first matching definition is chosen during
computation.\footnote{Here, the \ensuremath{@} allows us to bind the value to
  a variable while also pattern matching it.} Alternatively, we can do
pattern matching with the help of \ensuremath{\mathbf{case}} expressions:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{myFunc}\mathbin{::}\Conid{Int}\to \Conid{Int}{}\<[E]%
\\
\>[B]{}\Varid{myFunc}\;\Varid{x}\mathrel{=}\mathbf{case}\;\Varid{x}\;\mathbf{of}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\mathrm{5}\to \mathrm{10}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\mathrm{10}\to \Varid{x}\mathbin{*}\mathrm{10}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\anonymous \to \Varid{x}\mathbin{*}\mathrm{2}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

These can be used just like ordinary expressions.

We can not, however, express boolean statements in this way. This is
because pattern matching is done on the structure of the value that is
being pattern matched. Later in this chapter, we will see what other
powerful things we can do with this technique.

\hypertarget{where-let}{%
\subsubsection{\texorpdfstring{\ensuremath{\mathbf{where}}, \ensuremath{\mathbf{let}}}{, }}\label{where-let}}

While Haskell does not have variables and only works on values instead,
it still allows the programmer to name sub-expressions so that either
the code becomes more clear or that it can be reused more easily. Here,
two different variants are available: \ensuremath{\mathbf{where}} and \ensuremath{\mathbf{let}}.

With the help of \ensuremath{\mathbf{where}} we can write code like


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{whereReuse}\mathbin{::}\Conid{Double}\to \Conid{Double}\to \Conid{String}{}\<[E]%
\\
\>[B]{}\Varid{whereReuse}\;\Varid{a}\;\Varid{b}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\mid \Varid{divided}\mathbin{>}\mathrm{10}\mathrel{=}\text{\tt \char34 a~is~more~than~10~times~b\char34}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\mid \Varid{divided}\equiv \mathrm{10}\mathrel{=}\text{\tt \char34 a~is~10~times~b\char34}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\mid \Varid{otherwise}\mathrel{=}\text{\tt \char34 a~is~less~than~10~times~b\char34}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\mathbf{where}\;\Varid{divided}\mathrel{=}\Varid{a}\mathbin{/}\Varid{b}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

It is important to note that \ensuremath{\mathbf{where}} is just syntactic sugar, though,
and can not used in expressions like \ensuremath{\Varid{f}\;(\Varid{a}\mathbin{*}\mathrm{2}\;\mathbf{where}\;\Varid{a}\mathrel{=}\mathrm{3})}. This is in
turn possible with \ensuremath{\mathbf{let}}, which allows us to write expressions like
\ensuremath{\Varid{f}\;(\mathbf{let}\;\Varid{a}\mathrel{=}\mathrm{3}\;\mathbf{in}\;\Varid{a}\mathbin{*}\mathrm{2})} or \ensuremath{\mathbf{let}\;\Varid{a}\mathrel{=}\mathrm{3}\;\mathbf{in}\;\Varid{f}\;(\Varid{a}\mathbin{*}\mathrm{2})}. \ensuremath{\mathbf{let}} can,
however, not be used in conjunction with guards.

\hypertarget{type-safety}{%
\subsubsection{Type safety}\label{type-safety}}

Haskell is a statically typed functional language. This means that
during compilation all types are checked for compatibility and type
declarations are not just treated as optional \enquote{hints} to the
type-checker. Pairing this with the pure aspect of the language means
that Haskell programs seem to be correct more often in practice if the
program compiles than in imperative languages. The compiler essentially
helps the programmer to write \emph{semantically correct} instead of
just syntactically correct code. It should be noted that this does not
mean that testing can be omitted. It is still extremely important, but
becomes less cumbersome because state is mostly a non-issue.

\hypertarget{type-classes}{%
\subsubsection{Type classes}\label{type-classes}}

The example function \ensuremath{\Varid{multiply}} which we have seen earlier seems a bit
restrictive as it only allows for the usage of \ensuremath{\Conid{Int}}s. \ensuremath{\Conid{Int}}s are
obviously not the only type which can be multiplied. Haskell has a way
to express this fact: type classes. We can express a type class
\ensuremath{\Conid{Multiplicable}\;\Varid{a}} that encapsulates the contract of \ensuremath{\Varid{multiply}} on some
type \ensuremath{\Varid{a}} as


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{class}\;\Conid{Multiplicable}\;\Varid{a}\;\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{multiply}\mathbin{::}\Varid{a}\to \Varid{a}\to \Varid{a}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

With this definition, we can then introduce instances -- implementations
of the contract -- for specific types. For example, the instance for
\ensuremath{\Conid{Int}}, \ensuremath{\Conid{Multiplicable}\;\Conid{Int}}, can be written as


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{instance}\;\Conid{Multiplicable}\;\Conid{Int}\;\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{multiply}\;\Varid{x}\;\Varid{y}\mathrel{=}\Varid{x}\mathbin{*}\Varid{y}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

Now if we want to use this new contract on a generic function \ensuremath{\Varid{f}}, we
require a \ensuremath{\Conid{Multiplicable}} instance for every type that we want to use
\ensuremath{\Varid{multiply}} on inside the function:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{f}\mathbin{::}\Conid{Multiplicable}\;\Varid{a}\Rightarrow \Varid{a}\to \Varid{a}\to \Varid{a}{}\<[E]%
\\
\>[B]{}\Varid{f}\;\Varid{x}\;\Varid{y}\mathrel{=}\Varid{multiply}\;(\Varid{multiply}\;\Varid{x}\;\Varid{y})\;\Varid{x}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

Such a function \ensuremath{\Varid{f}} does work with the contract of \ensuremath{\Conid{Multiplicable}}
instead of requiring some specific type. Using this technique allows
many definitions in Haskell to be reused even though it is a statically
typed language.

We can also write type classes with more than one type parameter. This
allows for encapsulation of contracts of arbitrary complexity.
Furthermore, type classes can themselves have constraints placed on what
types are allowed. Both can be seen here:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}c<{\hspost}@{}}%
\column{5E}{@{}l@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{class}\;(\Conid{SomeClass}\;\Varid{a},\Conid{SomeOtherClass}\;\Varid{b})\Rightarrow \Conid{MyClass}\;\Varid{a}\;\Varid{b}\;\Varid{c}\;\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\mathbin{...}{}\<[5E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

\hypertarget{custom-types}{%
\subsubsection{Custom types}\label{custom-types}}

As in any mature programming language, Haskell programmers obviously do
not have to represent everything with only some base-set of types and
allows to introduce custom types. They are usually defined in three
different ways. For starters, we can give types aliases with the \ensuremath{\mathbf{type}}
keyword like


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mbox{\onelinecomment  Tuple of a and b}{}\<[E]%
\\
\>[B]{}\mathbf{type}\;\Conid{Tuple}\;\Varid{a}\;\Varid{b}\mathrel{=}(\Varid{a},\Varid{b}){}\<[E]%
\\[\blanklineskip]%
\>[B]{}\mbox{\onelinecomment  Tuple of ints}{}\<[E]%
\\
\>[B]{}\mathbf{type}\;\Conid{IntTuple}\mathrel{=}(\Conid{Int},\Conid{Int}){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

which are treated just like the original \ensuremath{(\Varid{a},\Varid{b})} or \ensuremath{(\Conid{Int},\Conid{Int})} would.
This means that we can use such types loosely and pass e.b. a
\ensuremath{\Conid{Tuple}\;\Conid{Int}\;\Conid{Int}} into a function \ensuremath{\Varid{f}\mathbin{::}(\Conid{Int},\Conid{Int})\to \mathbin{...}}. The same also
holds true for instances of type classes. Because aliases do not count
as new types, \ensuremath{\mathbf{type}} declarations are often used to make function types
easier to read.

The second way to declare types, \ensuremath{\mathbf{data}}, declares new-types as in actual
new types in the type system:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{7}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{data}\;\Conid{Direction}\mathrel{=}{}\<[E]%
\\
\>[B]{}\hsindent{7}{}\<[7]%
\>[7]{}\Conid{North}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\mid \Conid{NorthEast}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\mid \Conid{East}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\mid \Conid{SouthEast}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\mid \Conid{South}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\mid \Conid{SouthWest}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\mid \Conid{West}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\mid \Conid{NorthWest}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

The words starting with uppercase letters, e.g. \ensuremath{\Conid{North}} - \ensuremath{\Conid{NorthWest}},
are called constructors.

\ensuremath{\mathbf{data}} types are not limited to enum-style types, though. They can also
hold values, like the \ensuremath{\Conid{Maybe}\;\Varid{a}} type from Haskell. This type, which
\emph{may} hold a value \ensuremath{\Varid{a}} internally, is usually used as a return type
for functions which not always return an actual result (e.g.~in case of
a failure). We can define this type as follows:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mbox{\onelinecomment  unnamed field}{}\<[E]%
\\
\>[B]{}\mathbf{data}\;\Conid{Maybe}\;\Varid{a}\mathrel{=}\Conid{Just}\;\Varid{a}\mid \Conid{Nothing}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

Values are created by calling the constructors with the appropriate
parameters (if any), i.e.~when passed into a function: \ensuremath{\Varid{f}\;(\Conid{Just}\;\mathrm{1})}.
Furthermore, \ensuremath{\mathbf{data}} constructors can have named fields defined like


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{7}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mbox{\onelinecomment  named field}{}\<[E]%
\\
\>[B]{}\mathbf{data}\;\Conid{Maybe}\;\Varid{a}\mathrel{=}{}\<[E]%
\\
\>[B]{}\hsindent{7}{}\<[7]%
\>[7]{}\Conid{Just}\;\{\mskip1.5mu \Varid{theThing}\mathbin{::}\Varid{a}\mskip1.5mu\}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\mid \Conid{Nothing}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

where values are created by calling the constructor and passing the
appropriate parameters to the properties, i.e.
\ensuremath{\Varid{f}\;(\Conid{Maybe}\;\{\mskip1.5mu \Varid{theThing}\mathrel{=}\mathrm{1}\mskip1.5mu\})} The final way to define custom types is via
\ensuremath{\mathbf{newtype}}:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mbox{\onelinecomment  unnamed field}{}\<[E]%
\\
\>[B]{}\mathbf{newtype}\;\Conid{MyNewType}\;\Varid{a}\mathrel{=}\Conid{Constructor}\;\Varid{a}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\mbox{\onelinecomment  named field}{}\<[E]%
\\
\>[B]{}\mathbf{newtype}\;\Conid{MyOtherNewType}\;\Varid{a}\mathrel{=}\Conid{Constructor}\;\{\mskip1.5mu \Varid{myOnlyThing}\mathbin{::}\Varid{a}\mskip1.5mu\}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

Types declared this way are similar to \ensuremath{\mathbf{data}} types, but can only
contain a single constructor with just a single field. Also, unlike
\ensuremath{\mathbf{data}}, constructors declared with \ensuremath{\mathbf{newtype}} are strict instead of lazy,
meaning the compiler can optimize away the surrounding declaration.
Everything else is handled exactly like with \ensuremath{\mathbf{data}} types. The specifics
of what laziness or strictness means will be explained in the next
section of this chapter. \ensuremath{\mathbf{newtype}} types are also a useful tool if we
were to write a wrapper for a type while not wanting to inherit all
instances of type classes, but are also often used when declaring more
complicated types.

\hypertarget{lazy-evaluation}{%
\subsubsection{Lazy Evaluation}\label{lazy-evaluation}}

One thing that is not obvious when looking at the definitions from this
chapter is that Haskell is a lazy language\footnote{Haskell is actually
  defined as a non-strict language, meaning that only as much as
  required is evaluated, not when it is done. Laziness is just a way to
  achieve non-strictness. The same could be achieved with an eager, but
  non-strict evaluation mechanism. But as Haskell's main compilers all
  implement non-strictness via lazy evaluation, it is okay to call
  Haskell a lazy language here. See
  \url{https://wiki.haskell.org/Lazy_vs._non-strict}.}. This means that
values are only evaluated when required. This has one major benefit: We
get a Producer/Consumer pattern for free. For example if we have the
lazy function \ensuremath{\Varid{producer}\mathbin{::}\Conid{Int}\to [\mskip1.5mu \Conid{Int}\mskip1.5mu]} producing some list of integers
and some consumer consuming \ensuremath{\Varid{consumer}\mathbin{::}[\mskip1.5mu \Conid{Int}\mskip1.5mu]\to \Conid{Int}} this list. Then,
in a program \ensuremath{\Varid{consumer}\mathbin{\circ}\Varid{producer}}, \ensuremath{\Varid{producer}} generates the elements of
the result-list as they are consumed. This also means that, if
\ensuremath{\Varid{consumer}} only requires the first few elements of the list to compute
the result, \ensuremath{\Varid{producer}} does not produce unneeded results.

Laziness even allows us to express infinite streams, which can be
helpful. As an example, an infinite list of ones is defined with the
help of the list constructor \ensuremath{(\mathbin{:})\mathbin{::}\Varid{a}\to [\mskip1.5mu \Varid{a}\mskip1.5mu]\to [\mskip1.5mu \Varid{a}\mskip1.5mu]}, which prepends a
value to a list, as


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{ones}\mathbin{::}[\mskip1.5mu \Conid{Int}\mskip1.5mu]{}\<[E]%
\\
\>[B]{}\Varid{ones}\mathrel{=}\mathrm{1}\mathbin{:}\Varid{ones}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

or, if we require a list of some value at least \(n\) times so that it
can be consumed with some list of length \(n\), we can just use an
infinite list instead of computing the actual required amount (which
would take \(n\) steps for a linked list). The helper function for this
is called \ensuremath{\Varid{repeat}} and can be written as


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{repeat}\mathbin{::}\Varid{a}\to [\mskip1.5mu \Varid{a}\mskip1.5mu]{}\<[E]%
\\
\>[B]{}\Varid{repeat}\;\Varid{a}\mathrel{=}\Varid{a}\mathbin{:}(\Varid{repeat}\;\Varid{a}){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

Another good example where laziness simplifies things is when branching
is involved:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{18}{@{}>{\hspre}l<{\hspost}@{}}%
\column{21}{@{}>{\hspre}l<{\hspost}@{}}%
\column{25}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{calculateStuff}\mathbin{::}[\mskip1.5mu \Conid{Int}\mskip1.5mu]\to \Conid{Int}{}\<[E]%
\\
\>[B]{}\Varid{calculateStuff}\mathrel{=}\mathbf{if}\mathbin{<}\Varid{someCondition}\mathbin{>}{}\<[E]%
\\
\>[B]{}\hsindent{18}{}\<[18]%
\>[18]{}\mathbf{then}\;\Varid{doStuff}\;\Varid{list1}\;\Varid{list2}{}\<[E]%
\\
\>[B]{}\hsindent{18}{}\<[18]%
\>[18]{}\mathbf{else}\;\Varid{doSomeOtherStuff}\;\Varid{list1}{}\<[E]%
\\
\>[18]{}\hsindent{3}{}\<[21]%
\>[21]{}\mathbf{where}{}\<[E]%
\\
\>[21]{}\hsindent{4}{}\<[25]%
\>[25]{}\Varid{list1}\mathrel{=}\mathbin{...}{}\<[E]%
\\
\>[21]{}\hsindent{4}{}\<[25]%
\>[25]{}\Varid{list2}\mathrel{=}\mathbin{...}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

Here, \ensuremath{\Varid{list2}} is not required in both branches of the \ensuremath{\mathbf{if}} statement.
Thanks to laziness, it is therefore only evaluated upon a successful
if-check. While such a behaviour is obviously possible in non-lazy
languages, the elegance of the above definition is apparent. We can
define as many variables in the same clear way without having
unnecessary computations or code dealing with conditional computation
like nested \ensuremath{\mathbf{where}}s.

Usually laziness is beneficial to programs and programmers as it allows
for easy composition and better structure in code, but sometimes we
require more control about when something is evaluated. Haskell
therefore has several ways to control \emph{when} and \emph{how} values
are evaluated.

The most basic primitive to force values to be \emph{strict} instead of
lazy is \ensuremath{\Varid{seq}\mathbin{::}\Varid{a}\to \Varid{b}\to \Varid{b}}, which is by nature part of the compiler
and can not be expressed in Haskell directly. It's semantics however,
are as follows: We tell the compiler that the first argument (of type
\ensuremath{\Varid{a}}) is to be evaluated before the second argument. For example, in an
expression like


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{9}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{myFun}\mathbin{::}\Conid{Int}\to (\Conid{Int},\Conid{Int}){}\<[E]%
\\
\>[B]{}\Varid{myFun}\;\Varid{x}\mathrel{=}\mathbf{let}\;\Varid{y}\mathrel{=}\Varid{f}\;\Varid{x}\;\mathbf{in}\;\Varid{y}\mathbin{\Varid{`seq`}}\Varid{g}\;\Varid{y}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\mathbf{where}{}\<[E]%
\\
\>[5]{}\hsindent{4}{}\<[9]%
\>[9]{}\Varid{f}\mathrel{=}\mathbin{...}{}\<[E]%
\\
\>[5]{}\hsindent{4}{}\<[9]%
\>[9]{}\Varid{g}\mathrel{=}\mathbin{...}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

we can then at the compiler that we want \ensuremath{\Varid{y}\mathrel{=}\Varid{f}\;\Varid{x}} evaluated before
returning the (still non-evaluated) result of \ensuremath{\Varid{g}\;\Varid{y}}. This trick is
usually used if during profiling a big chunk of non-evaluated values are
noticed to aggregate before or in the process of evaluation of \ensuremath{\Varid{f}\;\Varid{x}}. As
this is a common pattern seen in Haskell programs, there exists the
strict function application operator \ensuremath{(\mathbin{\$!})\mathbin{::}(\Varid{a}\to \Varid{b})\to \Varid{a}\to \Varid{b}} to
encapsulate it. It is straightforwardly defined as:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}(\mathbin{\$!})\mathbin{::}(\Varid{a}\to \Varid{b})\to \Varid{a}\to \Varid{b}{}\<[E]%
\\
\>[B]{}\Varid{f}\mathbin{\$!}\Varid{x}\mathrel{=}\Varid{x}\mathbin{\Varid{`seq`}}\Varid{f}\;\Varid{x}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

With it, we can write our example function as


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{9}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{myFun}\mathbin{::}\Conid{Int}\to (\Conid{Int},\Conid{Int}){}\<[E]%
\\
\>[B]{}\Varid{myFun}\;\Varid{x}\mathrel{=}\Varid{g}\mathbin{\$!}\Varid{f}\;\Varid{x}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\mathbf{where}{}\<[E]%
\\
\>[5]{}\hsindent{4}{}\<[9]%
\>[9]{}\Varid{f}\mathrel{=}\mathbin{...}{}\<[E]%
\\
\>[5]{}\hsindent{4}{}\<[9]%
\>[9]{}\Varid{g}\mathrel{=}\mathbin{...}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

Note that the strictness implied by these two operations does not equal
\emph{complete} evaluation. They only force to weak-head-normal-form
(WHNF) meaning that evaluation is only forced until the outermost
constructor in contrast to normal-form (NF) which stands for full
evaluation. This means that if we were to evaluate some calculation
\ensuremath{\Varid{f}\;(\Varid{g}\;(\Varid{h}\;(\Varid{i}\;\Varid{x})))} embedded in some lazy tuple \ensuremath{(\Varid{y},\Varid{z})} to WHNF, \ensuremath{\Varid{y}} and
\ensuremath{\Varid{z}} would not be touched as the evaluation stops at the tuple
constructor (for more about constructors see the section on custom
types). All the computations that lead to this constructor however,
would be forced to be evaluated. Therefore, if we had wanted to make the
insides of a tuple strict, we would have to write something along the
lines of


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{let}\;\Varid{tup}\;@\;(\Varid{y},\Varid{z})\mathrel{=}\Varid{f}\;(\Varid{g}\;(\Varid{h}\;(\Varid{i}\;\Varid{x})))\;\mathbf{in}\;\Varid{y}\mathbin{\Varid{`seq`}}\Varid{z}\mathbin{\Varid{`seq`}}\Varid{tup}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

instead of just


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{let}\;\Varid{tup}\mathrel{=}\Varid{f}\;(\Varid{g}\;(\Varid{h}\;(\Varid{i}\;\Varid{x})))\;\mathbf{in}\;\Varid{y}\mathbin{\Varid{`seq`}}\Varid{y}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

But as \ensuremath{\Varid{seq}} and \ensuremath{\mathbin{\$!}} both only evaluate to WHNF, \ensuremath{\Varid{y}} and \ensuremath{\Varid{z}} might
still not be completely evaluated, since they could be of some more
complex type than just \ensuremath{\Conid{Int}} or any other primitive. This is the reason
why in the Haskell eco system, there exists the library
\ensuremath{\Varid{deepseq}}\footnote{See
  \url{haskell.org/package/deepseq-1.4.3.0/docs/Control-DeepSeq.html}.}
which comes with the type class \ensuremath{\Conid{NFData}} defined as


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{class}\;\Conid{NFData}\;\Varid{a}\;\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{rnf}\mathbin{::}\Varid{a}\to (){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

Instances of this type class for some type \ensuremath{\Varid{a}} are required to provide
an appropriate implementation of \ensuremath{\Varid{rnf}} for \emph{full} evaluation to
normal-form, where \ensuremath{\Varid{rnf}} stands for \enquote{reduce-to-normal-form}.
With this, we can then implement the NF equivalent to \ensuremath{\Varid{seq}}, \ensuremath{\Varid{deepseq}},
as


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{deepseq}\mathbin{::}\Conid{NFData}\;\Varid{a}\Rightarrow \Varid{a}\to \Varid{b}\to \Varid{b}{}\<[E]%
\\
\>[B]{}\Varid{deepseq}\;\Varid{a}\mathrel{=}\Varid{rnf}\;\Varid{a}\mathbin{\Varid{`seq`}}\Varid{a}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

A deep analogue to \ensuremath{\mathbin{\$!}} is then easily definable as well as


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}(\mathbin{\$!!})\mathbin{::}\Conid{NFData}\;\Varid{a}\Rightarrow (\Varid{a}\to \Varid{b})\to \Varid{a}\to \Varid{b}{}\<[E]%
\\
\>[B]{}\Varid{f}\mathbin{\$!!}\Varid{x}\mathrel{=}\Varid{x}\mathbin{`\Varid{deepseq}`}\Varid{f}\;\Varid{x}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

When dealing with WHNF and NF, note that in all computations annotated
with some forcing construct, be it \ensuremath{\Varid{seq}} or \ensuremath{\Varid{deepseq}}, laziness does not
go away entirely. All forced values, even the ones forced to NF, can
still be considered somewhat lazy as they are only forced when they are
requested. However, in practice, this is usually a desired property.

\hypertarget{pattern-matching}{%
\subsubsection{Pattern Matching}\label{pattern-matching}}

While we have seen pattern matching as an alternative to
\ensuremath{\mathbf{if}\mathbin{...}\mathbf{then}\mathbin{...}\mathbf{else}} and guard statements, it can do more things. For
example, if we have a datatype \ensuremath{\Conid{MyType}\;\Varid{a}} defined as


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{data}\;\Conid{MyType}\;\Varid{a}\mathrel{=}\Conid{SomeConstructor}\;\Varid{a}\mid \Conid{SomeOtherConstructor}\;\Varid{a}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

and we want to write a function \ensuremath{\Varid{unwrap}\mathbin{::}\Conid{MyType}\;\Varid{a}\to \Varid{a}} to unwrap the
\ensuremath{\Varid{a}} value, we use pattern matching like this:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{unwrap}\mathbin{::}\Conid{MyType}\;\Varid{a}\to \Varid{a}{}\<[E]%
\\
\>[B]{}\Varid{unwrap}\;(\Conid{SomeConstructor}\;\Varid{x})\mathrel{=}\Varid{x}{}\<[E]%
\\
\>[B]{}\Varid{unwrap}\;(\Conid{SomeOtherConstructor}\;\Varid{x})\mathrel{=}\Varid{x}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

This kind of unwrapping can also be done with the help of \ensuremath{\mathbf{case}}
statements so that we do not require a new function definition:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{someFunc}\mathbin{::}\Conid{MyType}\;\Varid{a}\to \Varid{a}{}\<[E]%
\\
\>[B]{}\Varid{someFunc}\;\Varid{t}\mathrel{=}\mathbf{case}\;\Varid{t}\;\mathbf{of}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}(\Conid{SomeConstructor}\;\Varid{a})\to \mathbin{...}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}(\Conid{SomeOtherConstructor}\;\Varid{a})\to \mathbin{...}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

In Haskell programs, we can also write unwrapping code with the help of
the \ensuremath{\mathbf{let}} notation for single constructor types like\footnote{It is
  possible to do this for types with more than one constructor as well,
  but this will lead to errors at runtime unless we can ensure that the
  matched constructor is the correct one.}:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{f}\mathbin{::}(\Conid{Double},\Conid{Double})\to \Conid{Double}{}\<[E]%
\\
\>[B]{}\Varid{f}\;\Varid{vec2d}\mathrel{=}\mathbf{let}\;(\Varid{x},\Varid{y})\mathrel{=}\Varid{vec2d}\;\mathbf{in}\;\Varid{sqrt}\;(\Varid{x}\mathbin{*}\Varid{x}\mathbin{+}\Varid{y}\mathbin{*}\Varid{y}){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

Predictably, we can do this with \ensuremath{\mathbf{where}} as well:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{f}\mathbin{::}(\Conid{Double},\Conid{Double})\to \Conid{Double}{}\<[E]%
\\
\>[B]{}\Varid{f}\;\Varid{vec2d}\mathrel{=}\Varid{sqrt}\;(\Varid{x}\mathbin{*}\Varid{x}\mathbin{+}\Varid{y}\mathbin{*}\Varid{y}){}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\mathbf{where}\;(\Varid{x},\Varid{y})\mathrel{=}\Varid{vec2d}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

Sometimes, we only care about some part of the value. For example, in a
definition of \ensuremath{\Varid{maybeHead}\mathbin{::}[\mskip1.5mu \Varid{a}\mskip1.5mu]\to \Conid{Maybe}\;\Varid{a}}, which should return the
first element of the list or \ensuremath{\Conid{Nothing}} if it is an empty list \ensuremath{[\mskip1.5mu \mskip1.5mu]}, we
can write this with the help of wildcards (\ensuremath{\anonymous }) as:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{maybeHead}\mathbin{::}[\mskip1.5mu \Varid{a}\mskip1.5mu]\to \Conid{Maybe}\;\Varid{a}{}\<[E]%
\\
\>[B]{}\Varid{maybeHead}\;(\Varid{x}\mathbin{:}\anonymous )\mathrel{=}\Conid{Just}\;\Varid{x}{}\<[E]%
\\
\>[B]{}\Varid{maybeHead}\;[\mskip1.5mu \mskip1.5mu]\mathrel{=}\Conid{Nothing}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

We could even write


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{maybeHead}\mathbin{::}[\mskip1.5mu \Varid{a}\mskip1.5mu]\to \Conid{Maybe}\;\Varid{a}{}\<[E]%
\\
\>[B]{}\Varid{maybeHead}\;(\Varid{x}\mathbin{:}\anonymous )\mathrel{=}\Conid{Just}\;\Varid{x}{}\<[E]%
\\
\>[B]{}\Varid{maybeHead}\;\anonymous \mathrel{=}\Conid{Nothing}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

as the second equation will only ever match when the list is
empty.\footnote{A single element list has two forms in Haskell, \ensuremath{\Varid{a}\mathbin{:}[\mskip1.5mu \mskip1.5mu]}
  and \ensuremath{[\mskip1.5mu \Varid{a}\mskip1.5mu]} of which the latter is just syntactic sugar of the former.}
Furthermore, functions \ensuremath{\Varid{isJust}\mathbin{::}\Conid{Maybe}\;\Varid{a}\to \Conid{Bool}}, if they only care
about the structure of the type, can be written with only wildcards:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{isJust}\mathbin{::}\Conid{Maybe}\;\Varid{a}\to \Conid{Bool}{}\<[E]%
\\
\>[B]{}\Varid{isJust}\;(\Conid{Just}\;\anonymous )\mathrel{=}\Conid{True}{}\<[E]%
\\
\>[B]{}\Varid{isJust}\;\anonymous \mathrel{=}\Conid{False}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

Additionally, if we want to preserve laziness and we are sure that a
match will work (e.g.~if we have called \ensuremath{\Varid{isJust}}), we can use
irrefutable patterns like \ensuremath{\mathord{\sim}(\Conid{Just}\;\Varid{a})\mathrel{=}\Varid{someMaybe}}.

\hypertarget{lambdas-and-partial-application}{%
\subsubsection{Lambdas and Partial
application}\label{lambdas-and-partial-application}}

As Functions are just another type that can be passed into other
(higher-order) functions, it makes sense to have a short-hand to write
anonymous functions -- lambdas. In Haskell, they look like this:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\lambda (\Varid{a},\Varid{b})\to \Varid{a}\mathbin{+}\Varid{b}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

Lambdas can easily be passed into functions, like \ensuremath{\Varid{zipWith}}\footnote{While
  \ensuremath{(\lambda (\Varid{a},\Varid{b})\to \Varid{a}\mathbin{+}\Varid{b})} is obviously the same as (+), we just write it as
  a lambda here for demonstration purposes.}:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{someFunc}\mathbin{::}[\mskip1.5mu \Conid{Int}\mskip1.5mu]\to [\mskip1.5mu \Conid{Int}\mskip1.5mu]\to [\mskip1.5mu \Conid{Int}\mskip1.5mu]{}\<[E]%
\\
\>[B]{}\Varid{someFunc}\;\Varid{xs}\;\Varid{ys}\mathrel{=}\Varid{zipWith}\;(\lambda (\Varid{a},\Varid{b})\to \Varid{a}\mathbin{+}\Varid{b})\;\Varid{xs}\;\Varid{ys}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

Here, we notice the reason for yet another feature in Haskell that is
commonly used: Partial application. While the definition of \ensuremath{\Varid{someFunc}}
is definitely not wrong, we could have written it more elegantly as


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{someFunc}\mathbin{::}[\mskip1.5mu \Conid{Int}\mskip1.5mu]\to [\mskip1.5mu \Conid{Int}\mskip1.5mu]\to [\mskip1.5mu \Conid{Int}\mskip1.5mu]{}\<[E]%
\\
\>[B]{}\Varid{someFunc}\mathrel{=}\Varid{zipWith}\;(\lambda (\Varid{a},\Varid{b})\to \Varid{a}\mathbin{+}\Varid{b}){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

where this means that \ensuremath{\Varid{someFunc}} is defined as
\ensuremath{\Varid{zipWith}\mathbin{::}(\Varid{a}\to \Varid{b}\to \Varid{c})\to [\mskip1.5mu \Varid{a}\mskip1.5mu]\to [\mskip1.5mu \Varid{b}\mskip1.5mu]\to [\mskip1.5mu \Varid{c}\mskip1.5mu])} partially applied with
the passed lambda to get a function with type \ensuremath{[\mskip1.5mu \Varid{a}\mskip1.5mu]\to [\mskip1.5mu \Varid{b}\mskip1.5mu]\to [\mskip1.5mu \Varid{c}\mskip1.5mu]} which
the compiler then automatically binds to the type of
\ensuremath{\Varid{someFunc}\mathbin{::}[\mskip1.5mu \Conid{Int}\mskip1.5mu]\to [\mskip1.5mu \Conid{Int}\mskip1.5mu]\to [\mskip1.5mu \Conid{Int}\mskip1.5mu]}.

\hypertarget{monads}{%
\subsection{Monads}\label{monads}}

\label{sec:monads}

Functional programmers try to avoid mutable state at all cost, but
programs that do not only just compute some function usually involve
some sort of it. So, doesn't this make Haskell useless being a pure
functional language without \emph{any} mutable state? No.~Functional
Programs generally just avoid \emph{unnecessary} mutable state at all
cost. The fact of the matter is that in functional programming, we can
represent mutable state as well, but we do so in a meaningful and
controlled manner.

While we could we could represent state in most computations by passing
it into every function that can possibly change it and returning it
alongside of the actual returned value like


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{comp}\mathbin{::}\Conid{MyState}\to \Conid{Int}\to (\Conid{Int},\Conid{MyState}){}\<[E]%
\\
\>[B]{}\Varid{comp}\;\Varid{curState}\;\Varid{x}\mathrel{=}(\Varid{x}\mathbin{+}\mathrm{3},\Varid{nextState}){}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\mathbf{where}\;\Varid{nextState}\mathrel{=}\Varid{changeState}\;\Varid{curState}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

this can become unnecessarily complicated to handle by hand. A better
alternative is the use of Monads, which are the main concept generally
used in computations involving some sort of mutable state. The type
class for a \ensuremath{\Conid{Monad}} can be defined as


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{class}\;\Conid{Monad}\;\Varid{m}\;\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}(\bind )\mathbin{::}\Varid{m}\;\Varid{a}\to (\Varid{a}\to \Varid{m}\;\Varid{b})\to \Varid{m}\;\Varid{b}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}(\sequ )\mathbin{::}\Varid{m}\;\Varid{a}\to \Varid{m}\;\Varid{b}\to \Varid{m}\;\Varid{b}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{m}\sequ \Varid{k}\mathrel{=}\Varid{m}\bind \mathbin{\char92 \char95 }\to \Varid{k}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{return}\mathbin{::}\Varid{a}\to \Varid{m}\;\Varid{a}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

Thinking of Monads as computations, we can come up with the following
explanation: \ensuremath{\Varid{return}} is used to create a computation \ensuremath{\Varid{m}\;\Varid{a}} just
returning the given value \ensuremath{\Varid{a}}. Next, \ensuremath{(\bind )} is used to compose some
monadic computation \ensuremath{\Varid{m}\;\Varid{a}} resulting in some \ensuremath{\Varid{a}} with a monadic function
\ensuremath{\Varid{a}\to \Varid{m}\;\Varid{b}} to return some computation \ensuremath{\Varid{m}\;\Varid{b}} resulting in some \ensuremath{\Varid{b}}.
Finally, \ensuremath{(\sequ )} is used to define the order of two monadic computations
\ensuremath{\Varid{m}\;\Varid{a}} and \ensuremath{\Varid{m}\;\Varid{b}} so that \ensuremath{\Varid{m}\;\Varid{a}} is computed before \ensuremath{\Varid{m}\;\Varid{b}} while discarding
the result of the first one as can also be seen in its default
implementation above.

Given this definition of a Monad, we can now take a look at how we would
implement a \ensuremath{\Conid{State}} Monad. Its type is defined as (Michaelson,
\protect\hyperlink{ref-learnyouahaskell}{2013})


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{newtype}\;\Conid{State}\;\Varid{s}\;\Varid{a}\mathrel{=}\Conid{State}\;\{\mskip1.5mu \Varid{runState}\mathbin{::}\Varid{s}\to (\Varid{a},\Varid{s})\mskip1.5mu\}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

\ensuremath{\Conid{State}\;\Varid{s}\;\Varid{a}} encapsulates a stateful computation on some state type \ensuremath{\Varid{s}}
yielding some value of type \ensuremath{\Varid{a}}. For easier understanding it is often
useful to think of \ensuremath{\Conid{State}\;\Varid{s}\;\Varid{a}} as a usability wrapper around a function
\ensuremath{\Varid{s}\to (\Varid{a},\Varid{s})} that returns some \ensuremath{\Varid{a}} and the final state \ensuremath{\Varid{s}} if we pass
it some starting state \ensuremath{\Varid{s}}. The State Monad therefore merely contains
the \enquote{blueprint} of the computation that can only be run if we
start it by passing a state. The instance for the \ensuremath{\Conid{Monad}} type class can
then be defined as


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{9}{@{}>{\hspre}l<{\hspost}@{}}%
\column{13}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{instance}\;\Conid{Monad}\;(\Conid{State}\;\Varid{s})\;\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}(\Conid{State}\;\Varid{h})\bind \Varid{f}\mathrel{=}\Conid{State}\mathbin{\$}\lambda \Varid{s}\to {}\<[E]%
\\
\>[5]{}\hsindent{4}{}\<[9]%
\>[9]{}\mathbf{let}\;(\Varid{a},\Varid{newState})\mathrel{=}\Varid{h}\;\Varid{s}{}\<[E]%
\\
\>[9]{}\hsindent{4}{}\<[13]%
\>[13]{}(\Conid{State}\;\Varid{g})\mathrel{=}\Varid{f}\;\Varid{a}{}\<[E]%
\\
\>[5]{}\hsindent{4}{}\<[9]%
\>[9]{}\mathbf{in}\;{}\<[13]%
\>[13]{}\Varid{g}\;\Varid{newState}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{return}\;\Varid{x}\mathrel{=}\Conid{State}\;\{\mskip1.5mu \Varid{runState}\mathrel{=}\lambda \Varid{s}\to (\Varid{x},\Varid{s})\mskip1.5mu\}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

Here, we declare the instance deliberately on top of \ensuremath{\Conid{State}\;\Varid{s}} meaning
that \ensuremath{\Conid{State}} itself is not a Monad, but it is a one together with some
state representation \ensuremath{\Varid{s}}.\footnote{We can not declare \ensuremath{\Conid{State}} a Monad
  anyways since \ensuremath{\Conid{Monad}} is a type class with just one type parameter.}
Note how the operations are defined here: \ensuremath{\Varid{return}} encapsulates the
given value \ensuremath{\Varid{x}\mathbin{::}\Varid{a}} inside the internal function and therefore is equal
to the identity \ensuremath{\Varid{id}\mathbin{::}\Varid{a}\to \Varid{a}} function on tuples with one parameter
already applied.\footnote{With the help of
  \ensuremath{\Varid{curry}\mathbin{::}((\Varid{a},\Varid{b})\to \Varid{c})\to \Varid{a}\to \Varid{b}\to \Varid{c}}, we could have therefore also
  written \ensuremath{\Varid{return}\;\Varid{x}\mathrel{=}\Conid{State}\;\{\mskip1.5mu \Varid{runState}\mathrel{=}(\Varid{curry}\;\Varid{id})\;\Varid{x}\mskip1.5mu\}}.} In the
composition operator \ensuremath{\bind }, the monadic computation
\ensuremath{\Conid{State}\;\Varid{h}\mathbin{::}\Conid{State}\;\Varid{s}\;\Varid{a}} is composed with the function
\ensuremath{\Varid{f}\mathbin{::}\Varid{a}\to \Conid{State}\;\Varid{s}\;\Varid{b}} into a new monadic computation of type
\ensuremath{\Conid{State}\;\Varid{s}\;\Varid{b}}. The internal function of the state is essentially taken out
of the first argument and composed with the second argument inside the
returned Monad.

Additionally, we can define helper operations to use this construct
with. The first one is \ensuremath{\Varid{put}\mathbin{::}\Varid{s}\to \Conid{State}\;\Varid{s}\;()}. It overwrites the
current state returning a unit \ensuremath{()} as result:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{put}\mathbin{::}\Varid{s}\to \Conid{State}\;\Varid{s}\;(){}\<[E]%
\\
\>[B]{}\Varid{put}\;\Varid{newState}\mathrel{=}\Conid{State}\;\{\mskip1.5mu \Varid{runState}\mathrel{=}\lambda \Varid{s}\to ((),\Varid{newState})\mskip1.5mu\}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

The second one is \ensuremath{\Varid{get}\mathbin{::}\Conid{State}\;\Varid{s}\;\Varid{s}}. It returns the current state, but
does not change it:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{get}\mathbin{::}\Conid{State}\;\Varid{s}\;\Varid{s}{}\<[E]%
\\
\>[B]{}\Varid{get}\mathrel{=}\Conid{State}\;\{\mskip1.5mu \Varid{runState}\mathrel{=}\lambda \Varid{s}\to (\Varid{s},\Varid{s})\mskip1.5mu\}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

With these operations, we can easily write stateful programs
like\footnote{Inspired and adapted from
  \url{https://gist.github.com/sdiehl/8d991a718f7a9c80f54b}.}


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{19}{@{}>{\hspre}l<{\hspost}@{}}%
\column{21}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{type}\;\Conid{Stack}\mathrel{=}[\mskip1.5mu \Conid{Int}\mskip1.5mu]{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{empty}\mathbin{::}\Conid{Stack}{}\<[E]%
\\
\>[B]{}\Varid{empty}\mathrel{=}[\mskip1.5mu \mskip1.5mu]{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{pop}\mathbin{::}\Conid{State}\;\Conid{Stack}\;\Conid{Int}{}\<[E]%
\\
\>[B]{}\Varid{pop}\mathrel{=}\Varid{get}\bind (\lambda (\Varid{x}\mathbin{:}\Varid{xs})\to \Varid{put}\;\Varid{xs}\sequ \Varid{return}\;\Varid{x}){}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{push}\mathbin{::}\Conid{Int}\to \Conid{State}\;\Conid{Stack}\;(){}\<[E]%
\\
\>[B]{}\Varid{push}\;\Varid{a}\mathrel{=}\Conid{State}\mathbin{\$}\lambda \Varid{xs}\to ((),\Varid{a}\mathbin{:}\Varid{xs}){}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{peek}\mathbin{::}\Conid{State}\;\Conid{Stack}\;\Conid{Int}{}\<[E]%
\\
\>[B]{}\Varid{peek}\mathrel{=}\Varid{get}\bind \lambda (\Varid{x}\mathbin{:}\Varid{xs})\to \Varid{return}\;\Varid{x}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{computeStateful}\mathbin{::}\Conid{State}\;\Conid{Stack}\;\Conid{Int}{}\<[E]%
\\
\>[B]{}\Varid{computeStateful}\mathrel{=}\Varid{push}\;\mathrm{10}\sequ {}\<[E]%
\\
\>[B]{}\hsindent{19}{}\<[19]%
\>[19]{}\Varid{push}\;\mathrm{20}\sequ {}\<[E]%
\\
\>[B]{}\hsindent{19}{}\<[19]%
\>[19]{}\Varid{pop}\bind \lambda \Varid{a}\to {}\<[E]%
\\
\>[19]{}\hsindent{2}{}\<[21]%
\>[21]{}(\Varid{pop}\bind \lambda \Varid{b}\to \Varid{push}\;(\Varid{a}\mathbin{+}\Varid{b}))\sequ {}\<[E]%
\\
\>[B]{}\hsindent{19}{}\<[19]%
\>[19]{}\Varid{peek}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\mbox{\onelinecomment  main program inside the IO Monad          }{}\<[E]%
\\
\>[B]{}\Varid{main}\mathbin{::}\Conid{IO}\;(){}\<[E]%
\\
\>[B]{}\Varid{main}\mathrel{=}\Varid{print}\;(\Varid{evalState}\;\Varid{computeStateful}\;\Varid{empty}){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

Here, \ensuremath{\Varid{computeStateful}} first pushes some values on top of a stack
represented by a list \ensuremath{[\mskip1.5mu \Conid{Int}\mskip1.5mu]} (the actual state inside of the \ensuremath{\Conid{State}}
Monad) and then \ensuremath{\Varid{pop}}s these values and \ensuremath{\Varid{push}}es their sum back on the
stack. Finally, we \ensuremath{\Varid{peek}} the top of our stack. This is then the result
of the computation. To make writing such code easier, Haskell has
syntactic sugar: The \ensuremath{\mathbf{do}} notation. With it, we can write the above
method \ensuremath{\Varid{computeStateful}} in a way that resembles imperative-style code
(but with side-effects clearly encapsulated) as:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{computeStateful}\mathbin{::}\Conid{State}\;\Conid{Stack}\;\Conid{Int}{}\<[E]%
\\
\>[B]{}\Varid{computeStateful}\mathrel{=}\mathbf{do}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{push}\;\mathrm{10}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{push}\;\mathrm{20}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{a}\leftarrow \Varid{pop}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{b}\leftarrow \Varid{pop}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{push}\;(\Varid{a}\mathbin{+}\Varid{b}){}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{peek}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

Here, we can also see the duality of \ensuremath{(\sequ )} and simple new lines as well
as the one between \ensuremath{(\bind )} and the special \ensuremath{\leftarrow } operator in \ensuremath{\mathbf{do}}
notation which facilitates the binding to a variable.\footnote{\ensuremath{(\bind )}
  is also often called \ensuremath{\Varid{bind}} in languages which do not support custom
  operators.}

Other often used Monads in the Haskell eco-system include the \ensuremath{\Conid{Writer}}
Monad, which is useful for e.g.~logging, or the \ensuremath{\Conid{IO}} Monad, which is
used to encapsulate I/O computations as well as low level internal
operations such as the usage of modifiable variables \ensuremath{\Conid{IORef}} or \ensuremath{\Conid{MVar}}
among others. Furthermore, as one of many other applications, Monads are
used in some parallel Haskells as we will see later in this thesis.

\hypertarget{arrows-1}{%
\subsection{Arrows}\label{arrows-1}}

\label{sec:arrows}

Arrows were introduced by Hughes
(\protect\hyperlink{ref-HughesArrows}{2000}) as a general interface for
computation and a less restrictive generalisation of Monads. Hughes
(\protect\hyperlink{ref-HughesArrows}{2000}) motivates the broader
interface of Arrows with the example of a parser with added static
meta-information that can not satisfy the monadic bind operator
\ensuremath{(\bind )\mathbin{::}\Conid{Monad}\;\Varid{m}\Rightarrow \Varid{m}\;\Varid{a}\to (\Varid{a}\to \Varid{m}\;\Varid{b})\to \Varid{m}\;\Varid{b}}.\footnote{In the example,
  a parser of the type \ensuremath{\Conid{Parser}\;\Varid{s}\;\Varid{a}} with static meta information \ensuremath{\Varid{s}} and
  result \ensuremath{\Varid{a}} is shown to not be able to use the static \ensuremath{\Varid{s}} without
  applying the monadic function \ensuremath{\Varid{a}\to \Varid{m}\;\Varid{b}}. With Arrows this is
  possible.}

An Arrow \ensuremath{\Varid{arr}\;\Varid{a}\;\Varid{b}} represents a computation that converts an input \ensuremath{\Varid{a}}
to an output \ensuremath{\Varid{b}}. The general concept is defined in the \ensuremath{\Conid{Arrow}} type
class shown in Figure \ref{fig:ArrowDefinition}. To lift an ordinary
function to an Arrow, \ensuremath{\Varid{arr}} is used, analogous to the monadic \ensuremath{\Varid{return}}.
Similarly, the composition operator \ensuremath{\mathbin{>\!\!>\!\!>}} is analogous to the monadic
composition \ensuremath{\bind } and combines two Arrows \ensuremath{\Varid{arr}\;\Varid{a}\;\Varid{b}} and \ensuremath{\Varid{arr}\;\Varid{b}\;\Varid{c}} by
\enquote{wiring} the outputs of the first to the inputs to the second to
get a new Arrow \ensuremath{\Varid{arr}\;\Varid{a}\;\Varid{c}}. Lastly, the \ensuremath{\Varid{first}} operator takes the input
Arrow \ensuremath{\Varid{arr}\;\Varid{a}\;\Varid{b}} and converts it into an Arrow on pairs
\ensuremath{\Varid{arr}\;(\Varid{a},\Varid{c})\;(\Varid{b},\Varid{c})} that leaves the second argument untouched. It allows
us to to save input across Arrows. Figure \ref{fig:arrows-viz} shows a
graphical representation of these basic Arrow combinators. The most
prominent instances of this interface (Figure \ref{fig:ArrowDefinition})
are regular functions \ensuremath{(\to )} and the Kleisli type, which wraps monadic
functions, e.g. \ensuremath{\Varid{a}\to \Varid{m}\;\Varid{b}}.\footnote{In \ref{sec:relWorkArrows} we
  referenced further relevant Arrow types, especially ones that can be
  used for Arrow-based functional reactive programming.}

\begin{figure}[t]
\centering
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{class}\;\Conid{Arrow}\;\Varid{arr}\;\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Varid{arr}\mathbin{::}(\Varid{a}\to \Varid{b})\to \Varid{arr}\;\Varid{a}\;\Varid{b}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}(\mathbin{>\!\!>\!\!>})\mathbin{::}\Varid{arr}\;\Varid{a}\;\Varid{b}\to \Varid{arr}\;\Varid{b}\;\Varid{c}\to \Varid{arr}\;\Varid{a}\;\Varid{c}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Varid{first}\mathbin{::}\Varid{arr}\;\Varid{a}\;\Varid{b}\to \Varid{arr}\;(\Varid{a},\Varid{c})\;(\Varid{b},\Varid{c}){}\<[E]%
\\[\blanklineskip]%
\>[B]{}\mathbf{instance}\;\Conid{Arrow}\;(\to )\;\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{arr}\;\Varid{f}\mathrel{=}\Varid{f}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{f}\mathbin{>\!\!>\!\!>}\Varid{g}\mathrel{=}\Varid{g}\mathbin{\circ}\Varid{f}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{first}\;\Varid{f}\mathrel{=}\lambda (\Varid{a},\Varid{c})\to (\Varid{f}\;\Varid{a},\Varid{c}){}\<[E]%
\\[\blanklineskip]%
\>[B]{}\mathbf{data}\;\Conid{Kleisli}\;\Varid{m}\;\Varid{a}\;\Varid{b}\mathrel{=}\Conid{Kleisli}\;\{\mskip1.5mu \Varid{run}\mathbin{::}\Varid{a}\to \Varid{m}\;\Varid{b}\mskip1.5mu\}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\mathbf{instance}\;\Conid{Monad}\;\Varid{m}\Rightarrow \Conid{Arrow}\;(\Conid{Kleisli}\;\Varid{m})\;\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{arr}\;\Varid{f}\mathrel{=}\Conid{Kleisli}\;(\Varid{return}\mathbin{\circ}\Varid{f}){}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{f}\mathbin{>\!\!>\!\!>}\Varid{g}\mathrel{=}\Conid{Kleisli}\;(\lambda \Varid{a}\to \Varid{f}\;\Varid{a}\bind \Varid{g}){}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{first}\;\Varid{f}\mathrel{=}\Conid{Kleisli}\;(\lambda (\Varid{a},\Varid{c})\to \Varid{f}\;\Varid{a}\bind \lambda \Varid{b}\to \Varid{return}\;(\Varid{b},\Varid{c})){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\caption{The \ensuremath{\Conid{Arrow}} type class and its two most typical instances.}\label{fig:ArrowDefinition}\end{figure}

\begin{figure}
\centering
\includegraphics{src/img/arrows-viz.pdf}
\caption[Schematic depiction of an Arrow and its basic
combinators \ensuremath{\Varid{arr}}, \ensuremath{\mathbin{>\!\!>\!\!>}} and \ensuremath{\Varid{first}}.]{Schematic depiction of an Arrow (left) and its basic
combinators \ensuremath{\Varid{arr}}, \ensuremath{\mathbin{>\!\!>\!\!>}} and \ensuremath{\Varid{first}} (right).\label{fig:arrows-viz}}
\end{figure}

\begin{figure}
\centering
\includegraphics{src/img/syntacticSugarArrows.pdf}
\caption{Visual depiction of syntactic sugar for
Arrows.\label{fig:syntacticSugarArrows}}
\end{figure}

Hughes also defined some syntactic sugar (Figure
\ref{fig:syntacticSugarArrows}): \ensuremath{\Varid{second}}, \ensuremath{\mathbin{*\!*\!*}} and \ensuremath{\mathbin{\&\!\&\!\&}}. \ensuremath{\Varid{second}} is
the mirrored version of \ensuremath{\Varid{first}}:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{second}\mathbin{::}\Conid{Arrow}\;\Varid{arr}\Rightarrow \Varid{arr}\;\Varid{a}\;\Varid{b}\to \Varid{arr}\;(\Varid{c},\Varid{a})\;(\Varid{c},\Varid{b}){}\<[E]%
\\
\>[B]{}\Varid{second}\;\Varid{f}\mathrel{=}\Varid{arr}\;\Varid{swap}\mathbin{>\!\!>\!\!>}\Varid{first}\;\Varid{f}\mathbin{>\!\!>\!\!>}\Varid{arr}\;\Varid{swap}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\mathbf{where}\;\Varid{swap}\;(\Varid{x},\Varid{y})\mathrel{=}(\Varid{y},\Varid{x}){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

The \ensuremath{\mathbin{*\!*\!*}} function combines \ensuremath{\Varid{first}} and \ensuremath{\Varid{second}} to handle two inputs in
one Arrow, and is defined as follows:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}(\mathbin{*\!*\!*})\mathbin{::}\Conid{Arrow}\;\Varid{arr}\Rightarrow \Varid{arr}\;\Varid{a}\;\Varid{b}\to \Varid{arr}\;\Varid{c}\;\Varid{d}\to \Varid{arr}\;(\Varid{a},\Varid{c})\;(\Varid{b},\Varid{d}){}\<[E]%
\\
\>[B]{}\Varid{f}\mathbin{*\!*\!*}\Varid{g}\mathrel{=}\Varid{first}\;\Varid{f}\mathbin{>\!\!>\!\!>}\Varid{second}\;\Varid{g}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

The \ensuremath{\mathbin{\&\!\&\!\&}} combinator, which constructs an Arrow that outputs two
different values like \ensuremath{\mathbin{*\!*\!*}}, but takes only one input, is:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}(\mathbin{\&\!\&\!\&})\mathbin{::}\Conid{Arrow}\;\Varid{arr}\Rightarrow \Varid{arr}\;\Varid{a}\;\Varid{b}\to \Varid{arr}\;\Varid{a}\;\Varid{c}\to \Varid{arr}\;\Varid{a}\;(\Varid{b},\Varid{c}){}\<[E]%
\\
\>[B]{}\Varid{f}\mathbin{\&\!\&\!\&}\Varid{g}\mathrel{=}\Varid{arr}\;(\lambda \Varid{a}\to (\Varid{a},\Varid{a}))\mathbin{>\!\!>\!\!>}\Varid{f}\mathbin{*\!*\!*}\Varid{g}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

A first short example given by Hughes on how to use the Arrow interface
is the addition of results of two generic Arrows in a new Arrow:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{add}\mathbin{::}\Conid{Arrow}\;\Varid{arr}\Rightarrow \Varid{arr}\;\Varid{a}\;\Conid{Int}\to \Varid{arr}\;\Varid{a}\;\Conid{Int}\to \Varid{arr}\;\Varid{a}\;\Conid{Int}{}\<[E]%
\\
\>[B]{}\Varid{add}\;\Varid{f}\;\Varid{g}\mathrel{=}\Varid{f}\mathbin{\&\!\&\!\&}\Varid{g}\mathbin{>\!\!>\!\!>}\Varid{arr}\;(\lambda (\Varid{u},\Varid{v})\to \Varid{u}\mathbin{+}\Varid{v}){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

As we can rewrite the monadic bind operation \ensuremath{(\bind )} with only the
Kleisli type into \ensuremath{\Varid{m}\;\Varid{a}\to \Conid{Kleisli}\;\Varid{m}\;\Varid{a}\;\Varid{b}\to \Varid{m}\;\Varid{b}}, but not with a general
Arrow \ensuremath{\Varid{arr}\;\Varid{a}\;\Varid{b}}, we can intuitively get an idea of why Arrows must be a
generalisation of Monads. While this also means that a general Arrow can
not express everything a Monad can, Hughes
(\protect\hyperlink{ref-HughesArrows}{2000}) shows in his parser example
that this trade-off is worth it in some cases.

\hypertarget{utility-combinators}{%
\subsubsection{Utility Combinators}\label{utility-combinators}}

\label{utilfns}

In order to ease the use of Arrows, we will now define some utility
Arrow combinators, namely \ensuremath{\Varid{evalN}} as well as \ensuremath{\Varid{mapArr}}. \ensuremath{\Varid{evalN}}, which
turns a list of Arrows \ensuremath{[\mskip1.5mu \Varid{arr}\;\Varid{a}\;\Varid{b}\mskip1.5mu]} into a new Arrow \ensuremath{\Varid{arr}\;[\mskip1.5mu \Varid{a}\mskip1.5mu]\;[\mskip1.5mu \Varid{b}\mskip1.5mu]}
evaluating a list of inputs \ensuremath{[\mskip1.5mu \Varid{a}\mskip1.5mu]} against these Arrows, is defined in
Figure \ref{fig:evalN}

\begin{figure}[h]
\centering
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{10}{@{}>{\hspre}l<{\hspost}@{}}%
\column{16}{@{}>{\hspre}l<{\hspost}@{}}%
\column{32}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{evalN}\mathbin{::}(\Conid{ArrowChoice}\;\Varid{arr})\Rightarrow [\mskip1.5mu \Varid{arr}\;\Varid{a}\;\Varid{b}\mskip1.5mu]\to \Varid{arr}\;[\mskip1.5mu \Varid{a}\mskip1.5mu]\;[\mskip1.5mu \Varid{b}\mskip1.5mu]{}\<[E]%
\\
\>[B]{}\Varid{evalN}\;(\Varid{f}\mathbin{:}\Varid{fs})\mathrel{=}\Varid{arr}\;\Varid{listcase}\mathbin{>\!\!>\!\!>}{}\<[E]%
\\
\>[B]{}\hsindent{10}{}\<[10]%
\>[10]{}\Varid{arr}\;(\Varid{const}\;[\mskip1.5mu \mskip1.5mu])\mathbin{\mid\!\mid\!\mid}(\Varid{f}\mathbin{*\!*\!*}\Varid{evalN}\;\Varid{fs}\mathbin{>\!\!>\!\!>}\Varid{arr}\;(\Varid{uncurry}\;(\mathbin{:}))){}\<[E]%
\\
\>[B]{}\hsindent{10}{}\<[10]%
\>[10]{}\mathbf{where}\;\Varid{listcase}\;[\mskip1.5mu \mskip1.5mu]{}\<[32]%
\>[32]{}\mathrel{=}\Conid{Left}\;(){}\<[E]%
\\
\>[10]{}\hsindent{6}{}\<[16]%
\>[16]{}\Varid{listcase}\;(\Varid{x}\mathbin{:}\Varid{xs})\mathrel{=}\Conid{Right}\;(\Varid{x},\Varid{xs}){}\<[E]%
\\
\>[B]{}\Varid{evalN}\;[\mskip1.5mu \mskip1.5mu]\mathrel{=}\Varid{arr}\;(\Varid{const}\;[\mskip1.5mu \mskip1.5mu]){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\caption{The definition of \ensuremath{\Varid{evalN}}.}\label{fig:evalN}\end{figure}

This combinator combinators makes use of the \ensuremath{\Conid{ArrowChoice}} type class
providing the \ensuremath{\mathbin{\mid\!\mid\!\mid}} combinator. This combinator takes two
Arrows \ensuremath{\Varid{arr}\;\Varid{a}\;\Varid{c}} and \ensuremath{\Varid{arr}\;\Varid{b}\;\Varid{c}} and combines them into a new Arrow
\ensuremath{\Varid{arr}\;(\Conid{Either}\;\Varid{a}\;\Varid{b})\;\Varid{c}} which pipes all \ensuremath{\Conid{Left}\;\Varid{a}}'s to the first Arrow and
all \ensuremath{\Conid{Right}\;\Varid{b}}'s to the second Arrow:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}(\mathbin{\mid\!\mid\!\mid})\mathbin{::}\Conid{ArrowChoice}\;\Varid{arr}\Rightarrow \Varid{arr}\;\Varid{a}\;\Varid{c}\to \Varid{arr}\;\Varid{b}\;\Varid{c}\to \Varid{arr}\;(\Conid{Either}\;\Varid{a}\;\Varid{b})\;\Varid{c}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

With this, we define the required recursion in \ensuremath{\Varid{evalN}} as follows. We
start by tagging the empty list as \ensuremath{\Conid{Left}\;()} and the non-empty list into
\ensuremath{\Conid{Right}\;(\Varid{a},[\mskip1.5mu \Varid{a}\mskip1.5mu])} with
\ensuremath{\Varid{arr}\;\Varid{listcase}\mathbin{::}\Varid{arr}\;[\mskip1.5mu \Varid{a}\mskip1.5mu]\;(\Conid{Either}\;(\Conid{Left}\;())\;(\Conid{Right}\;(\Varid{a},[\mskip1.5mu \Varid{a}\mskip1.5mu])))}. In the
\ensuremath{\Conid{Right}\;(\Varid{a},[\mskip1.5mu \Varid{a}\mskip1.5mu])} case, the first element of the tuple is the head of the
list and the second one the tail. Now that the inputs are tagged
depending on their structure, we can then feed these into a branching
structure that uses \ensuremath{\mathbin{\mid\!\mid\!\mid}} and works similar to an if-statement:
If we encounter \ensuremath{\Conid{Left}\;()} -- the base case of the recursion, we return
the empty list with \ensuremath{\Varid{arr}\;(\Varid{const}\;[\mskip1.5mu \mskip1.5mu])}. Otherwise, in the recursive step,
we evaluate the current function \ensuremath{\Varid{f}} and the recursive call \ensuremath{\Varid{evalN}\;\Varid{fs}}
at the same time with \ensuremath{\Varid{f}\mathbin{*\!*\!*}\Varid{evalN}\;\Varid{fs}\mathbin{::}\Varid{arr}\;(\Varid{a},[\mskip1.5mu \Varid{a}\mskip1.5mu])\;(\Varid{b},[\mskip1.5mu \Varid{b}\mskip1.5mu])}. Now, in
order to concatenate the result of this into a list \ensuremath{[\mskip1.5mu \Varid{b}\mskip1.5mu]}, we
concatenate the resulting tuple \ensuremath{(\Varid{b},[\mskip1.5mu \Varid{b}\mskip1.5mu])} with arr \ensuremath{(\Varid{uncurry}\;(\mathbin{:}))}.

Next, we have the \ensuremath{\Varid{mapArr}} combinator (Figure \ref{fig:mapArr}). It
lifts any Arrow \ensuremath{\Varid{arr}\;\Varid{a}\;\Varid{b}} to an Arrow \ensuremath{\Varid{arr}\;[\mskip1.5mu \Varid{a}\mskip1.5mu]\;[\mskip1.5mu \Varid{b}\mskip1.5mu]}. The original
inspiration was from Hughes (\protect\hyperlink{ref-Hughes2005}{2005}),
but the definition was then unified with \ensuremath{\Varid{evalN}} as with the help of
\ensuremath{\Varid{repeat}\mathbin{::}\Varid{a}\to \Varid{a}} it can easily be defined as

\begin{figure}[h]
\centering
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{mapArr}\mathbin{::}\Conid{ArrowChoice}\;\Varid{arr}\Rightarrow \Varid{arr}\;\Varid{a}\;\Varid{b}\to \Varid{arr}\;[\mskip1.5mu \Varid{a}\mskip1.5mu]\;[\mskip1.5mu \Varid{b}\mskip1.5mu]{}\<[E]%
\\
\>[B]{}\Varid{mapArr}\mathrel{=}\Varid{evalN}\mathbin{\circ}\Varid{repeat}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\caption{The definition of \ensuremath{\Varid{map}} over Arrows.}\label{fig:mapArr}\end{figure}

One thing we can see from these utility Arrows is how easily we can
define generic Arrow code that will work on any sufficient specific
Arrow. This is exactly why we aim to allow for parallelisation of
programs via Arrows. We want to empower such generic code to be easily
parallelised so that these generic types of APIs are better to use.

\hypertarget{notes}{%
\subsubsection{Notes}\label{notes}}

In this thesis we will show that parallel computations can be expressed
with this more general interface of Arrows without requiring Monads (we
will see an example of monadic parallelism in Chapter
\ref{sec:parallelHaskells}). We also do not restrict the compatible
Arrows to ones which have \ensuremath{\Conid{ArrowApply}} instances -- as every Arrow that
has a \ensuremath{\Conid{ArrowApply}} instance gives rise to a Monad -- but instead only
require instances for \ensuremath{\Conid{ArrowChoice}} (for the if-then-else construction
in \ensuremath{\Varid{evalN}} (Figure \ref{fig:evalN})) and \ensuremath{\Conid{ArrowLoop}} (for the looping
used in the topological skeletons in Chapter
\ref{sec:topology-skeletons}). Because of this, we will have a truly
more general interface when compared to a monadic one or a purely
function \ensuremath{(\to )} based one.

While we could have based our DSL on Profunctors\footnote{See
  \url{http://hackage.haskell.org/package/profunctors-5.3/docs/Data-Profunctor.html}
  for more information as well as the Haskell interface.} as well, we
chose Arrows in this thesis since they allow for a more direct way of
thinking about parallelism than general Profunctors because of their
composability. However, they are a promising candidate for future
improvements of our DSL. Some Profunctors, especially ones supporting a
composition operation, choice, and looping, can already be adapted to
our interface as shown in Appendix \ref{app:profunctorArrows}.

\hypertarget{introduction-to-parallel-haskells}{%
\section{\texorpdfstring{Introduction to \emph{parallel}
Haskells}{Introduction to parallel Haskells}}\label{introduction-to-parallel-haskells}}

\label{sec:parallelHaskells} \label{sec:parEvalNIntro}

In Chapter \ref{sec:fuproHaskell}, we cited Hughes
(\protect\hyperlink{ref-Hughes:1990:WFP:119830.119832}{1990}) saying
that in functional programming, the order of evaluation is irrelevant.
In parallel programs this is not the case, as at least some kind of
structure of evaluation is required to have actual speedup in programs.
Now, one might think that the idea of us wanting side effects (parallel
evaluation is a side-effect) and having to think about order of
evaluation in a pure functional program seems a bit odd. The fact of the
matter is that functional programs only aim to avoid \emph{unnecessary}
side-effects and in the case of parallelism it is obvious that some
amount of side-effects are required. Also, parallel Haskells generally
aim to encapsulate all the necessary and complicated code in a way such
that the room for code-breaking errors is almost impossible. If some
parallel evaluation code is written in a sub-optimal way, only the
performance is affected, but not the result, which will always be
tractable no matter the order of evaluation.\footnote{Some exceptions
  using unsafe and non-deterministic operations exist, though. These
  situations can however only be achieved if the programmer actively
  chooses to use these kinds of operations.} In the following, we will
take a look at how parallelism can be achieved in Haskell programs in
general.

In its purest form, parallel computation (on functions) can be looked at
as the execution of some functions \ensuremath{[\mskip1.5mu \Varid{a}\to \Varid{b}\mskip1.5mu]} in parallel or
\ensuremath{\Varid{parEvalN}\mathbin{::}[\mskip1.5mu \Varid{a}\to \Varid{b}\mskip1.5mu]\to [\mskip1.5mu \Varid{a}\mskip1.5mu]\to [\mskip1.5mu \Varid{b}\mskip1.5mu]}, as also Figure \ref{fig:parEvalN}
symbolically shows. In this chapter, we will implement this non-Arrow
version which will later be adapted for usage in our Arrow-based
parallel Haskell.

\begin{figure}
\centering
\includegraphics{src/img/parEvalN.pdf}
\caption[Schematic illustration of \ensuremath{\Varid{parEvalN}}.]{Schematic illustration of \ensuremath{\Varid{parEvalN}}. A list of inputs is
transformed by different functions in parallel.\label{fig:parEvalN}}
\end{figure}

There exist several different parallel Haskells, already. As stated in
the Introduction, this is the reason why we base our efforts on existing
work which we wrap as backends behind a common interface so that we do
not re-implement yet another parallel runtime. As described earlier, we
here focus on three of the most important ones -- GpH (Trinder et al.,
\protect\hyperlink{ref-Trinder1996}{1996},
\protect\hyperlink{ref-Trinder1998a}{1998}, based on \ensuremath{\Varid{par}} and \ensuremath{\Varid{pseq}}
\enquote{hints}), the \ensuremath{\Conid{Par}} Monad (Foltzer et al.,
\protect\hyperlink{ref-Foltzer:2012:MPC:2398856.2364562}{2012}, a Monad
for deterministic parallelism; Marlow et al.,
\protect\hyperlink{ref-par-monad}{2011}) and Eden (Loogen,
\protect\hyperlink{ref-Loogen2012}{2012}, a parallel Haskell for
distributed memory; Loogen et al., \protect\hyperlink{ref-eden}{2005}).

We will now go into some detail on these parallel Haskells, and also
give their respective implementations of the non-Arrow version of
\ensuremath{\Varid{parEvalN}}. Chapter \ref{sec:GpHIntro} covers GpH, while Chapters
\ref{sec:ParIntro} and Chapter \ref{sec:EdenIntro} explain the \ensuremath{\Conid{Par}}
Monad and Eden, respectively.

\hypertarget{glasgow-parallel-haskell-gph}{%
\subsection{Glasgow parallel Haskell --
GpH}\label{glasgow-parallel-haskell-gph}}

\label{sec:GpHIntro}

GpH (Marlow et al., \protect\hyperlink{ref-Marlow2009}{2009}; Trinder et
al., \protect\hyperlink{ref-Trinder1998a}{1998}) is one of the simplest
ways to do parallel processing that is found in standard GHC.\footnote{The
  Multicore implementation of GpH is available on Hackage under
  \url{https://hackage.haskell.org/package/parallel-3.2.1.0}, compiler
  support is integrated in the stock GHC.} Besides some basic
primitives,\footnote{\ensuremath{\Varid{par}\mathbin{::}\Varid{a}\to \Varid{b}\to \Varid{b}} to evaluate \ensuremath{\Varid{a}} and \ensuremath{\Varid{b}} in
  parallel and \ensuremath{\Varid{pseq}\mathbin{::}\Varid{a}\to \Varid{b}\to \Varid{b}}, a special variant of \ensuremath{\Varid{seq}} that
  allows enforcing of parallel evaluation.} it already ships with
parallel evaluation strategies for several types which can be applied
with \ensuremath{\Varid{using}\mathbin{::}\Varid{a}\to \Conid{Strategy}\;\Varid{a}\to \Varid{a}}. This is exactly what is required
for an implementation of \ensuremath{\Varid{parEvalN}}:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{18}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{parEvalN}\mathbin{::}(\Conid{NFData}\;\Varid{b})\Rightarrow [\mskip1.5mu \Varid{a}\to \Varid{b}\mskip1.5mu]\to [\mskip1.5mu \Varid{a}\mskip1.5mu]\to [\mskip1.5mu \Varid{b}\mskip1.5mu]{}\<[E]%
\\
\>[B]{}\Varid{parEvalN}\;\Varid{fs}\;\Varid{as}\mathrel{=}\mathbf{let}\;\Varid{bs}\mathrel{=}\Varid{zipWith}\;(\mathbin{\$})\;\Varid{fs}\;\Varid{as}{}\<[E]%
\\
\>[B]{}\hsindent{18}{}\<[18]%
\>[18]{}\mathbf{in}\;\Varid{bs}\mathbin{`\Varid{using}`}\Varid{parList}\;\Varid{rdeepseq}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

In the above definition of \ensuremath{\Varid{parEvalN}}, we just apply the list of
functions \ensuremath{[\mskip1.5mu \Varid{a}\to \Varid{b}\mskip1.5mu]} to the list of inputs \ensuremath{[\mskip1.5mu \Varid{a}\mskip1.5mu]} by zipping them with
the application operator \ensuremath{\mathbin{\$}}. We then evaluate this lazy list \ensuremath{[\mskip1.5mu \Varid{b}\mskip1.5mu]}
according to a \ensuremath{\Conid{Strategy}\;[\mskip1.5mu \Varid{b}\mskip1.5mu]} with \ensuremath{\Varid{using}\mathbin{::}\Varid{a}\to \Conid{Strategy}\;\Varid{a}\to \Varid{a}}. We
construct such a strategy with \ensuremath{\Varid{parList}\mathbin{::}\Conid{Strategy}\;\Varid{a}\to \Conid{Strategy}\;[\mskip1.5mu \Varid{a}\mskip1.5mu]}
and \ensuremath{\Varid{rdeepseq}\mathbin{::}\Conid{NFData}\;\Varid{a}\Rightarrow \Conid{Strategy}\;\Varid{a}}, where the latter is a strategy
which evaluates to normal form. Other strategies like e.g.~evaluation to
weak head normal form are available as well. GpH also allows for custom
\ensuremath{\Conid{Strategy}} implementations to be used. Figure
\ref{fig:parEvalNMulticoreImg} shows a visual representation of this
code.

\begin{figure}
\centering
\includegraphics{src/img/parEvalNMulticoreImg.pdf}
\caption{\ensuremath{\Varid{parEvalN}} (GpH).\label{fig:parEvalNMulticoreImg}}
\end{figure}

\hypertarget{par-monad}{%
\subsection{\texorpdfstring{\ensuremath{\Conid{Par}} Monad}{ Monad}}\label{par-monad}}

\label{sec:ParIntro}

The \ensuremath{\Conid{Par}} Monad\footnote{The \ensuremath{\Conid{Par}} Monad can be found in the \ensuremath{\Varid{monad}\mathbin{-}\Varid{par}}
  package on Hackage under
  \url{https://hackage.haskell.org/package/monad-par-0.3.4.8/}.},
introduced by (Marlow et al., \protect\hyperlink{ref-par-monad}{2011}),
is a Monad designed for the composition of parallel programs:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{6}{@{}>{\hspre}l<{\hspost}@{}}%
\column{9}{@{}>{\hspre}l<{\hspost}@{}}%
\column{26}{@{}>{\hspre}l<{\hspost}@{}}%
\column{34}{@{}>{\hspre}l<{\hspost}@{}}%
\column{35}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{myComp}\mathbin{::}\Conid{Par}\;(\Varid{a},\Varid{b}){}\<[E]%
\\
\>[B]{}\Varid{myComp}\mathrel{=}\mathbf{do}{}\<[E]%
\\
\>[B]{}\hsindent{6}{}\<[6]%
\>[6]{}\Varid{fx}\leftarrow \Varid{spawn}\mathbin{\$}\Varid{return}\;(\Varid{f}\;\Varid{x}){}\<[34]%
\>[34]{}\mbox{\onelinecomment  start evaluating (f x)}{}\<[E]%
\\
\>[B]{}\hsindent{6}{}\<[6]%
\>[6]{}\Varid{gx}\leftarrow \Varid{spawnP}\mathbin{\$}\Varid{return}\;(\Varid{g}\;\Varid{x}){}\<[35]%
\>[35]{}\mbox{\onelinecomment  start evaluating (g x)}{}\<[E]%
\\
\>[B]{}\hsindent{6}{}\<[6]%
\>[6]{}\Varid{a}{}\<[9]%
\>[9]{}\leftarrow \Varid{get}\;\Varid{fx}{}\<[26]%
\>[26]{}\mbox{\onelinecomment  wait for fx}{}\<[E]%
\\
\>[B]{}\hsindent{6}{}\<[6]%
\>[6]{}\Varid{b}{}\<[9]%
\>[9]{}\leftarrow \Varid{get}\;\Varid{gx}{}\<[26]%
\>[26]{}\mbox{\onelinecomment  wait for gx}{}\<[E]%
\\
\>[B]{}\hsindent{6}{}\<[6]%
\>[6]{}\Varid{return}\;(\Varid{a},\Varid{b}){}\<[26]%
\>[26]{}\mbox{\onelinecomment  return results}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

We, however, do not need its composition features and only use its
parallel backend in our definition of \ensuremath{\Varid{parEvalN}}:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{parEvalN}\mathbin{::}(\Conid{NFData}\;\Varid{b})\Rightarrow [\mskip1.5mu \Varid{a}\to \Varid{b}\mskip1.5mu]\to [\mskip1.5mu \Varid{a}\mskip1.5mu]\to [\mskip1.5mu \Varid{b}\mskip1.5mu]{}\<[E]%
\\
\>[B]{}\Varid{parEvalN}\;\Varid{fs}\;\Varid{as}\mathrel{=}\Varid{runPar}\mathbin{\$}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}(\Varid{sequenceA}\;(\Varid{map}\;(\Varid{return}\mathbin{\circ}\Varid{spawn})\;(\Varid{zipWith}\;(\mathbin{\$})\;\Varid{fs}\;\Varid{as})))\bind \Varid{mapM}\;\Varid{get}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

This \ensuremath{\Conid{Par}} Monad version of our parallel evaluation function \ensuremath{\Varid{parEvalN}}
is defined by zipping the list of \ensuremath{[\mskip1.5mu \Varid{a}\to \Varid{b}\mskip1.5mu]} with the list of inputs
\ensuremath{[\mskip1.5mu \Varid{a}\mskip1.5mu]} with the application operator \ensuremath{\mathbin{\$}} just like with GpH. Then, we map
over this not yet evaluated lazy list of results \ensuremath{[\mskip1.5mu \Varid{b}\mskip1.5mu]} with
\ensuremath{\Varid{spawn}\mathbin{::}\Conid{NFData}\;\Varid{a}\Rightarrow \Conid{Par}\;\Varid{a}\to \Conid{Par}\;(\Conid{IVar}\;\Varid{a})} to transform them to a list
of not yet evaluated forked away computations \ensuremath{[\mskip1.5mu \Conid{Par}\;(\Conid{IVar}\;\Varid{b})\mskip1.5mu]}, which we
convert to \ensuremath{\Conid{Par}\;[\mskip1.5mu \Conid{IVar}\;\Varid{b}\mskip1.5mu]} with \ensuremath{\Varid{sequenceA}}. Next, we wait for the
computations to finish by mapping over the \ensuremath{\Conid{IVar}\;\Varid{b}} values inside the
\ensuremath{\Conid{Par}} Monad with \ensuremath{\Varid{get}}, which results in \ensuremath{\Conid{Par}\;[\mskip1.5mu \Varid{b}\mskip1.5mu]}. We execute this
process with \ensuremath{\Varid{runPar}} to finally get the fully evaluated list of
results\ensuremath{[\mskip1.5mu \Varid{b}\mskip1.5mu]}. While we used \ensuremath{\Varid{spawn}} in the definition above, a
head-strict variant can easily be defined by replacing \ensuremath{\Varid{spawn}} with
\ensuremath{\Varid{spawn\char95 }\mathbin{::}\Conid{Par}\;\Varid{a}\to \Conid{Par}\;(\Conid{IVar}\;\Varid{a})}. Figure \ref{fig:parEvalNParMonadImg}
shows a graphical representation.

\begin{figure}
\centering
\includegraphics{src/img/parEvalNParMonadImg.pdf}
\caption{\ensuremath{\Varid{parEvalN}} (\ensuremath{\Conid{Par}} Monad).\label{fig:parEvalNParMonadImg}}
\end{figure}

\hypertarget{eden}{%
\subsection{Eden}\label{eden}}

\label{sec:EdenIntro}

Eden (Loogen, \protect\hyperlink{ref-Loogen2012}{2012}; Loogen et al.,
\protect\hyperlink{ref-eden}{2005}) is a parallel Haskell for
distributed memory and allows for MPI and PVM as distributed
backends.\footnote{The projects homepage can be found at
  \url{http://www.mathematik.uni-marburg.de/~eden/}. The Hackage page is
  at \url{https://hackage.haskell.org/package/edenmodules-1.2.0.0/}.} It
is targeted towards clusters, but also works well in a shared-memory
setting with a further simple backend. However, in contrast to many
other parallel Haskells, in Eden each process has its own heap. This
seems to be a waste of memory, but with the distributed programming
paradigm and individual GC per process, Eden yields good performance
results on multicores, as well (Aswad et al.,
\protect\hyperlink{ref-aswad2009low}{2009}; Berthold et al.,
\protect\hyperlink{ref-arcs-dc}{2009}\protect\hyperlink{ref-arcs-dc}{a}).

While Eden comes with a Monad \ensuremath{\Conid{PA}} for parallel evaluation, it also
ships with a completely functional interface that includes a
\ensuremath{\Varid{spawnF}\mathbin{::}(\Conid{Trans}\;\Varid{a},\Conid{Trans}\;\Varid{b})\Rightarrow [\mskip1.5mu \Varid{a}\to \Varid{b}\mskip1.5mu]\to [\mskip1.5mu \Varid{a}\mskip1.5mu]\to [\mskip1.5mu \Varid{b}\mskip1.5mu]}\footnote{The
  type class \ensuremath{\Conid{Trans}} stands for Transmissible. The Eden library already
  has instances for most common types and allows for easy construction
  of further instances.} function that allows us to define \ensuremath{\Varid{parEvalN}}
directly:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{parEvalN}\mathbin{::}(\Conid{Trans}\;\Varid{a},\Conid{Trans}\;\Varid{b})\Rightarrow [\mskip1.5mu \Varid{a}\to \Varid{b}\mskip1.5mu]\to [\mskip1.5mu \Varid{a}\mskip1.5mu]\to [\mskip1.5mu \Varid{b}\mskip1.5mu]{}\<[E]%
\\
\>[B]{}\Varid{parEvalN}\mathrel{=}\Varid{spawnF}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

\hypertarget{eden-traceviewer}{%
\subsubsection{Eden TraceViewer}\label{eden-traceviewer}}

\label{sec:edentv}

To comprehend the efficiency -- or the lack thereof, in a parallel
program, an inspection of its execution is extremely helpful. While some
large-scale solutions exist (Geimer et al.,
\protect\hyperlink{ref-Geimer2010}{2010}), the parallel Haskell
community mainly utilises the tools Threadscope (Wheeler and Thain,
\protect\hyperlink{ref-Wheeler2009}{2009}) and Eden
TraceViewer\footnote{See \url{http://hackage.haskell.org/package/edentv}
  on Hackage for the last available version of Eden TraceViewer.}
(Berthold and Loogen, \protect\hyperlink{ref-Berthold2007a}{2007}). In
the next chapters, we will present some \emph{trace visualisations}, the
post-mortem process diagrams of Eden processes and their activity.

The trace visualisations are colour-coded. In such a visualisation
(Figure \ref{fig:withoutFutures}), the \ensuremath{\Varid{x}} axis shows the time, the \ensuremath{\Varid{y}}
axis enumerates the machines and processes. The visualisation shows a
running process in green, a blocked process is red. If the process is
\enquote{runnable}, i.e.~it may run, but does not, it is yellow. The
typical reason for this is garbage collection (GC). An inactive machine,
where no processes are started yet, or all are already terminated, shows
as a blue bar. A communication from one process to another is
represented with a black arrow. A stream of communications, e.g.~a
transmitted list is shows as a dark shading between sender and receiver
processes.

\hypertarget{parallel-arrows}{%
\chapter{Parallel Arrows}\label{parallel-arrows}}

\label{sec:parallel-arrows}

Having explained the idea of Arrows as well as the basics of the APIs
that we wish to use as backends, we can now discuss the design and
implementation of the actual PArrows DSL. We present the \ensuremath{\Conid{ArrowParallel}}
type class and the reasoning behind it in Chapter
\ref{sec:parallel-arrows-type-class} before discussing its
implementations in GpH, the \ensuremath{\Conid{Par}} Monad and Eden in Chapter
\ref{sec:arrowparallelimpl}. Then, we give first basic extensions in
Chapter \ref{sec:extending-interface}. Finally, we present basic
\ensuremath{\Varid{map}}-based skeletons in Chapter \ref{sec:skeletons}.

\hypertarget{the-arrowparallel-type-class}{%
\section{\texorpdfstring{The \ensuremath{\Conid{ArrowParallel}} type
class}{The  type class}}\label{the-arrowparallel-type-class}}

\label{sec:parallel-arrows-type-class}

A parallel computation (on functions) can be seen as the execution of
some functions \ensuremath{[\mskip1.5mu \Varid{a}\to \Varid{b}\mskip1.5mu]} in parallel, as our \ensuremath{\Varid{parEvalN}} prototype shows
(Chapter \ref{sec:parEvalNIntro}). Translating this into Arrow terms
gives us a new operator \ensuremath{\Varid{parEvalN}} that lifts a list of Arrows
\ensuremath{[\mskip1.5mu \Varid{arr}\;\Varid{a}\;\Varid{b}\mskip1.5mu]} to a parallel Arrow \ensuremath{\Varid{arr}\;[\mskip1.5mu \Varid{a}\mskip1.5mu]\;[\mskip1.5mu \Varid{b}\mskip1.5mu]}. This combinator is
similar to the evaluation combinator \ensuremath{\Varid{evalN}\mathbin{::}[\mskip1.5mu \Varid{arr}\;\Varid{a}\;\Varid{b}\mskip1.5mu]\to \Varid{arr}\;[\mskip1.5mu \Varid{a}\mskip1.5mu]\;[\mskip1.5mu \Varid{b}\mskip1.5mu]}
from Chapter \ref{utilfns}, but does parallel instead of serial
evaluation:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{parEvalN}\mathbin{::}(\Conid{Arrow}\;\Varid{arr})\Rightarrow [\mskip1.5mu \Varid{arr}\;\Varid{a}\;\Varid{b}\mskip1.5mu]\to \Varid{arr}\;[\mskip1.5mu \Varid{a}\mskip1.5mu]\;[\mskip1.5mu \Varid{b}\mskip1.5mu]{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

With such a definition of \ensuremath{\Varid{parEvalN}}, parallel execution is yet another
Arrow combinator. But since the implementation may differ depending on
the actual type of the Arrow \ensuremath{\Varid{arr}} -- or even the input \ensuremath{\Varid{a}} and output
\ensuremath{\Varid{b}} -- and we want this to be an interface for different backends that
we should be able to switch between, we introduce a new type class
\ensuremath{\Conid{ArrowParallel}\;\Varid{arr}\;\Varid{a}\;\Varid{b}}.


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{class}\;\Conid{Arrow}\;\Varid{arr}\Rightarrow \Conid{ArrowParallel}\;\Varid{arr}\;\Varid{a}\;\Varid{b}\;\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{parEvalN}\mathbin{::}[\mskip1.5mu \Varid{arr}\;\Varid{a}\;\Varid{b}\mskip1.5mu]\to \Varid{arr}\;[\mskip1.5mu \Varid{a}\mskip1.5mu]\;[\mskip1.5mu \Varid{b}\mskip1.5mu]{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

Sometimes parallel Haskells require or allow additional configuration
parameters, e.g.~information about the execution environment or the
level of evaluation (WHNF vs.~NF, see the section on laziness in Chapter
\ref{sec:shortIntroHaskell}). For this reason we introduce an additional
\ensuremath{\Varid{conf}} parameter as we do not want \ensuremath{\Varid{conf}} to be a fixed type and the
configuration parameters can differ for different instances of
\ensuremath{\Conid{ArrowParallel}}.


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{class}\;\Conid{Arrow}\;\Varid{arr}\Rightarrow \Conid{ArrowParallel}\;\Varid{arr}\;\Varid{a}\;\Varid{b}\;\Varid{conf}\;\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{parEvalN}\mathbin{::}\Varid{conf}\to [\mskip1.5mu \Varid{arr}\;\Varid{a}\;\Varid{b}\mskip1.5mu]\to \Varid{arr}\;[\mskip1.5mu \Varid{a}\mskip1.5mu]\;[\mskip1.5mu \Varid{b}\mskip1.5mu]{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

By restricting the implementations of our backends to a specific \ensuremath{\Varid{conf}}
type, we also get interoperability between backends for free because it
serves as a discriminator for which backend has to be used. We can
therefore parallelise one part of a program using one backend, and do
the same for the next but with another one by just passing a different
configuration type.

\hypertarget{arrowparallel-instances}{%
\section{\texorpdfstring{\ensuremath{\Conid{ArrowParallel}}
instances}{ instances}}\label{arrowparallel-instances}}

\label{sec:arrowparallelimpl}

With the \ensuremath{\Conid{ArrowParallel}} type class defined, we will now give
implementations of it with GpH (Chapter \ref{sec:parrows:multicore}),
the \ensuremath{\Conid{Par}} Monad (Chapter \ref{sec:parrows:parmonad}) and Eden (Chapter
\ref{sec:parrows-Eden}). Finally, we explain default configuration
instances in Chapter \ref{sec:defaultConfigInstances}.

\hypertarget{glasgow-parallel-haskell}{%
\subsection{Glasgow parallel Haskell}\label{glasgow-parallel-haskell}}

\label{sec:parrows:multicore}

The GpH instance of \ensuremath{\Conid{ArrowParallel}} is implemented in a straightforward
manner in Figure \ref{fig:ArrowParallelMulticore}, but a bit different
compared to the variant from Chapter \ref{sec:GpHIntro}. We use
\ensuremath{\Varid{evalN}\mathbin{::}[\mskip1.5mu \Varid{arr}\;\Varid{a}\;\Varid{b}\mskip1.5mu]\to \Varid{arr}\;[\mskip1.5mu \Varid{a}\mskip1.5mu]\;[\mskip1.5mu \Varid{b}\mskip1.5mu]} (definition in Appendix
\ref{utilfns}, think \ensuremath{\Varid{zipWith}\;(\mathbin{\$})} on Arrows) combined with
\ensuremath{\Varid{withStrategy}\mathbin{::}\Conid{Strategy}\;\Varid{a}\to \Varid{a}\to \Varid{a}} from GpH, where \ensuremath{\Varid{withStrategy}} is
the same as \ensuremath{\Varid{using}\mathbin{::}\Varid{a}\to \Conid{Strategy}\;\Varid{a}\to \Varid{a}}, but with flipped
parameters. Our \ensuremath{\Conid{Conf}\;\Varid{a}} datatype simply wraps a \ensuremath{\Conid{Strategy}\;\Varid{a}}, but could
be extended in future versions of our DSL.

\begin{figure}[h]
\centering
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{9}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{data}\;\Conid{Conf}\;\Varid{a}\mathrel{=}\Conid{Conf}\;(\Conid{Strategy}\;\Varid{a}){}\<[E]%
\\[\blanklineskip]%
\>[B]{}\mathbf{instance}\;(\Conid{ArrowChoice}\;\Varid{arr})\Rightarrow {}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Conid{ArrowParallel}\;\Varid{arr}\;\Varid{a}\;\Varid{b}\;(\Conid{Conf}\;\Varid{b})\;\mathbf{where}{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\Varid{parEvalN}\;(\Conid{Conf}\;\Varid{strat})\;\Varid{fs}\mathrel{=}{}\<[E]%
\\
\>[5]{}\hsindent{4}{}\<[9]%
\>[9]{}\Varid{evalN}\;\Varid{fs}\mathbin{>\!\!>\!\!>}{}\<[E]%
\\
\>[5]{}\hsindent{4}{}\<[9]%
\>[9]{}\Varid{arr}\;(\Varid{withStrategy}\;(\Varid{parList}\;\Varid{strat})){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\caption{GpH \ensuremath{\Conid{ArrowParallel}} instance.}\label{fig:ArrowParallelMulticore}\end{figure}

\hypertarget{par-monad-1}{%
\subsection{\texorpdfstring{\ensuremath{\Conid{Par}} Monad}{ Monad}}\label{par-monad-1}}

\label{sec:parrows:parmonad}

As for GpH we can easily lift the definition of \ensuremath{\Varid{parEvalN}} for the \ensuremath{\Conid{Par}}
Monad to Arrows in Figure \ref{fig:ArrowParallelParMonad}. To start off,
we define the \ensuremath{\Conid{Strategy}\;\Varid{a}} and \ensuremath{\Conid{Conf}\;\Varid{a}} type so we can have a
configurable instance of ArrowParallel:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{type}\;\Conid{Strategy}\;\Varid{a}\mathrel{=}\Varid{a}\to \Conid{Par}\;(\Conid{IVar}\;\Varid{a}){}\<[E]%
\\
\>[B]{}\mathbf{data}\;\Conid{Conf}\;\Varid{a}\mathrel{=}\Conid{Conf}\;(\Conid{Strategy}\;\Varid{a}){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

Now we can once again define our \ensuremath{\Conid{ArrowParallel}} instance as follows:
First, we convert our Arrows \ensuremath{[\mskip1.5mu \Varid{arr}\;\Varid{a}\;\Varid{b}\mskip1.5mu]} with
\ensuremath{\Varid{evalN}\;(\Varid{map}\;(\mathbin{>\!\!>\!\!>}\Varid{arr}\;\Varid{strat})\;\Varid{fs})} into an Arrow
\ensuremath{\Varid{arr}\;[\mskip1.5mu \Varid{a}\mskip1.5mu]\;[\mskip1.5mu (\Conid{Par}\;(\Conid{IVar}\;\Varid{b}))\mskip1.5mu]} that yields composable computations in the
\ensuremath{\Conid{Par}} Monad. By combining the result of this Arrow with \ensuremath{\Varid{arr}\;\Varid{sequenceA}},
we get an Arrow \ensuremath{\Varid{arr}\;[\mskip1.5mu \Varid{a}\mskip1.5mu]\;(\Conid{Par}\;[\mskip1.5mu \Conid{IVar}\;\Varid{b}\mskip1.5mu])}. Then, in order to fetch the
results of the different threads, we map over the \ensuremath{\Conid{IVar}}s inside the
\ensuremath{\Conid{Par}} Monad with \ensuremath{\Varid{arr}\;(\bind \Varid{mapM}\;\Varid{get})} -- our intermediary Arrow is of
type \ensuremath{\Varid{arr}\;[\mskip1.5mu \Varid{a}\mskip1.5mu]\;(\Conid{Par}\;[\mskip1.5mu \Varid{b}\mskip1.5mu])}. Finally, we execute the computation \ensuremath{\Conid{Par}\;[\mskip1.5mu \Varid{b}\mskip1.5mu]}
by composing with \ensuremath{\Varid{arr}\;\Varid{runPar}} and get the final Arrow \ensuremath{\Varid{arr}\;[\mskip1.5mu \Varid{a}\mskip1.5mu]\;[\mskip1.5mu \Varid{b}\mskip1.5mu]}.

\begin{figure}[h]
\centering
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{9}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{type}\;\Conid{Strategy}\;\Varid{a}\mathrel{=}\Varid{a}\to \Conid{Par}\;(\Conid{IVar}\;\Varid{a}){}\<[E]%
\\
\>[B]{}\mathbf{data}\;\Conid{Conf}\;\Varid{a}\mathrel{=}\Conid{Conf}\;(\Conid{Strategy}\;\Varid{a}){}\<[E]%
\\[\blanklineskip]%
\>[B]{}\mathbf{instance}\;(\Conid{ArrowChoice}\;\Varid{arr})\Rightarrow \Conid{ArrowParallel}\;\Varid{arr}\;\Varid{a}\;\Varid{b}\;(\Conid{Conf}\;\Varid{b})\;\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{parEvalN}\;(\Conid{Conf}\;\Varid{strat})\;\Varid{fs}\mathrel{=}{}\<[E]%
\\
\>[5]{}\hsindent{4}{}\<[9]%
\>[9]{}\Varid{evalN}\;(\Varid{map}\;(\mathbin{>\!\!>\!\!>}\Varid{arr}\;\Varid{strat})\;\Varid{fs})\mathbin{>\!\!>\!\!>}{}\<[E]%
\\
\>[5]{}\hsindent{4}{}\<[9]%
\>[9]{}\Varid{arr}\;\Varid{sequenceA}\mathbin{>\!\!>\!\!>}{}\<[E]%
\\
\>[5]{}\hsindent{4}{}\<[9]%
\>[9]{}\Varid{arr}\;(\bind \Varid{mapM}\;\Varid{get})\mathbin{>\!\!>\!\!>}{}\<[E]%
\\
\>[5]{}\hsindent{4}{}\<[9]%
\>[9]{}\Varid{arr}\;\Varid{runPar}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\caption{\ensuremath{\Conid{Par}} Monad \ensuremath{\Conid{ArrowParallel}} instance.}\label{fig:ArrowParallelParMonad}\end{figure}

\hypertarget{eden-1}{%
\subsection{Eden}\label{eden-1}}

\label{sec:parrows-Eden}

For both the GpH Haskell and \ensuremath{\Conid{Par}} Monad implementations we could use
general instances of \ensuremath{\Conid{ArrowParallel}} that just require the \ensuremath{\Conid{ArrowChoice}}
type class. With Eden this is not the case as we can only spawn a list
of functions which we cannot extract from general Arrows. While we could
still manage to have only one instance in the module by introducing a
type class


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{class}\;(\Conid{Arrow}\;\Varid{arr})\Rightarrow \Conid{ArrowUnwrap}\;\Varid{arr}\;\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{unwrap}\mathbin{::}\Varid{arr}\;\Varid{a}\;\Varid{b}\to (\Varid{a}\to \Varid{b}){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

we avoid doing so for aesthetic reasons. For now, we just implement
\ensuremath{\Conid{ArrowParallel}} for normal functions and the Kleisli type in Figure
\ref{fig:ArrowParallelEden}, where \ensuremath{\Conid{Conf}} is simply defined as
\ensuremath{\mathbf{data}\;\Conid{Conf}\mathrel{=}\Conid{Nil}} since Eden does not have a configurable \ensuremath{\Varid{spawnF}}
variant.

\begin{figure}[h]
\centering
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{7}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{instance}\;(\Conid{Trans}\;\Varid{a},\Conid{Trans}\;\Varid{b})\Rightarrow \Conid{ArrowParallel}\;(\to )\;\Varid{a}\;\Varid{b}\;\Conid{Conf}\;\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{parEvalN}\;\anonymous \mathrel{=}\Varid{spawnF}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\mathbf{instance}\;(\Conid{ArrowParallel}\;(\to )\;\Varid{a}\;(\Varid{m}\;\Varid{b})\;\Conid{Conf},{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Conid{Monad}\;\Varid{m},\Conid{Trans}\;\Varid{a},\Conid{Trans}\;(\Varid{m}\;\Varid{b}))\Rightarrow {}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Conid{ArrowParallel}\;(\Conid{Kleisli}\;\Varid{m})\;\Varid{a}\;\Varid{b}\;\Conid{Conf}\;\mathbf{where}{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\Varid{parEvalN}\;\Varid{conf}\;\Varid{fs}\mathrel{=}{}\<[E]%
\\
\>[5]{}\hsindent{2}{}\<[7]%
\>[7]{}\Varid{arr}\;(\Varid{parEvalN}\;\Varid{conf}\;(\Varid{map}\;(\lambda (\Conid{Kleisli}\;\Varid{f})\to \Varid{f})\;\Varid{fs}))\mathbin{>\!\!>\!\!>}{}\<[E]%
\\
\>[5]{}\hsindent{2}{}\<[7]%
\>[7]{}\Conid{Kleisli}\;\Varid{sequence}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\caption{Eden \ensuremath{\Conid{ArrowParallel}} instance.}\label{fig:ArrowParallelEden}\end{figure}

\emph{Note that while writing this thesis, we found another solution
that could be feasible: We could write the instance \ensuremath{\Varid{parEvalN}} as}


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{instance}\;(\Conid{Trans}\;\Varid{b},\Conid{ArrowChoice}\;\Varid{arr})\Rightarrow \Conid{ArrowParallel}\;\Varid{arr}\;\Varid{a}\;\Varid{b}\;\Conid{Conf}\;\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Varid{parEvalN}\;\anonymous \;\Varid{fs}\mathrel{=}\Varid{evalN}\;\Varid{fs}\mathbin{>\!\!>\!\!>}\Varid{arr}\;(\Varid{spawnF}\;(\Varid{repeat}\;\Varid{id})){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

\emph{We were however, not able to prove that this behaves exactly the
same as the variant presented above since this would have required
re-running the whole test-suite. First tests suggest correct behaviour,
though.}

\hypertarget{default-configuration-instances}{%
\subsection{Default configuration
instances}\label{default-configuration-instances}}

\label{sec:defaultConfigInstances}

While the configurability of the \ensuremath{\Conid{ArrowParallel}} instances above is
nice, users probably would like to have proper default configurations
for many parallel programs as well. These can also easily be defined as
we can see by the example of the default implementation of
\ensuremath{\Conid{ArrowParallel}} for GpH:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{instance}\;(\Conid{NFData}\;\Varid{b},\Conid{ArrowChoice}\;\Varid{arr},\Conid{ArrowParallel}\;\Varid{arr}\;\Varid{a}\;\Varid{b}\;(\Conid{Conf}\;\Varid{b}))\Rightarrow {}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Conid{ArrowParallel}\;\Varid{arr}\;\Varid{a}\;\Varid{b}\;()\;\mathbf{where}{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\Varid{parEvalN}\;\anonymous \;\Varid{fs}\mathrel{=}\Varid{parEvalN}\;(\Varid{defaultConf}\;\Varid{fs})\;\Varid{fs}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{defaultConf}\mathbin{::}(\Conid{NFData}\;\Varid{b})\Rightarrow [\mskip1.5mu \Varid{arr}\;\Varid{a}\;\Varid{b}\mskip1.5mu]\to \Conid{Conf}\;\Varid{b}{}\<[E]%
\\
\>[B]{}\Varid{defaultConf}\;\Varid{fs}\mathrel{=}\Varid{stratToConf}\;\Varid{fs}\;\Varid{rdeepseq}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{stratToConf}\mathbin{::}[\mskip1.5mu \Varid{arr}\;\Varid{a}\;\Varid{b}\mskip1.5mu]\to \Conid{Strategy}\;\Varid{b}\to \Conid{Conf}\;\Varid{b}{}\<[E]%
\\
\>[B]{}\Varid{stratToConf}\;\anonymous \;\Varid{strat}\mathrel{=}\Conid{Conf}\;\Varid{strat}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

The other backends have similarly structured implementations which we do
not discuss here for the sake of brevity. We can, however, only have one
instance of \ensuremath{\Conid{ArrowParallel}\;\Varid{arr}\;\Varid{a}\;\Varid{b}\;()} present at a time, which should
not be a problem, anyways.

Up until now we have discussed Arrow operations in greater detail, but
in the following sections we focus more on the data-flow between the
Arrows, now that we have seen that Arrows are capable of expressing
parallelism. We nevertheless do explain new concepts in more depth if
required for better understanding.

\newpage

\hypertarget{extending-the-interface}{%
\section{Extending the interface}\label{extending-the-interface}}

\label{sec:extending-interface}

With the \ensuremath{\Conid{ArrowParallel}} type class in place, we can now define other
parallel interface functions. These are basic algorithmic skeletons that
are used to define more sophisticated ones later in this thesis. Namely,
these are a lazy variant of \ensuremath{\Varid{parEvalN}} (Chapter \ref{sec:lazyParEvalN})
as well as a method to spawn heterogeneous tasks (Chapter
\ref{sec:hetereogeneoustasks}).

\hypertarget{lazy-parevaln}{%
\subsection{\texorpdfstring{Lazy
\ensuremath{\Varid{parEvalN}}}{Lazy }}\label{lazy-parevaln}}

\label{sec:lazyParEvalN}

\begin{figure}[h]
\centering
\includegraphics{src/img/parEvalNLazy.pdf}
\caption{\ensuremath{\Varid{parEvalNLazy}} depiction.\label{fig:parEvalNLazyImg}}
\end{figure}

The resulting Arrow of \ensuremath{\Varid{parEvalN}} fully traverses the list of input
Arrows as well as their inputs. Sometimes, this might not be feasible,
as it will not work on infinite lists of Arrows/functions like e.g.
\ensuremath{\Varid{map}\;(\Varid{arr}\mathbin{\circ}(\mathbin{+}))\;[\mskip1.5mu \mathrm{1}\mathinner{\ldotp\ldotp}\mskip1.5mu]} or just because in case we need the Arrows
evaluated in chunks. \ensuremath{\Varid{parEvalNLazy}} (Figs. \ref{fig:parEvalNLazyImg},
\ref{fig:parEvalNLazy}) fixes this. It works by first chunking the input
from \ensuremath{[\mskip1.5mu \Varid{a}\mskip1.5mu]} to \ensuremath{[\mskip1.5mu [\mskip1.5mu \Varid{a}\mskip1.5mu]\mskip1.5mu]} with the given \ensuremath{\Varid{chunkSize}} in
\ensuremath{\Varid{arr}\;(\Varid{chunksOf}\;\Varid{chunkSize})}. These chunks are then fed into a list
\ensuremath{[\mskip1.5mu \Varid{arr}\;[\mskip1.5mu \Varid{a}\mskip1.5mu]\;[\mskip1.5mu \Varid{b}\mskip1.5mu]\mskip1.5mu]} of chunk-wise parallel Arrows with the help of our lazy
and sequential \ensuremath{\Varid{evalN}}. The resulting \ensuremath{[\mskip1.5mu [\mskip1.5mu \Varid{b}\mskip1.5mu]\mskip1.5mu]} is lastly converted into
\ensuremath{[\mskip1.5mu \Varid{b}\mskip1.5mu]} with \ensuremath{\Varid{arr}\;\Varid{concat}}.

\begin{figure}[h]
\centering
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{7}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{type}\;\Conid{ChunkSize}\mathrel{=}\Conid{Int}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{parEvalNLazy}\mathbin{::}(\Conid{ArrowParallel}\;\Varid{arr}\;\Varid{a}\;\Varid{b}\;\Varid{conf},\Conid{ArrowChoice}\;\Varid{arr})\Rightarrow {}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{conf}\to \Conid{ChunkSize}\to [\mskip1.5mu \Varid{arr}\;\Varid{a}\;\Varid{b}\mskip1.5mu]\to (\Varid{arr}\;[\mskip1.5mu \Varid{a}\mskip1.5mu]\;[\mskip1.5mu \Varid{b}\mskip1.5mu]){}\<[E]%
\\
\>[B]{}\Varid{parEvalNLazy}\;\Varid{conf}\;\Varid{chunkSize}\;\Varid{fs}\mathrel{=}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{arr}\;(\Varid{chunksOf}\;\Varid{chunkSize})\mathbin{>\!\!>\!\!>}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{evalN}\;\Varid{fchunks}\mathbin{>\!\!>\!\!>}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{arr}\;\Varid{concat}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\mathbf{where}{}\<[E]%
\\
\>[5]{}\hsindent{2}{}\<[7]%
\>[7]{}\Varid{fchunks}\mathrel{=}\Varid{map}\;(\Varid{parEvalN}\;\Varid{conf})\;(\Varid{chunksOf}\;\Varid{chunkSize}\;\Varid{fs}){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\caption{Definition of \ensuremath{\Varid{parEvalNLazy}}.}\label{fig:parEvalNLazy}\end{figure}

\hypertarget{heterogeneous-tasks}{%
\subsection{Heterogeneous tasks}\label{heterogeneous-tasks}}

\label{sec:hetereogeneoustasks}

\begin{figure}[h]
\centering
\includegraphics{src/img/parEval2Img.pdf}
\caption{\ensuremath{\Varid{parEval2}} depiction.\label{fig:parEval2Img}}
\end{figure}

We have only talked about the parallelization of Arrows of the same
input and output types until now. But sometimes we want to parallelise
heterogeneous types as well. We can implement such a \ensuremath{\Varid{parEval2}}
combinator (Figs. \ref{fig:parEval2Img}, \ref{fig:parEval2}) which
combines two Arrows \ensuremath{\Varid{arr}\;\Varid{a}\;\Varid{b}} and \ensuremath{\Varid{arr}\;\Varid{c}\;\Varid{d}} into a new parallel Arrow
\ensuremath{\Varid{arr}\;(\Varid{a},\Varid{c})\;(\Varid{b},\Varid{d})} quite easily with the help of the \ensuremath{\Conid{ArrowChoice}} type
class. Here, the general idea is to use the \ensuremath{\mathbin{+\!\!+\!\!+}} combinator which takes
two Arrows \ensuremath{\Varid{arr}\;\Varid{a}\;\Varid{b}} and \ensuremath{\Varid{arr}\;\Varid{c}\;\Varid{d}} and transforms them into
\ensuremath{\Varid{arr}\;(\Conid{Either}\;\Varid{a}\;\Varid{c})\;(\Conid{Either}\;\Varid{b}\;\Varid{d})} to get a common Arrow type that we can
then feed into \ensuremath{\Varid{parEvalN}}.

We can implement this idea as follows: Starting off, we transform the
\ensuremath{(\Varid{a},\Varid{c})} input into a two-element list \ensuremath{[\mskip1.5mu \Conid{Either}\;\Varid{a}\;\Varid{c}\mskip1.5mu]} by first tagging
the two inputs with \ensuremath{\Conid{Left}} and \ensuremath{\Conid{Right}} and wrapping the right element in
a singleton list with \ensuremath{\Varid{return}} so that we can combine them with
\ensuremath{\Varid{arr}\;(\Varid{uncurry}\;(\mathbin{:}))}. Next, we feed this list into a parallel Arrow
running on two instances of \ensuremath{\Varid{f}\mathbin{+\!\!+\!\!+}\Varid{g}}. After the calculation is
finished, we convert the resulting \ensuremath{[\mskip1.5mu \Conid{Either}\;\Varid{b}\;\Varid{d}\mskip1.5mu]} into \ensuremath{([\mskip1.5mu \Varid{b}\mskip1.5mu],[\mskip1.5mu \Varid{d}\mskip1.5mu])} with
\ensuremath{\Varid{arr}\;\Varid{partitionEithers}}. The two lists in this tuple each contain only
one element by construction, so we can finally just convert the tuple to
\ensuremath{(\Varid{b},\Varid{d})} with \ensuremath{\Varid{arr}\;\Varid{head}\mathbin{*\!*\!*}\Varid{arr}\;\Varid{head}} in the last step.

\begin{figure}[h]
\centering
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{parEval2}\mathbin{::}(\Conid{ArrowChoice}\;\Varid{arr},{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Conid{ArrowParallel}\;\Varid{arr}\;(\Conid{Either}\;\Varid{a}\;\Varid{c})\;(\Conid{Either}\;\Varid{b}\;\Varid{d})\;\Varid{conf})\Rightarrow {}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{conf}\to \Varid{arr}\;\Varid{a}\;\Varid{b}\to \Varid{arr}\;\Varid{c}\;\Varid{d}\to \Varid{arr}\;(\Varid{a},\Varid{c})\;(\Varid{b},\Varid{d}){}\<[E]%
\\
\>[B]{}\Varid{parEval2}\;\Varid{conf}\;\Varid{f}\;\Varid{g}\mathrel{=}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{arr}\;\Conid{Left}\mathbin{*\!*\!*}(\Varid{arr}\;\Conid{Right}\mathbin{>\!\!>\!\!>}\Varid{arr}\;\Varid{return})\mathbin{>\!\!>\!\!>}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{arr}\;(\Varid{uncurry}\;(\mathbin{:}))\mathbin{>\!\!>\!\!>}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{parEvalN}\;\Varid{conf}\;(\Varid{replicate}\;\mathrm{2}\;(\Varid{f}\mathbin{+\!\!+\!\!+}\Varid{g}))\mathbin{>\!\!>\!\!>}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{arr}\;\Varid{partitionEithers}\mathbin{>\!\!>\!\!>}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{arr}\;\Varid{head}\mathbin{*\!*\!*}\Varid{arr}\;\Varid{head}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\caption{\ensuremath{\Varid{parEval2}} definition.}\label{fig:parEval2}\end{figure}

\newpage

\hypertarget{basic-map-based-skeletons}{%
\section{\texorpdfstring{Basic \ensuremath{\Varid{map}}-based
skeletons}{Basic -based skeletons}}\label{basic-map-based-skeletons}}

\label{sec:skeletons} \label{sec:map-skeletons}

Now we have developed Parallel Arrows far enough to define some still
basic, yet useful, algorithmic skeletons that abstract typical parallel
computations -- parallel \ensuremath{\Varid{map}}s. The essential differences between the
skeletons presented here are in terms of order of evaluation and work
distribution. They nevertheless still provide the same semantics as a
sequential \ensuremath{\Varid{map}}. We discuss a basic parallel \ensuremath{\Varid{map}} and a lazy variant
thereof (Chapter \ref{sec:parMapAndLaziness}) as well as a statically
load balancing parallel \ensuremath{\Varid{map}} (Chapter
\ref{sec:staticallyloadbalancing}).

\hypertarget{parallel-map-and-laziness}{%
\subsection{\texorpdfstring{Parallel \ensuremath{\Varid{map}} and
laziness}{Parallel  and laziness}}\label{parallel-map-and-laziness}}

\label{sec:parMapAndLaziness}

The \ensuremath{\Varid{parMap}} skeleton (Figs. \ref{fig:parMapImg}, \ref{fig:parMap}) is
probably the most common skeleton for parallel programs. We can
implement it with \ensuremath{\Conid{ArrowParallel}} by repeating an Arrow \ensuremath{\Varid{arr}\;\Varid{a}\;\Varid{b}} and
then passing it into \ensuremath{\Varid{parEvalN}} to obtain an Arrow \ensuremath{\Varid{arr}\;[\mskip1.5mu \Varid{a}\mskip1.5mu]\;[\mskip1.5mu \Varid{b}\mskip1.5mu]}.

\begin{figure}[h]
\centering
\includegraphics{src/img/parMap.pdf}
\caption{\ensuremath{\Varid{parMap}} depiction.\label{fig:parMapImg}}
\end{figure}

\begin{figure}[h]
\centering
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{parMap}\mathbin{::}(\Conid{ArrowParallel}\;\Varid{arr}\;\Varid{a}\;\Varid{b}\;\Varid{conf})\Rightarrow {}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{conf}\to (\Varid{arr}\;\Varid{a}\;\Varid{b})\to (\Varid{arr}\;[\mskip1.5mu \Varid{a}\mskip1.5mu]\;[\mskip1.5mu \Varid{b}\mskip1.5mu]){}\<[E]%
\\
\>[B]{}\Varid{parMap}\;\Varid{conf}\;\Varid{f}\mathrel{=}\Varid{parEvalN}\;\Varid{conf}\;(\Varid{repeat}\;\Varid{f}){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\caption{\ensuremath{\Varid{parMap}} definition.}\label{fig:parMap}\end{figure}

Just like \ensuremath{\Varid{parEvalN}}, \ensuremath{\Varid{parMap}} traverses all input Arrows as well as the
inputs. Because of this, it has the same restrictions as \ensuremath{\Varid{parEvalN}} as
compared to \ensuremath{\Varid{parEvalNLazy}}. So it makes sense to also have a
\ensuremath{\Varid{parMapStream}} (Figs. \ref{fig:parMapStreamImg}, \ref{fig:parMapStream})
which behaves like \ensuremath{\Varid{parMap}}, but uses \ensuremath{\Varid{parEvalNLazy}} instead of
\ensuremath{\Varid{parEvalN}}.

\begin{figure}[h]
\centering
\includegraphics{src/img/parMapStream.pdf}
\caption{\ensuremath{\Varid{parMapStream}} depiction.\label{fig:parMapStreamImg}}
\end{figure}

\begin{figure}[h]
\centering
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{type}\;\Conid{ChunkSize}\mathrel{=}\Conid{Int}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{parMapStream}\mathbin{::}(\Conid{ArrowParallel}\;\Varid{arr}\;\Varid{a}\;\Varid{b}\;\Varid{conf},{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Conid{ArrowChoice}\;\Varid{arr},{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Conid{ArrowApply}\;\Varid{arr})\Rightarrow {}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{conf}\to \Conid{ChunkSize}\to \Varid{arr}\;\Varid{a}\;\Varid{b}\to \Varid{arr}\;[\mskip1.5mu \Varid{a}\mskip1.5mu]\;[\mskip1.5mu \Varid{b}\mskip1.5mu]{}\<[E]%
\\
\>[B]{}\Varid{parMapStream}\;\Varid{conf}\;\Varid{chunkSize}\;\Varid{f}\mathrel{=}\Varid{parEvalNLazy}\;\Varid{conf}\;\Varid{chunkSize}\;(\Varid{repeat}\;\Varid{f}){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\caption{\ensuremath{\Varid{parMapStream}} definition.}\label{fig:parMapStream}\end{figure}

\hypertarget{statically-load-balancing-parallel-map}{%
\subsection{\texorpdfstring{Statically load-balancing parallel
\ensuremath{\Varid{map}}}{Statically load-balancing parallel }}\label{statically-load-balancing-parallel-map}}

\label{sec:staticallyloadbalancing}

Our \ensuremath{\Varid{parMap}} spawns every single computation separately (at least for
the instances of \ensuremath{\Conid{ArrowParallel}} we presented in this thesis). This can
be quite wasteful and a statically load-balancing \ensuremath{\Varid{farm}} (Figs. \ref{fig:farmImg},
\ref{fig:farm}) that equally distributes the workload
over \ensuremath{\Varid{numCores}} workers seems useful. The definitions of the helper
functions \ensuremath{\Varid{unshuffle}} and \ensuremath{\Varid{shuffle}}, which are used for this
distribution, (Figure \ref{fig:edenshuffleetc}) originate from an Eden
skeleton\footnote{Available on Hackage under
  \url{https://hackage.haskell.org/package/edenskel-2.1.0.0/docs/src/Control-Parallel-Eden-Map.html}.}.

\begin{figure}[h]
\centering
\includegraphics{src/img/farmImg.pdf}
\caption{\ensuremath{\Varid{farm}} depiction.\label{fig:farmImg}}
\end{figure}

\begin{figure}[h]
\centering
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{type}\;\Conid{NumCores}\mathrel{=}\Conid{Int}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{farm}\mathbin{::}(\Conid{ArrowParallel}\;\Varid{arr}\;\Varid{a}\;\Varid{b}\;\Varid{conf},{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Conid{ArrowParallel}\;\Varid{arr}\;[\mskip1.5mu \Varid{a}\mskip1.5mu]\;[\mskip1.5mu \Varid{b}\mskip1.5mu]\;\Varid{conf},{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Conid{ArrowChoice}\;\Varid{arr})\Rightarrow {}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{conf}\to \Conid{NumCores}\to \Varid{arr}\;\Varid{a}\;\Varid{b}\to \Varid{arr}\;[\mskip1.5mu \Varid{a}\mskip1.5mu]\;[\mskip1.5mu \Varid{b}\mskip1.5mu]{}\<[E]%
\\
\>[B]{}\Varid{farm}\;\Varid{conf}\;\Varid{numCores}\;\Varid{f}\mathrel{=}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{unshuffle}\;\Varid{numCores}\mathbin{>\!\!>\!\!>}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{parEvalN}\;\Varid{conf}\;(\Varid{repeat}\;(\Varid{mapArr}\;\Varid{f}))\mathbin{>\!\!>\!\!>}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{shuffle}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\caption{\ensuremath{\Varid{farm}} definition.}\label{fig:farm}\end{figure}

Since a \ensuremath{\Varid{farm}} is basically just \ensuremath{\Varid{parMap}} with a different work
distribution, it has the same restrictions as \ensuremath{\Varid{parEvalN}} and \ensuremath{\Varid{parMap}}.
We can, however, define \ensuremath{\Varid{farmChunk}} (Figs. \ref{fig:farmChunkImg}, \ref{fig:farmChunk})
which uses \ensuremath{\Varid{parEvalNLazy}} instead of \ensuremath{\Varid{parEvalN}}.
Its definition is identical to the one for \ensuremath{\Varid{farm}} apart from the use of
\ensuremath{\Varid{parEvalNLazy}} instead of \ensuremath{\Varid{parEvalN}}.

\begin{figure}
\centering
\includegraphics{src/img/farmChunkImg.pdf}
\caption{\ensuremath{\Varid{farmChunk}} depiction.\label{fig:farmChunkImg}}
\end{figure}

\begin{figure}
\centering
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{type}\;\Conid{ChunkSize}\mathrel{=}\Conid{Int}{}\<[E]%
\\
\>[B]{}\mathbf{type}\;\Conid{NumCores}\mathrel{=}\Conid{Int}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{farmChunk}\mathbin{::}(\Conid{ArrowParallel}\;\Varid{arr}\;\Varid{a}\;\Varid{b}\;\Varid{conf},{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Conid{ArrowParallel}\;\Varid{arr}\;[\mskip1.5mu \Varid{a}\mskip1.5mu]\;[\mskip1.5mu \Varid{b}\mskip1.5mu]\;\Varid{conf},{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Conid{ArrowChoice}\;\Varid{arr},{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Conid{ArrowApply}\;\Varid{arr})\Rightarrow {}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{conf}\to \Conid{ChunkSize}\to \Conid{NumCores}\to \Varid{arr}\;\Varid{a}\;\Varid{b}\to \Varid{arr}\;[\mskip1.5mu \Varid{a}\mskip1.5mu]\;[\mskip1.5mu \Varid{b}\mskip1.5mu]{}\<[E]%
\\
\>[B]{}\Varid{farmChunk}\;\Varid{conf}\;\Varid{chunkSize}\;\Varid{numCores}\;\Varid{f}\mathrel{=}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{unshuffle}\;\Varid{numCores}\mathbin{>\!\!>\!\!>}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{parEvalNLazy}\;\Varid{conf}\;\Varid{chunkSize}\;(\Varid{repeat}\;(\Varid{mapArr}\;\Varid{f}))\mathbin{>\!\!>\!\!>}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{shuffle}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\caption{\ensuremath{\Varid{farmChunk}} definition.}\label{fig:farmChunk}\end{figure}

\hypertarget{further-development-of-parallel-arrows}{%
\chapter{Further development of Parallel
Arrows}\label{further-development-of-parallel-arrows}}

\label{sec:further-development}

With the basic PArrows API in place, we will develop the API even
further in this chapter. In Chapter \ref{sec:futures} we introduce the
concept of Futures that allows for direct inter-process communication.
Then, in Chapter \ref{sec:topology-skeletons} we use this concept to
define more sophisticated topological skeletons, namely a \ensuremath{\Varid{pipe}}, a
\ensuremath{\Varid{ring}} and a \ensuremath{\Varid{torus}}.

\hypertarget{futures}{%
\section{Futures}\label{futures}}

\label{sec:futures}

Consider the following outline parallel Arrow combinator:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{outlineCombinator}\mathbin{::}({}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Conid{ArrowParallel}\;\Varid{arr}\;\Varid{a}\;\Varid{b}\;(),{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Conid{ArrowParallel}\;\Varid{arr}\;\Varid{b}\;\Varid{c}\;())\Rightarrow {}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}[\mskip1.5mu \Varid{arr}\;\Varid{a}\;\Varid{b}\mskip1.5mu]\to [\mskip1.5mu \Varid{arr}\;\Varid{b}\;\Varid{c}\mskip1.5mu]\to \Varid{arr}\;[\mskip1.5mu \Varid{a}\mskip1.5mu]\;[\mskip1.5mu \Varid{c}\mskip1.5mu]{}\<[E]%
\\
\>[B]{}\Varid{outlineCombinator}\;\Varid{fs1}\;\Varid{fs2}\mathrel{=}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{parEvalN}\;()\;\Varid{fs1}\mathbin{>\!\!>\!\!>}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{rightRotate}\mathbin{>\!\!>\!\!>}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{parEvalN}\;()\;\Varid{fs2}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

In a distributed environment, this evaluates all \ensuremath{[\mskip1.5mu \Varid{arr}\;\Varid{a}\;\Varid{b}\mskip1.5mu]} in
parallel, sends the results back to the master node, rotates the input
once and then evaluates the \ensuremath{[\mskip1.5mu \Varid{arr}\;\Varid{b}\;\Varid{c}\mskip1.5mu]} in parallel to then gather the
input once again on the master node. Such situations arise, e.g.~in
scientific computations when data distributed across the nodes needs to
be transposed. A concrete example is 2D FFT computation (Berthold et
al.,
\protect\hyperlink{ref-Berthold2009-fft}{2009}\protect\hyperlink{ref-Berthold2009-fft}{c};
Gorlatch and Bischof, \protect\hyperlink{ref-Gorlatch}{1998}).

While the example could be rewritten into a single \ensuremath{\Varid{parEvalN}} call by
directly wiring the Arrows together before spawning, it illustrates an
important problem. When using a \ensuremath{\Conid{ArrowParallel}} backend that resides on
multiple computers, all communication between the nodes is done via the
master node, as shown in the Eden trace in Figure
\ref{fig:withoutFutures}. This can become a serious bottleneck for
larger amounts of data and number of processes as e.g. Berthold et al.
(\protect\hyperlink{ref-Berthold2009-fft}{2009}\protect\hyperlink{ref-Berthold2009-fft}{c})
showcases.

\begin{figure}
\centering
\includegraphics{src/img/withoutFutures.pdf}
\caption[Communication between four Eden processes without Futures.]{Communication between four Eden processes without Futures. All
communication goes through the master node. Each bar represents one
process. Black lines represent communication. Colours: blue \(\hat{=}\)
idle, green \(\hat{=}\) running, red \(\hat{=}\) blocked, yellow
\(\hat{=}\) suspended.\label{fig:withoutFutures}}
\end{figure}

This is usually only a problem in distributed memory where we should
allow nodes to communicate directly with each other. Eden already
provides \enquote{remote data} that enable this (Alt and Gorlatch,
\protect\hyperlink{ref-AlGo03a}{2003}; Dieterle et al.,
\protect\hyperlink{ref-Dieterle2010}{2010}\protect\hyperlink{ref-Dieterle2010}{b}).
But as we want code using our DSL to be agnostic in terms of which
backend is used, we have to wrap this concept. We do this with the
\ensuremath{\Conid{Future}} type class to abstract the idea of handles on data that can be
passed between nodes:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{class}\;\Conid{Future}\;\Varid{fut}\;\Varid{a}\;\Varid{conf}\mid \Varid{a}\;\Varid{conf}\to \Varid{fut}\;\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{put}\mathbin{::}(\Conid{Arrow}\;\Varid{arr})\Rightarrow \Varid{conf}\to \Varid{arr}\;\Varid{a}\;(\Varid{fut}\;\Varid{a}){}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{get}\mathbin{::}(\Conid{Arrow}\;\Varid{arr})\Rightarrow \Varid{conf}\to \Varid{arr}\;(\Varid{fut}\;\Varid{a})\;\Varid{a}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

A \ensuremath{\Varid{conf}} parameter is required here as well, but only so that Haskells
type system allows us to have multiple Future implementations imported
at once without breaking any dependencies similar to what we did with
the \ensuremath{\Conid{ArrowParallel}} type class earlier. However, we can obviously yet
again define default utility instances \ensuremath{\Conid{Future}\;\Varid{fut}\;\Varid{a}\;()} for each
backend similar to how \ensuremath{\Conid{ArrowParallel}\;\Varid{arr}\;\Varid{a}\;\Varid{b}\;()} was defined in Chapter
\ref{sec:parallel-arrows} as we will shortly see in the implementations
for the backends.

Maybe even more interestingly, we use a functional dependency
\ensuremath{\Varid{a}\;\Varid{conf}\to \Varid{fut}} in the definition. This means that the type of \ensuremath{\Varid{fut}} can
always be fully determined from the actual types of \ensuremath{\Varid{a}} and \ensuremath{\Varid{conf}}. We
need this because we do not want users of our DSL to have to rely on a
specific type of Future in their code. They only have to declare that
they require a compatible Future type and do not need to worry about any
specifics. This can be seen in the Future version of \ensuremath{\Varid{outlineCombinator}}
we will define soon.

In order to implement this type class for Eden and since \ensuremath{\Conid{RD}} is only a
type synonym for a communication type that is used internally in their
library, we have to use some wrapper classes to fit the type class, as
the following code showcases\footnote{Instances of type classes can not
  be declared on type synonyms.}:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{data}\;\Conid{RemoteData}\;\Varid{a}\mathrel{=}\Conid{RD}\;\{\mskip1.5mu \Varid{rd}\mathbin{::}\Conid{RD}\;\Varid{a}\mskip1.5mu\}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{put'}\mathbin{::}(\Conid{Arrow}\;\Varid{arr})\Rightarrow \Varid{arr}\;\Varid{a}\;(\Conid{BasicFuture}\;\Varid{a}){}\<[E]%
\\
\>[B]{}\Varid{put'}\mathrel{=}\Varid{arr}\;\Conid{BF}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{get'}\mathbin{::}(\Conid{Arrow}\;\Varid{arr})\Rightarrow \Varid{arr}\;(\Conid{BasicFuture}\;\Varid{a})\;\Varid{a}{}\<[E]%
\\
\>[B]{}\Varid{get'}\mathrel{=}\Varid{arr}\;(\lambda (\mathord{\sim}(\Conid{BF}\;\Varid{a}))\to \Varid{a}){}\<[E]%
\\[\blanklineskip]%
\>[B]{}\mathbf{instance}\;\Conid{NFData}\;(\Conid{RemoteData}\;\Varid{a})\;\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{rnf}\mathrel{=}\Varid{rnf}\mathbin{\circ}\Varid{rd}{}\<[E]%
\\
\>[B]{}\mathbf{instance}\;\Conid{Trans}\;(\Conid{RemoteData}\;\Varid{a}){}\<[E]%
\\[\blanklineskip]%
\>[B]{}\mathbf{instance}\;(\Conid{Trans}\;\Varid{a})\Rightarrow \Conid{Future}\;\Conid{RemoteData}\;\Varid{a}\;\Conid{Conf}\;\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{put}\;\anonymous \mathrel{=}\Varid{put'}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{get}\;\anonymous \mathrel{=}\Varid{get'}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\mathbf{instance}\;(\Conid{Trans}\;\Varid{a})\Rightarrow \Conid{Future}\;\Conid{RemoteData}\;\Varid{a}\;()\;\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{put}\;\anonymous \mathrel{=}\Varid{put'}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{get}\;\anonymous \mathrel{=}\Varid{get'}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

For GpH and \ensuremath{\Conid{Par}} Monad, we can simply use \ensuremath{\Conid{BasicFuture}}s, which are
just simple wrappers around the actual data with boiler-plate logic so
that the type class is satisfied. This is because the concept of a
\ensuremath{\Conid{Future}} does not change anything for shared-memory execution as there
are no communication problems to fix. Nevertheless, we require a common
interface so the parallel Arrows are portable across backends. Here, the
implementation is:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{data}\;\Conid{BasicFuture}\;\Varid{a}\mathrel{=}\Conid{BF}\;\Varid{a}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{put'}\mathbin{::}(\Conid{Arrow}\;\Varid{arr})\Rightarrow \Varid{arr}\;\Varid{a}\;(\Conid{BasicFuture}\;\Varid{a}){}\<[E]%
\\
\>[B]{}\Varid{put'}\mathrel{=}\Varid{arr}\;\Conid{BF}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{get'}\mathbin{::}(\Conid{Arrow}\;\Varid{arr})\Rightarrow \Varid{arr}\;(\Conid{BasicFuture}\;\Varid{a})\;\Varid{a}{}\<[E]%
\\
\>[B]{}\Varid{get'}\mathrel{=}\Varid{arr}\;(\lambda (\mathord{\sim}(\Conid{BF}\;\Varid{a}))\to \Varid{a}){}\<[E]%
\\[\blanklineskip]%
\>[B]{}\mathbf{instance}\;\Conid{NFData}\;\Varid{a}\Rightarrow \Conid{NFData}\;(\Conid{BasicFuture}\;\Varid{a})\;\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{rnf}\;(\Conid{BF}\;\Varid{a})\mathrel{=}\Varid{rnf}\;\Varid{a}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\mathbf{instance}\;\Conid{Future}\;\Conid{BasicFuture}\;\Varid{a}\;(\Conid{Conf}\;\Varid{a})\;\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{put}\;\anonymous \mathrel{=}\Varid{put'}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{get}\;\anonymous \mathrel{=}\Varid{get'}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\mathbf{instance}\;\Conid{Future}\;\Conid{BasicFuture}\;\Varid{a}\;()\;\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{put}\;\anonymous \mathrel{=}\Varid{put'}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{get}\;\anonymous \mathrel{=}\Varid{get'}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

Now, we can use this \ensuremath{\Conid{Future}} concept in our communication example for
direct communication between nodes:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{outlineCombinator}\mathbin{::}({}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Conid{ArrowParallel}\;\Varid{arr}\;\Varid{a}\;(\Varid{fut}\;\Varid{b})\;(),{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Conid{ArrowParallel}\;\Varid{arr}\;(\Varid{fut}\;\Varid{b})\;\Varid{c}\;(),{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Conid{Future}\;\Varid{fut}\;\Varid{b}\;())\Rightarrow {}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}[\mskip1.5mu \Varid{arr}\;\Varid{a}\;\Varid{b}\mskip1.5mu]\to [\mskip1.5mu \Varid{arr}\;\Varid{b}\;\Varid{c}\mskip1.5mu]\to \Varid{arr}\;[\mskip1.5mu \Varid{a}\mskip1.5mu]\;[\mskip1.5mu \Varid{c}\mskip1.5mu]{}\<[E]%
\\
\>[B]{}\Varid{outlineCombinator}\;\Varid{fs1}\;\Varid{fs2}\mathrel{=}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{parEvalN}\;()\;(\Varid{map}\;(\mathbin{>\!\!>\!\!>}\Varid{put}\;())\;\Varid{fs1})\mathbin{>\!\!>\!\!>}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{rightRotate}\mathbin{>\!\!>\!\!>}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{parEvalN}\;()\;(\Varid{map}\;(\Varid{get}\;()\mathbin{>\!\!>\!\!>})\;\Varid{fs2}){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

In a distributed environment, this gives us a communication scheme with
messages going through the master node only if it is needed -- similar
to what is shown in the trace visualisation in Figure
\ref{fig:withFutures}. This is because only the handles to the data are
passed through the master node, while all communication of actual data
can happen between the actual nodes. We will build upon this concept in
more complicated combinators in the next chapter.

\begin{figure}
\centering
\includegraphics{src/img/withFutures.pdf}
\caption[Communication between four Eden processes with Futures.]{Communication between four Eden processes with Futures. Unlike in
Figure \ref{fig:withoutFutures}, processes communicate directly (one
example message is highlighted) instead of always going through the
master node (bottom bar).\label{fig:withFutures}}
\end{figure}

\newpage

\hypertarget{advanced-topological-skeletons}{%
\section{Advanced topological
skeletons}\label{advanced-topological-skeletons}}

\label{sec:topology-skeletons}

Even though many algorithms can be expressed by \ensuremath{\Varid{parMap}}s, some problems
require more sophisticated skeletons. The Eden library resolves this
problem and already comes with more predefined skeletons\footnote{Available
  on Hackage:
  \url{https://hackage.haskell.org/package/edenskel-2.1.0.0/docs/Control-Parallel-Eden-Topology.html}.},
among them a \ensuremath{\Varid{pipe}}, a \ensuremath{\Varid{ring}}, and a \ensuremath{\Varid{torus}} implementation (Loogen et
al., \protect\hyperlink{ref-Eden:SkeletonBookChapter02}{2003}). These
seem like reasonable candidates to be ported to our Arrow-based parallel
Haskell. By doing so, we aim to showcase that we can express more
sophisticated skeletons with parallel Arrows as well.

If we were to use the original definition of \ensuremath{\Varid{parEvalN}}, however, these
skeletons would produce an infinite loop with the GpH and \ensuremath{\Conid{Par}} Monad
which during runtime would result in the program crashing. This
materialises with the usage of \ensuremath{\Varid{loop}} of the \ensuremath{\Conid{ArrowLoop}} type class and
we think that this is due to difference of how evaluation is done in
these backends compared to Eden. An investigation of why this difference
exists is beyond the scope of this work -- the results of the
experimental Cloud Haskell backend in Chapter
\ref{sec:CloudHaskellArrowParallelLimitsMitigation} touch on the likely
root cause of this problem, though. We only provide a workaround for
these types of skeletons as such they probably are not of much
importance outside of a distributed memory environment. Nevertheless,
our workaround enables users of the DSL to test their code within a
shared memory setting.

The idea of the fix is to provide a \ensuremath{\Conid{ArrowLoopParallel}} type class that
has two functions -- \ensuremath{\Varid{loopParEvalN}} and \ensuremath{\Varid{postLoopParEvalN}}. The first is
to be used inside an \ensuremath{\Varid{loop}} construct while the latter will be used
right outside of the \ensuremath{\Varid{loop}}. This way, we can delegate to the actual
\ensuremath{\Varid{parEvalN}} in the spot where the backend supports it.


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{class}\;\Conid{ArrowParallel}\;\Varid{arr}\;\Varid{a}\;\Varid{b}\;\Varid{conf}\Rightarrow {}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Conid{ArrowLoopParallel}\;\Varid{arr}\;\Varid{a}\;\Varid{b}\;\Varid{conf}\;\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{loopParEvalN}\mathbin{::}\Varid{conf}\to [\mskip1.5mu \Varid{arr}\;\Varid{a}\;\Varid{b}\mskip1.5mu]\to \Varid{arr}\;[\mskip1.5mu \Varid{a}\mskip1.5mu]\;[\mskip1.5mu \Varid{b}\mskip1.5mu]{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{postLoopParEvalN}\mathbin{::}\Varid{conf}\to [\mskip1.5mu \Varid{arr}\;\Varid{a}\;\Varid{b}\mskip1.5mu]\to \Varid{arr}\;[\mskip1.5mu \Varid{a}\mskip1.5mu]\;[\mskip1.5mu \Varid{b}\mskip1.5mu]{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

Because Eden has no problems with the looping skeletons, we use this
instance:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{instance}\;(\Conid{ArrowChoice}\;\Varid{arr},\Conid{ArrowParallel}\;\Varid{arr}\;\Varid{a}\;\Varid{b}\;\Conid{Conf})\Rightarrow {}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Conid{ArrowLoopParallel}\;\Varid{arr}\;\Varid{a}\;\Varid{b}\;\Conid{Conf}\;\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{loopParEvalN}\mathrel{=}\Varid{parEvalN}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{postLoopParEvalN}\;\anonymous \mathrel{=}\Varid{evalN}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

The \ensuremath{\Conid{Par}} Monad and GpH implementations of \ensuremath{\Varid{parEvalN}} have problems
inside of \ensuremath{\Varid{loop}}. Their respective instances for \ensuremath{\Conid{ArrowLoopParallel}}
look like this:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{instance}\;(\Conid{ArrowChoice}\;\Varid{arr},\Conid{ArrowParallel}\;\Varid{arr}\;\Varid{a}\;\Varid{b}\;(\Conid{Conf}\;\Varid{b}))\Rightarrow {}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Conid{ArrowLoopParallel}\;\Varid{arr}\;\Varid{a}\;\Varid{b}\;(\Conid{Conf}\;\Varid{b})\;\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{loopParEvalN}\;\anonymous \mathrel{=}\Varid{evalN}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{postLoopParEvalN}\mathrel{=}\Varid{parEvalN}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

Chapter \ref{sec:pipe} explains how to achieve a parallel \ensuremath{\Varid{pipe}} with
our DSL. Then, Chapter \ref{sec:ring} goes into detail on how to achieve
a \ensuremath{\Varid{ring}} skeleton, which we then extend to achieve a \ensuremath{\Varid{torus}} in Chapter
\ref{sec:torus}.

\hypertarget{parallel-pipe}{%
\subsection{Parallel pipe}\label{parallel-pipe}}

\label{sec:pipe}

We start with the parallel \ensuremath{\Varid{pipe}} skeleton, which is semantically
equivalent to folding over a list \ensuremath{[\mskip1.5mu \Varid{arr}\;\Varid{a}\;\Varid{a}\mskip1.5mu]} of Arrows with \ensuremath{\mathbin{>\!\!>\!\!>}}, but
in parallel, meaning that the Arrows do not have to reside on the same
thread/machine. We implement this skeleton using the \ensuremath{\Conid{ArrowLoop}} type
class which provides us with the \ensuremath{\Varid{loop}\mathbin{::}\Varid{arr}\;(\Varid{a},\Varid{b})\;(\Varid{c},\Varid{b})\to \Varid{arr}\;\Varid{a}\;\Varid{c}}
combinator allowing us to express recursive fix-point computations in
which output values are fed back as input. For example


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{loop}\;(\Varid{arr}\;(\lambda (\Varid{a},\Varid{b})\to (\Varid{b},\Varid{a}\mathbin{:}\Varid{b}))){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

which is the same as


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{loop}\;(\Varid{arr}\;\Varid{snd}\mathbin{\&\!\&\!\&}\Varid{arr}\;(\Varid{uncurry}\;(\mathbin{:}))){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

defines an Arrow that takes its input \ensuremath{\Varid{a}} and converts it into an
infinite stream \ensuremath{[\mskip1.5mu \Varid{a}\mskip1.5mu]} of it. Using \ensuremath{\Varid{loop}} to our advantage and adapting
it to apply our list of functions in sequence gives us a first draft of
a pipe implementation (Figure \ref{fig:pipeSimple}): We plug the
parallel evaluation call \ensuremath{\Varid{loopParEvalN}\;\Varid{conf}\;\Varid{fs}} inside the second
argument of \ensuremath{\mathbin{\&\!\&\!\&}} and then only pick the first element of the resulting
list with \ensuremath{\Varid{arr}\;\Varid{last}} outside of the \ensuremath{\Varid{loop}}. Here, the length of the
input list of Arrows determines when the loop stops. For finite input
lengths this means that the computation will definitely finish.

\begin{figure}[t]
\centering
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{9}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{pipeSimple}\mathbin{::}(\Conid{ArrowLoop}\;\Varid{arr},\Conid{ArrowLoopParallel}\;\Varid{arr}\;\Varid{a}\;\Varid{a}\;\Varid{conf})\Rightarrow {}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{conf}\to [\mskip1.5mu \Varid{arr}\;\Varid{a}\;\Varid{a}\mskip1.5mu]\to \Varid{arr}\;\Varid{a}\;\Varid{a}{}\<[E]%
\\
\>[B]{}\Varid{pipeSimple}\;\Varid{conf}\;\Varid{fs}\mathrel{=}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{loop}\;(\Varid{arr}\;\Varid{snd}\mathbin{\&\!\&\!\&}{}\<[E]%
\\
\>[5]{}\hsindent{4}{}\<[9]%
\>[9]{}(\Varid{arr}\;(\Varid{uncurry}\;(\mathbin{:})\mathbin{>\!\!>\!\!>}\Varid{lazy})\mathbin{>\!\!>\!\!>}\Varid{loopParEvalN}\;\Varid{conf}\;\Varid{fs}))\mathbin{>\!\!>\!\!>}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{arr}\;\Varid{last}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\caption[Simple \ensuremath{\Varid{pipe}} skeleton.]{Simple \ensuremath{\Varid{pipe}} skeleton. The use of \ensuremath{\Varid{lazy}} (Figure \ref{fig:edenlazyrightrotate}) is essential as without it programs using this definition would never halt. We need to ensure that the evaluation of the input \ensuremath{[\mskip1.5mu \Varid{a}\mskip1.5mu]} is not forced fully before passing it into \ensuremath{\Varid{loopParEvalN}}.}\label{fig:pipeSimple}\end{figure}

However, using this definition directly will make the master node a
potential bottleneck in distributed environments just like the first
version of the outline combinator in Chapter \ref{sec:futures}.
Therefore, we introduce a more sophisticated version that internally
uses Futures and obtain the final definition of \ensuremath{\Varid{pipe}} in Figure
\ref{fig:pipe}.

\begin{figure}[t]
\centering
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{pipe}\mathbin{::}(\Conid{ArrowLoop}\;\Varid{arr},{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Conid{ArrowLoopParallel}\;\Varid{arr}\;(\Varid{fut}\;\Varid{a})\;(\Varid{fut}\;\Varid{a})\;\Varid{conf},{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Conid{Future}\;\Varid{fut}\;\Varid{a}\;\Varid{conf})\Rightarrow {}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{conf}\to [\mskip1.5mu \Varid{arr}\;\Varid{a}\;\Varid{a}\mskip1.5mu]\to \Varid{arr}\;\Varid{a}\;\Varid{a}{}\<[E]%
\\
\>[B]{}\Varid{pipe}\;\Varid{conf}\;\Varid{fs}\mathrel{=}\Varid{unliftFut}\;\Varid{conf}\;(\Varid{pipeSimple}\;\Varid{conf}\;(\Varid{map}\;(\Varid{liftFut}\;\Varid{conf})\;\Varid{fs})){}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{liftFut}\mathbin{::}(\Conid{Arrow}\;\Varid{arr},\Conid{Future}\;\Varid{fut}\;\Varid{a}\;\Varid{conf},\Conid{Future}\;\Varid{fut}\;\Varid{b}\;\Varid{conf})\Rightarrow {}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{conf}\to \Varid{arr}\;\Varid{a}\;\Varid{b}\to \Varid{arr}\;(\Varid{fut}\;\Varid{a})\;(\Varid{fut}\;\Varid{b}){}\<[E]%
\\
\>[B]{}\Varid{liftFut}\;\Varid{conf}\;\Varid{f}\mathrel{=}\Varid{get}\;\Varid{conf}\mathbin{>\!\!>\!\!>}\Varid{f}\mathbin{>\!\!>\!\!>}\Varid{put}\;\Varid{conf}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{unliftFut}\mathbin{::}(\Conid{Arrow}\;\Varid{arr},\Conid{Future}\;\Varid{fut}\;\Varid{a}\;\Varid{conf},\Conid{Future}\;\Varid{fut}\;\Varid{b}\;\Varid{conf})\Rightarrow {}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{conf}\to \Varid{arr}\;(\Varid{fut}\;\Varid{a})\;(\Varid{fut}\;\Varid{b})\to \Varid{arr}\;\Varid{a}\;\Varid{b}{}\<[E]%
\\
\>[B]{}\Varid{unliftFut}\;\Varid{conf}\;\Varid{f}\mathrel{=}\Varid{put}\;\Varid{conf}\mathbin{>\!\!>\!\!>}\Varid{f}\mathbin{>\!\!>\!\!>}\Varid{get}\;\Varid{conf}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\caption{\ensuremath{\Varid{pipe}} skeleton definition with Futures.}\label{fig:pipe}\end{figure}

Sometimes, this \ensuremath{\Varid{pipe}} definition can be a bit inconvenient, especially
if we want to pipe Arrows of mixed types together, i.e. \ensuremath{\Varid{arr}\;\Varid{a}\;\Varid{b}} and
\ensuremath{\Varid{arr}\;\Varid{b}\;\Varid{c}}. By wrapping these two Arrows inside a bigger Arrow
\ensuremath{\Varid{arr}\;(([\mskip1.5mu \Varid{a}\mskip1.5mu],[\mskip1.5mu \Varid{b}\mskip1.5mu]),[\mskip1.5mu \Varid{c}\mskip1.5mu])\;(([\mskip1.5mu \Varid{a}\mskip1.5mu],[\mskip1.5mu \Varid{b}\mskip1.5mu]),[\mskip1.5mu \Varid{c}\mskip1.5mu])} suitable for \ensuremath{\Varid{pipe}}, we can
define \ensuremath{\Varid{pipe2}} as in Figure \ref{fig:pipe2}.

\begin{figure}[t]
\centering
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{9}{@{}>{\hspre}l<{\hspost}@{}}%
\column{13}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{pipe2}\mathbin{::}(\Conid{ArrowLoop}\;\Varid{arr},\Conid{ArrowChoice}\;\Varid{arr},{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Conid{ArrowLoopParallel}\;\Varid{arr}\;(\Varid{fut}\;(([\mskip1.5mu \Varid{a}\mskip1.5mu],[\mskip1.5mu \Varid{b}\mskip1.5mu]),[\mskip1.5mu \Varid{c}\mskip1.5mu]))\;(\Varid{fut}\;(([\mskip1.5mu \Varid{a}\mskip1.5mu],[\mskip1.5mu \Varid{b}\mskip1.5mu]),[\mskip1.5mu \Varid{c}\mskip1.5mu]))\;\Varid{conf},{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Conid{Future}\;\Varid{fut}\;(([\mskip1.5mu \Varid{a}\mskip1.5mu],[\mskip1.5mu \Varid{b}\mskip1.5mu]),[\mskip1.5mu \Varid{c}\mskip1.5mu])\;\Varid{conf})\Rightarrow {}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{conf}\to \Varid{arr}\;\Varid{a}\;\Varid{b}\to \Varid{arr}\;\Varid{b}\;\Varid{c}\to \Varid{arr}\;\Varid{a}\;\Varid{c}{}\<[E]%
\\
\>[B]{}\Varid{pipe2}\;\Varid{conf}\;\Varid{f}\;\Varid{g}\mathrel{=}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}(\Varid{arr}\;\Varid{return}\mathbin{\&\!\&\!\&}\Varid{arr}\;(\Varid{const}\;[\mskip1.5mu \mskip1.5mu]))\mathbin{\&\!\&\!\&}\Varid{arr}\;(\Varid{const}\;[\mskip1.5mu \mskip1.5mu])\mathbin{>\!\!>\!\!>}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{pipe}\;\Varid{conf}\;(\Varid{replicate}\;\mathrm{2}\;(\Varid{unify}\;\Varid{f}\;\Varid{g}))\mathbin{>\!\!>\!\!>}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{arr}\;\Varid{snd}\mathbin{>\!\!>\!\!>}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{arr}\;\Varid{head}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\mathbf{where}{}\<[E]%
\\
\>[5]{}\hsindent{4}{}\<[9]%
\>[9]{}\Varid{unify}\mathbin{::}(\Conid{ArrowChoice}\;\Varid{arr})\Rightarrow {}\<[E]%
\\
\>[9]{}\hsindent{4}{}\<[13]%
\>[13]{}\Varid{arr}\;\Varid{a}\;\Varid{b}\to \Varid{arr}\;\Varid{b}\;\Varid{c}\to \Varid{arr}\;(([\mskip1.5mu \Varid{a}\mskip1.5mu],[\mskip1.5mu \Varid{b}\mskip1.5mu]),[\mskip1.5mu \Varid{c}\mskip1.5mu])\;(([\mskip1.5mu \Varid{a}\mskip1.5mu],[\mskip1.5mu \Varid{b}\mskip1.5mu]),[\mskip1.5mu \Varid{c}\mskip1.5mu]){}\<[E]%
\\
\>[5]{}\hsindent{4}{}\<[9]%
\>[9]{}\Varid{unify}\;\Varid{f'}\;\Varid{g'}\mathrel{=}{}\<[E]%
\\
\>[9]{}\hsindent{4}{}\<[13]%
\>[13]{}(\Varid{mapArr}\;\Varid{f'}\mathbin{*\!*\!*}\Varid{mapArr}\;\Varid{g'})\mathbin{*\!*\!*}\Varid{arr}\;(\Varid{const}\;[\mskip1.5mu \mskip1.5mu])\mathbin{>\!\!>\!\!>}{}\<[E]%
\\
\>[9]{}\hsindent{4}{}\<[13]%
\>[13]{}\Varid{arr}\;(\lambda ((\Varid{b},\Varid{c}),\Varid{a})\to ((\Varid{a},\Varid{b}),\Varid{c})){}\<[E]%
\\[\blanklineskip]%
\>[B]{}(\mathbin{|\!>\!\!>\!\!>\!|})\mathbin{::}(\Conid{ArrowLoop}\;\Varid{arr},\Conid{ArrowChoice}\;\Varid{arr},{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Conid{ArrowLoopParallel}\;\Varid{arr}\;(\Varid{fut}\;(([\mskip1.5mu \Varid{a}\mskip1.5mu],[\mskip1.5mu \Varid{b}\mskip1.5mu]),[\mskip1.5mu \Varid{c}\mskip1.5mu]))\;(\Varid{fut}\;(([\mskip1.5mu \Varid{a}\mskip1.5mu],[\mskip1.5mu \Varid{b}\mskip1.5mu]),[\mskip1.5mu \Varid{c}\mskip1.5mu]))\;(),{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Conid{Future}\;\Varid{fut}\;(([\mskip1.5mu \Varid{a}\mskip1.5mu],[\mskip1.5mu \Varid{b}\mskip1.5mu]),[\mskip1.5mu \Varid{c}\mskip1.5mu])\;())\Rightarrow {}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{arr}\;\Varid{a}\;\Varid{b}\to \Varid{arr}\;\Varid{b}\;\Varid{c}\to \Varid{arr}\;\Varid{a}\;\Varid{c}{}\<[E]%
\\
\>[B]{}(\mathbin{|\!>\!\!>\!\!>\!|})\mathrel{=}\Varid{pipe2}\;(){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\caption{Definition of \ensuremath{\Varid{pipe2}} and \ensuremath{\mathbin{|\!>\!\!>\!\!>\!|}}, a parallel \ensuremath{\mathbin{>\!\!>\!\!>}}.}\label{fig:pipe2}\end{figure}

Extensive use of \ensuremath{\Varid{pipe2}} over \ensuremath{\Varid{pipe}} with a hand-written combination
data type will probably result in worse performance because of more
communication overhead from the many calls to \ensuremath{\Varid{parEvalN}} inside of
\ensuremath{\Varid{evalN}}. Nonetheless, we can define a parallel piping operator
\ensuremath{\mathbin{|\!>\!\!>\!\!>\!|}}, which is semantically equivalent to \ensuremath{\mathbin{>\!\!>\!\!>}} similarly to other
parallel syntactic sugar from Appendix \ref{syntacticSugar}.

\hypertarget{ring-skeleton}{%
\subsection{Ring skeleton}\label{ring-skeleton}}

\label{sec:ring}

\begin{figure}
\centering
\includegraphics{src/img/ringImg.pdf}
\caption{\ensuremath{\Varid{ring}} skeleton depiction.\label{fig:ringImg}}
\end{figure}

Eden comes with a ring skeleton\footnote{Available on Hackage:
  \url{https://hackage.haskell.org/package/edenskel-2.1.0.0/docs/Control-Parallel-Eden-Topology.html}.}
(Figure \ref{fig:ringImg}) implementation that allows the computation of
a function \ensuremath{[\mskip1.5mu \Varid{i}\mskip1.5mu]\to [\mskip1.5mu \Varid{o}\mskip1.5mu]} with a ring of nodes that communicate with each
other. Its input is a node function \ensuremath{\Varid{i}\to \Varid{r}\to (\Varid{o},\Varid{r})} in which \ensuremath{\Varid{r}}
serves as the intermediary output that is sent to the neighbour of each
node. It uses the direct \enquote{remote data} communication channels
that were already mentioned in Chapter \ref{sec:futures}. We depict it
in the Appendix, Figure \ref{fig:ringEden}.

We can rewrite this functionality easily with the use of \ensuremath{\Varid{loop}} as the
definition of the node function, \ensuremath{\Varid{arr}\;(\Varid{i},\Varid{r})\;(\Varid{o},\Varid{r})}, after being
transformed into an Arrow, already fits quite neatly into \ensuremath{\Varid{loop}}'s
signature: \ensuremath{\Varid{arr}\;(\Varid{a},\Varid{b})\;(\Varid{c},\Varid{b})\to \Varid{arr}\;\Varid{a}\;\Varid{c}}. In each iteration we start by
rotating the intermediary input from the nodes \ensuremath{[\mskip1.5mu \Varid{fut}\;\Varid{r}\mskip1.5mu]} with
\ensuremath{\Varid{second}\;(\Varid{rightRotate}\mathbin{>\!\!>\!\!>}\Varid{lazy})} (Figure \ref{fig:edenlazyrightrotate}).
Similarly to the \ensuremath{\Varid{pipe}} from Chapter \ref{sec:pipe} (Figure
\ref{fig:pipeSimple}), we have to feed the intermediary input into our
\ensuremath{\Varid{lazy}} (Figure \ref{fig:edenlazyrightrotate}) Arrow here, or the
evaluation would fail to terminate. The reasoning is explained by Loogen
(\protect\hyperlink{ref-Loogen2012}{2012}) as a demand problem.

Next, we zip the resulting \ensuremath{([\mskip1.5mu \Varid{i}\mskip1.5mu],[\mskip1.5mu \Varid{fut}\;\Varid{r}\mskip1.5mu])} to \ensuremath{[\mskip1.5mu (\Varid{i},\Varid{fut}\;\Varid{r})\mskip1.5mu]} with
\ensuremath{\Varid{arr}\;(\Varid{uncurry}\;\Varid{zip})}. We then feed this into our parallel Arrow
\ensuremath{\Varid{arr}\;[\mskip1.5mu (\Varid{i},\Varid{fut}\;\Varid{r})\mskip1.5mu]\;[\mskip1.5mu (\Varid{o},\Varid{fut}\;\Varid{r})\mskip1.5mu]} obtained by transforming our input Arrow
\ensuremath{\Varid{f}\mathbin{::}\Varid{arr}\;(\Varid{i},\Varid{r})\;(\Varid{o},\Varid{r})} into \ensuremath{\Varid{arr}\;(\Varid{i},\Varid{fut}\;\Varid{r})\;(\Varid{o},\Varid{fut}\;\Varid{r})} before
\ensuremath{\Varid{repeat}}ing and lifting it with \ensuremath{\Varid{loopParEvalN}}. Finally, we unzip the
output list \ensuremath{[\mskip1.5mu (\Varid{o},\Varid{fut}\;\Varid{r})\mskip1.5mu]} into \ensuremath{([\mskip1.5mu \Varid{o}\mskip1.5mu],[\mskip1.5mu \Varid{fut}\;\Varid{r}\mskip1.5mu])}.

Plugging this Arrow \ensuremath{\Varid{arr}\;([\mskip1.5mu \Varid{i}\mskip1.5mu],[\mskip1.5mu \Varid{fut}\;\Varid{r}\mskip1.5mu])\;([\mskip1.5mu \Varid{o}\mskip1.5mu],\Varid{fut}\;\Varid{r})} into the
definition of \ensuremath{\Varid{loop}} from earlier gives us \ensuremath{\Varid{arr}\;[\mskip1.5mu \Varid{i}\mskip1.5mu]\;[\mskip1.5mu \Varid{o}\mskip1.5mu]}, our ring Arrow
(Figure \ref{fig:ringFinal}). To make sure this algorithm has speedup on
shared-memory machines as well, we pass the result of this Arrow to
\ensuremath{\Varid{postLoopParEvalN}\;\Varid{conf}\;(\Varid{repeat}\;(\Varid{arr}\;\Varid{id}))}. This combinator can, for
example, be used to calculate the shortest paths in a graph using
Warshall's algorithm.

\begin{figure}[t]
\centering
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{9}{@{}>{\hspre}l<{\hspost}@{}}%
\column{13}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{ring}\mathbin{::}(\Conid{Future}\;\Varid{fut}\;\Varid{r}\;\Varid{conf},{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Conid{ArrowLoop}\;\Varid{arr},{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Conid{ArrowLoopParallel}\;\Varid{arr}\;(\Varid{i},\Varid{fut}\;\Varid{r})\;(\Varid{o},\Varid{fut}\;\Varid{r})\;\Varid{conf},{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Conid{ArrowLoopParallel}\;\Varid{arr}\;\Varid{o}\;\Varid{o}\;\Varid{conf})\Rightarrow {}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{conf}\to \Varid{arr}\;(\Varid{i},\Varid{r})\;(\Varid{o},\Varid{r})\to \Varid{arr}\;[\mskip1.5mu \Varid{i}\mskip1.5mu]\;[\mskip1.5mu \Varid{o}\mskip1.5mu]{}\<[E]%
\\
\>[B]{}\Varid{ring}\;\Varid{conf}\;\Varid{f}\mathrel{=}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{loop}\;(\Varid{second}\;(\Varid{rightRotate}\mathbin{>\!\!>\!\!>}\Varid{lazy})\mathbin{>\!\!>\!\!>}{}\<[E]%
\\
\>[5]{}\hsindent{4}{}\<[9]%
\>[9]{}\Varid{arr}\;(\Varid{uncurry}\;\Varid{zip})\mathbin{>\!\!>\!\!>}{}\<[E]%
\\
\>[5]{}\hsindent{4}{}\<[9]%
\>[9]{}\Varid{loopParEvalN}\;\Varid{conf}\;{}\<[E]%
\\
\>[9]{}\hsindent{4}{}\<[13]%
\>[13]{}(\Varid{repeat}\;(\Varid{second}\;(\Varid{get}\;\Varid{conf})\mathbin{>\!\!>\!\!>}\Varid{f}\mathbin{>\!\!>\!\!>}\Varid{second}\;(\Varid{put}\;\Varid{conf})))\mathbin{>\!\!>\!\!>}{}\<[E]%
\\
\>[5]{}\hsindent{4}{}\<[9]%
\>[9]{}\Varid{arr}\;\Varid{unzip})\mathbin{>\!\!>\!\!>}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{postLoopParEvalN}\;\Varid{conf}\;(\Varid{repeat}\;(\Varid{arr}\;\Varid{id})){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\caption{\ensuremath{\Varid{ring}} skeleton definition.}\label{fig:ringFinal}\end{figure}

\hypertarget{torus-skeleton}{%
\subsection{Torus skeleton}\label{torus-skeleton}}

\label{sec:torus}

\begin{figure}
\centering
\includegraphics{src/img/ringTorusImg.pdf}
\caption{\ensuremath{\Varid{torus}} skeleton depiction.\label{fig:ringTorusImg}}
\end{figure}

If we take the concept of a \ensuremath{\Varid{ring}} from Chapter \ref{sec:ring} one
dimension further, we obtain a \ensuremath{\Varid{torus}} skeleton (Figure
\ref{fig:ringTorusImg}, \ref{fig:torus}). In a \ensuremath{\Varid{torus}}, every node sends
and receives data from horizontal and vertical neighbours in each
communication round. With our Parallel Arrows, we re-implement this
combinator from Eden\footnote{Available on Hackage:
  \url{https://hackage.haskell.org/package/edenskel-2.1.0.0/docs/Control-Parallel-Eden-Topology.html}.}
-- yet again with the help of the \ensuremath{\Conid{ArrowLoop}} type class.

Similar to the \ensuremath{\Varid{ring}}, we start by rotating the input (Figure
\ref{fig:edenlazyrightrotate}), but this time not only in one direction,
but in two. This means that the intermediary input from the neighbour
nodes has to be stored in a tuple \ensuremath{([\mskip1.5mu [\mskip1.5mu \Varid{fut}\;\Varid{a}\mskip1.5mu]\mskip1.5mu],[\mskip1.5mu [\mskip1.5mu \Varid{fut}\;\Varid{b}\mskip1.5mu]\mskip1.5mu])} in the second
argument (\ensuremath{\Varid{loop}} only allows for two arguments) of our looped Arrow of
type


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{arr}\;([\mskip1.5mu [\mskip1.5mu \Varid{c}\mskip1.5mu]\mskip1.5mu],([\mskip1.5mu [\mskip1.5mu \Varid{fut}\;\Varid{a}\mskip1.5mu]\mskip1.5mu],[\mskip1.5mu [\mskip1.5mu \Varid{fut}\;\Varid{b}\mskip1.5mu]\mskip1.5mu]))\;([\mskip1.5mu [\mskip1.5mu \Varid{d}\mskip1.5mu]\mskip1.5mu],([\mskip1.5mu [\mskip1.5mu \Varid{fut}\;\Varid{a}\mskip1.5mu]\mskip1.5mu],[\mskip1.5mu [\mskip1.5mu \Varid{fut}\;\Varid{b}\mskip1.5mu]\mskip1.5mu])){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

and our rotation Arrow becomes


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{second}\;((\Varid{mapArr}\;\Varid{rightRotate}\mathbin{>\!\!>\!\!>}\Varid{lazy})\mathbin{*\!*\!*}(\Varid{arr}\;\Varid{rightRotate}\mathbin{>\!\!>\!\!>}\Varid{lazy})){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

instead of the singular rotation in the ring as we rotate \ensuremath{[\mskip1.5mu [\mskip1.5mu \Varid{fut}\;\Varid{a}\mskip1.5mu]\mskip1.5mu]}
horizontally and \ensuremath{[\mskip1.5mu [\mskip1.5mu \Varid{fut}\;\Varid{b}\mskip1.5mu]\mskip1.5mu]} vertically. Then, we zip the inputs for the
input Arrow with


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{arr}\;(\Varid{uncurry3}\;\Varid{zipWith3}\;\Varid{lazyzip3}){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

from \ensuremath{([\mskip1.5mu [\mskip1.5mu \Varid{c}\mskip1.5mu]\mskip1.5mu],([\mskip1.5mu [\mskip1.5mu \Varid{fut}\;\Varid{a}\mskip1.5mu]\mskip1.5mu],[\mskip1.5mu [\mskip1.5mu \Varid{fut}\;\Varid{b}\mskip1.5mu]\mskip1.5mu]))} to \ensuremath{[\mskip1.5mu [\mskip1.5mu (\Varid{c},\Varid{fut}\;\Varid{a},\Varid{fut}\;\Varid{b})\mskip1.5mu]\mskip1.5mu]}, which
we then evaluate in parallel.

This, however, is more complicated than in the \ensuremath{\Varid{ring}} case as we have
one more dimension of inputs that needs to be transformed. We first have
to \ensuremath{\Varid{shuffle}} all the inputs and then pass them into
\ensuremath{\Varid{loopParEvalN}\;\Varid{conf}\;(\Varid{repeat}\;(\Varid{ptorus}\;\Varid{conf}\;\Varid{f}))} to get an output of
\ensuremath{[\mskip1.5mu (\Varid{d},\Varid{fut}\;\Varid{a},\Varid{fut}\;\Varid{b})\mskip1.5mu]}. We then \ensuremath{\Varid{unshuffle}} this list back to its
original ordering by feeding it into \ensuremath{\Varid{arr}\;(\Varid{uncurry}\;\Varid{unshuffle})} which
takes the input length we saved one step earlier as additional input to
get a result matrix \ensuremath{[\mskip1.5mu [\mskip1.5mu [\mskip1.5mu (\Varid{d},\Varid{fut}\;\Varid{a},\Varid{fut}\;\Varid{b})\mskip1.5mu]\mskip1.5mu]}. Finally, we unpack this
matrix with \ensuremath{\Varid{arr}\;(\Varid{map}\;\Varid{unzip3})\mathbin{>\!\!>\!\!>}\Varid{arr}\;\Varid{unzip3}\mathbin{>\!\!>\!\!>}\Varid{threetotwo}} to get
\ensuremath{([\mskip1.5mu [\mskip1.5mu \Varid{d}\mskip1.5mu]\mskip1.5mu],([\mskip1.5mu [\mskip1.5mu \Varid{fut}\;\Varid{a}\mskip1.5mu]\mskip1.5mu],[\mskip1.5mu [\mskip1.5mu \Varid{fut}\;\Varid{b}\mskip1.5mu]\mskip1.5mu]))}.

This internal looping computation is once again fed into \ensuremath{\Varid{loop}} and we
also compose a final \ensuremath{\Varid{postLoopParEvalN}\;\Varid{conf}\;(\Varid{repeat}\;(\Varid{arr}\;\Varid{id}))} due to
the same problem with \ensuremath{\Varid{loop}} as explained for the \ensuremath{\Varid{ring}} skeleton.

\begin{figure}[t]
\centering
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{7}{@{}>{\hspre}l<{\hspost}@{}}%
\column{9}{@{}>{\hspre}l<{\hspost}@{}}%
\column{11}{@{}>{\hspre}l<{\hspost}@{}}%
\column{21}{@{}>{\hspre}l<{\hspost}@{}}%
\column{25}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{torus}\mathbin{::}(\Conid{Future}\;\Varid{fut}\;\Varid{a}\;\Varid{conf},\Conid{Future}\;\Varid{fut}\;\Varid{b}\;\Varid{conf},{}\<[E]%
\\
\>[B]{}\hsindent{7}{}\<[7]%
\>[7]{}\Conid{ArrowLoop}\;\Varid{arr},\Conid{ArrowChoice}\;\Varid{arr},{}\<[E]%
\\
\>[B]{}\hsindent{7}{}\<[7]%
\>[7]{}\Conid{ArrowLoopParallel}\;\Varid{arr}\;(\Varid{c},\Varid{fut}\;\Varid{a},\Varid{fut}\;\Varid{b})\;(\Varid{d},\Varid{fut}\;\Varid{a},\Varid{fut}\;\Varid{b})\;\Varid{conf},{}\<[E]%
\\
\>[B]{}\hsindent{7}{}\<[7]%
\>[7]{}\Conid{ArrowLoopParallel}\;\Varid{arr}\;[\mskip1.5mu \Varid{d}\mskip1.5mu]\;[\mskip1.5mu \Varid{d}\mskip1.5mu]\;\Varid{conf})\Rightarrow {}\<[E]%
\\
\>[B]{}\hsindent{7}{}\<[7]%
\>[7]{}\Varid{conf}\to \Varid{arr}\;(\Varid{c},\Varid{a},\Varid{b})\;(\Varid{d},\Varid{a},\Varid{b})\to \Varid{arr}\;[\mskip1.5mu [\mskip1.5mu \Varid{c}\mskip1.5mu]\mskip1.5mu]\;[\mskip1.5mu [\mskip1.5mu \Varid{d}\mskip1.5mu]\mskip1.5mu]{}\<[E]%
\\
\>[B]{}\Varid{torus}\;\Varid{conf}\;\Varid{f}\mathrel{=}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{loop}\;(\Varid{second}\;((\Varid{mapArr}\;\Varid{rightRotate}\mathbin{>\!\!>\!\!>}\Varid{lazy}){}\<[E]%
\\
\>[5]{}\hsindent{16}{}\<[21]%
\>[21]{}\mathbin{*\!*\!*}(\Varid{arr}\;\Varid{rightRotate}\mathbin{>\!\!>\!\!>}\Varid{lazy}))\mathbin{>\!\!>\!\!>}{}\<[E]%
\\
\>[5]{}\hsindent{4}{}\<[9]%
\>[9]{}\Varid{arr}\;(\Varid{uncurry3}\;(\Varid{zipWith3}\;\Varid{lazyzip3}))\mathbin{>\!\!>\!\!>}{}\<[E]%
\\
\>[5]{}\hsindent{4}{}\<[9]%
\>[9]{}\Varid{arr}\;\Varid{length}\mathbin{\&\!\&\!\&}(\Varid{shuffle}\mathbin{>\!\!>\!\!>}{}\<[E]%
\\
\>[9]{}\hsindent{16}{}\<[25]%
\>[25]{}\Varid{loopParEvalN}\;\Varid{conf}\;(\Varid{repeat}\;(\Varid{ptorus}\;\Varid{conf}\;\Varid{f})))\mathbin{>\!\!>\!\!>}{}\<[E]%
\\
\>[5]{}\hsindent{4}{}\<[9]%
\>[9]{}\Varid{arr}\;(\Varid{uncurry}\;\Varid{unshuffle})\mathbin{>\!\!>\!\!>}{}\<[E]%
\\
\>[5]{}\hsindent{4}{}\<[9]%
\>[9]{}\Varid{arr}\;(\Varid{map}\;\Varid{unzip3})\mathbin{>\!\!>\!\!>}\Varid{arr}\;\Varid{unzip3}\mathbin{>\!\!>\!\!>}\Varid{threetotwo})\mathbin{>\!\!>\!\!>}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{postLoopParEvalN}\;\Varid{conf}\;(\Varid{repeat}\;(\Varid{arr}\;\Varid{id})){}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{ptorus}\mathbin{::}(\Conid{Arrow}\;\Varid{arr},\Conid{Future}\;\Varid{fut}\;\Varid{a}\;\Varid{conf},\Conid{Future}\;\Varid{fut}\;\Varid{b}\;\Varid{conf})\Rightarrow {}\<[E]%
\\
\>[B]{}\hsindent{11}{}\<[11]%
\>[11]{}\Varid{conf}\to {}\<[E]%
\\
\>[B]{}\hsindent{11}{}\<[11]%
\>[11]{}\Varid{arr}\;(\Varid{c},\Varid{a},\Varid{b})\;(\Varid{d},\Varid{a},\Varid{b})\to {}\<[E]%
\\
\>[B]{}\hsindent{11}{}\<[11]%
\>[11]{}\Varid{arr}\;(\Varid{c},\Varid{fut}\;\Varid{a},\Varid{fut}\;\Varid{b})\;(\Varid{d},\Varid{fut}\;\Varid{a},\Varid{fut}\;\Varid{b}){}\<[E]%
\\
\>[B]{}\Varid{ptorus}\;\Varid{conf}\;\Varid{f}\mathrel{=}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{arr}\;(\lambda \mathord{\sim}(\Varid{c},\Varid{a},\Varid{b})\to (\Varid{c},\Varid{get}\;\Varid{conf}\;\Varid{a},\Varid{get}\;\Varid{conf}\;\Varid{b}))\mathbin{>\!\!>\!\!>}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{f}\mathbin{>\!\!>\!\!>}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{arr}\;(\lambda \mathord{\sim}(\Varid{d},\Varid{a},\Varid{b})\to (\Varid{d},\Varid{put}\;\Varid{conf}\;\Varid{a},\Varid{put}\;\Varid{conf}\;\Varid{b})){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\caption{\ensuremath{\Varid{torus}} skeleton definition. \ensuremath{\Varid{lazyzip3}}, \ensuremath{\Varid{uncurry3}} and \ensuremath{\Varid{threetotwo}} definitions are in Figure \ref{fig:lazyzip3etc}.}\label{fig:torus}\end{figure}

As an example of using this skeleton, Loogen et al.
(\protect\hyperlink{ref-Eden:SkeletonBookChapter02}{2003}) showed the
matrix multiplication using Gentleman's algorithm (Gentleman,
\protect\hyperlink{ref-Gentleman1978}{1978}). An adapted version can be
found in Figure \ref{fig:torusMatMult}.

\begin{figure}[t]
\centering
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{13}{@{}>{\hspre}l<{\hspost}@{}}%
\column{17}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{type}\;\Conid{Matrix}\mathrel{=}[\mskip1.5mu [\mskip1.5mu \Conid{Int}\mskip1.5mu]\mskip1.5mu]{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{prMM\char95 torus}\mathbin{::}\Conid{Int}\to \Conid{Int}\to \Conid{Matrix}\to \Conid{Matrix}\to \Conid{Matrix}{}\<[E]%
\\
\>[B]{}\Varid{prMM\char95 torus}\;\Varid{numCores}\;\Varid{problemSizeVal}\;\Varid{m1}\;\Varid{m2}\mathrel{=}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{combine}\mathbin{\$}\Varid{torus}\;()\;(\Varid{mult}\;\Varid{torusSize})\mathbin{\$}\Varid{zipWith}\;\Varid{zip}\;(\Varid{split1}\;\Varid{m1})\;(\Varid{split2}\;\Varid{m2}){}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\mathbf{where}\;{}\<[13]%
\>[13]{}\Varid{torusSize}\mathrel{=}(\Varid{floor}\mathbin{\circ}\Varid{sqrt})\mathbin{\$}\Varid{fromIntegral}\mathbin{\$}\Varid{numCoreCalc}\;\Varid{numCores}{}\<[E]%
\\
\>[13]{}\Varid{combine}\;\Varid{x}\mathrel{=}\Varid{concat}\;(\Varid{map}\;((\Varid{map}\;(\Varid{concat}))\mathbin{\circ}\Varid{transpose})\;\Varid{x}){}\<[E]%
\\
\>[13]{}\Varid{split1}\;\Varid{x}\mathrel{=}\Varid{staggerHorizontally}{}\<[E]%
\\
\>[13]{}\hsindent{4}{}\<[17]%
\>[17]{}(\Varid{splitMatrix}\;(\Varid{problemSizeVal}\mathbin{\Varid{`div`}}\Varid{torusSize})\;\Varid{x}){}\<[E]%
\\
\>[13]{}\Varid{split2}\;\Varid{x}\mathrel{=}\Varid{staggerVertically}{}\<[E]%
\\
\>[13]{}\hsindent{4}{}\<[17]%
\>[17]{}(\Varid{splitMatrix}\;(\Varid{problemSizeVal}\mathbin{\Varid{`div`}}\Varid{torusSize})\;\Varid{x}){}\<[E]%
\\[\blanklineskip]%
\>[B]{}\mbox{\onelinecomment  Function performed by each worker}{}\<[E]%
\\
\>[B]{}\Varid{mult}\mathbin{::}\Conid{Int}\to {}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}((\Conid{Matrix},\Conid{Matrix}),[\mskip1.5mu \Conid{Matrix}\mskip1.5mu],[\mskip1.5mu \Conid{Matrix}\mskip1.5mu])\to {}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}(\Conid{Matrix},[\mskip1.5mu \Conid{Matrix}\mskip1.5mu],[\mskip1.5mu \Conid{Matrix}\mskip1.5mu]){}\<[E]%
\\
\>[B]{}\Varid{mult}\;\Varid{size}\;((\Varid{sm1},\Varid{sm2}),\Varid{sm1s},\Varid{sm2s})\mathrel{=}(\Varid{result},\Varid{toRight},\Varid{toBottom}){}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\mathbf{where}\;{}\<[13]%
\>[13]{}\Varid{toRight}\mathrel{=}\Varid{take}\;(\Varid{size}\mathbin{-}\mathrm{1})\;(\Varid{sm1}\mathbin{:}\Varid{sm1s}){}\<[E]%
\\
\>[13]{}\Varid{toBottom}\mathrel{=}\Varid{take}\;(\Varid{size}\mathbin{-}\mathrm{1})\;(\Varid{sm2}\mathbin{:}\Varid{sm2s}){}\<[E]%
\\
\>[13]{}\Varid{sms}\mathrel{=}\Varid{zipWith}\;\Varid{prMM}\;(\Varid{sm1}\mathbin{:}\Varid{sm1s})\;(\Varid{sm2}\mathbin{:}\Varid{sm2s}){}\<[E]%
\\
\>[13]{}\Varid{result}\mathrel{=}\Varid{foldl1'}\;\Varid{matAdd}\;\Varid{sms}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\caption[Adapted matrix multiplication in Eden using the \ensuremath{\Varid{torus}} skeleton.]{Adapted matrix multiplication in Eden using the \ensuremath{\Varid{torus}} skeleton. \ensuremath{\Varid{prMM\char95 torus}} is the parallel matrix multiplication. \ensuremath{\Varid{mult}} is the function performed by each worker. \ensuremath{\Varid{prMM}} is the sequential matrix multiplication in the chunks. \ensuremath{\Varid{splitMatrix}} splits the Matrix into chunks. \ensuremath{\Varid{staggerHorizontally}} and \ensuremath{\Varid{staggerVertically}} pre-rotate the matrices. \ensuremath{\Varid{matAdd}} calculates $A + B$. Omitted definitions can be found in \ref{fig:torus_example_rest}.}\label{fig:torusMatMult}\end{figure}

If we compare the trace from a call using our Arrow definition of the
\ensuremath{\Varid{torus}} (Figure \ref{fig:torus_parrows_trace}) with the Eden version
(Figure \ref{fig:torus_eden_trace}) we can see that the behaviour of the
Arrow version and execution times are comparable -- our port was
successful. We discuss further benchmarks on larger clusters in more
detail in Chapter \ref{sec:benchmarks}.

\begin{figure}
\centering
\includegraphics{src/img/torus_matrix_parrows_trace.pdf}
\caption{Communication trace of a matrix multiplication with \ensuremath{\Varid{torus}}
(PArrows).\label{fig:torus_parrows_trace}}
\end{figure}

\begin{figure}
\centering
\includegraphics{src/img/torus_matrix_parrows_trace.pdf}
\caption{Communication trace of a matrix multiplication with \ensuremath{\Varid{torus}}
(Eden).\label{fig:torus_eden_trace}}
\end{figure}

\hypertarget{experiment-cloud-haskell-backend}{%
\chapter{Experiment: Cloud Haskell
Backend}\label{experiment-cloud-haskell-backend}}

\label{sec:cloudHaskellExperiment}

Cloud Computing has become more and more prevalent in the recent years.
Servers are replaced with virtual ones positioned all around the globe.
These can easily be brought up when required and shut down when they are
not in use. This trend in computing has also been embraced by the
Haskell community, and therefore libraries such as Cloud Haskell were
born. Cloud Haskell is described on the project's website\footnote{See
  \url{http://haskell-distributed.github.io/}.} as:

\begin{quote}
Cloud Haskell: Erlang-style concurrent and distributed programming in
Haskell. The Cloud Haskell Platform consists of a generic network
transport API, libraries for sending static closures to remote nodes, a
rich API for distributed programming and a set of platform libraries
modelled after Erlang's Open Telecom Platform.

Generic network transport backends have been developed for TCP and
in-memory messaging, and several other implementations are available
including a transport for Windows Azure.{[}\ldots{}{]}
\end{quote}

It is basically a set of APIs and libraries for communication between
networks of nodes in a cloud environment. With it, programmers can write
fully-featured, Haskell based cloud solutions targeting a wide range of
architectures.

While users can already write concurrent applications with the help of
Cloud Haskell using some of its libraries or even with the bare
communication API, it seems like a good idea to enable writing parallel
programs requiring less involvement from the user. This way, they can
focus on parallel algorithms instead of manual communication. In the
following chapter, we will therefore explore the possibility of a Cloud
Haskell based backend for the PArrows DSL, while explaining all the
necessary parts of Cloud Haskell's API. For easier testing and as this
is only meant as a proof of concept, we only work with a local-net Cloud
Haskell backend in this thesis. The results of this experiment, however,
are transferable to other architectures as well when building upon the
results presented here.\footnote{With the help of virtual private
  networks one could even use this local-net variant.}

The following is structured as follows. We start by explaining how to
discover nodes with a master-slave structure while also defining a
program startup harness that can be used with this scheme in Chapter
\ref{sec:nodeDiscAndHarness}. Then, we explain how parallel evaluation
of arbitrary data is possible with Cloud Haskell in Chapter
\ref{sec:parEvalCloudHaskell} and also discuss how we can implement the
PArrows DSL with this knowledge in Chapter
\ref{sec:CloudHaskellArrowParallel}.

\hypertarget{node-discovery-and-program-harness}{%
\section{Node discovery and program
harness}\label{node-discovery-and-program-harness}}

\label{sec:nodeDiscAndHarness}

In cloud services, changes of the architecture in the running network
are more common than in ordinary computing clusters where the
participating nodes are usually known at startup. In the
SimpleLocalNet\footnote{See
  \url{http://hackage.haskell.org/package/distributed-process-simplelocalnet}.}
Cloud Haskell variant we are using for this experiment, this is
reflected in the fact that there already exists a pre-implemented
master-slave structure. The master node -- the node that starts the
computation is considered the master node here -- has to keep track of
all the available slave nodes. The slave nodes wait for tasks and handle
them as required.

We will now first go into detail on the data-structure (Chapter
\ref{sec:cloudhaskellstate}) we use in order to handle this information
to then explain how to start slave (Chapter
\ref{sec:cloudhaskellslaves}) and master nodes (Chapter
\ref{sec:cloudhaskellmasters}). We also explain how to create a startup
harness (Chapter \ref{sec:cloudhaskellstartupharness}) to wrap all of
this.

\hypertarget{the-state-data-structure}{%
\subsection{\texorpdfstring{The \ensuremath{\Conid{State}}
data-structure}{The  data-structure}}\label{the-state-data-structure}}

\label{sec:cloudhaskellstate}

The data-structure containing all relevant information about the state
of the network as well as the computation in general, \ensuremath{\Conid{State}}, is
defined as


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{data}\;\Conid{State}\mathrel{=}\Conid{State}\;\{\mskip1.5mu {}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Varid{workers}\mathbin{::}\Conid{MVar}\;[\mskip1.5mu \Conid{NodeId}\mskip1.5mu],{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Varid{shutdown}\mathbin{::}\Conid{MVar}\;\Conid{Bool},{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Varid{started}\mathbin{::}\Conid{MVar}\;\Conid{Bool},{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Varid{localNode}\mathbin{::}\Conid{LocalNode},{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Varid{serializeBufferSize}\mathbin{::}\Conid{Int}{}\<[E]%
\\
\>[B]{}\mskip1.5mu\}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

Notice that \ensuremath{\Varid{workers}\mathbin{::}\Conid{MVar}\;[\mskip1.5mu \Conid{NodeId}\mskip1.5mu]}, \ensuremath{\Varid{shutdown}\mathbin{::}\Conid{MVar}\;\Conid{Bool}} and
\ensuremath{\Varid{started}\mathbin{::}\Conid{MVar}\;()} are all low level mutable variables instead of
regular fields. This is because we pass this \ensuremath{\Conid{State}} around between
functions, but want to update it with new information on-the-fly. These
modifiable variables can be created empty with
\ensuremath{\Varid{newEmptyMVar}\mathbin{::}\Conid{IO}\;(\Conid{MVar}\;\Varid{a})} or already with contents with
\ensuremath{\Varid{newMVar}\mathbin{::}\Varid{a}\to \Conid{IO}\;(\Conid{MVar}\;\Varid{a})}. They can be read with
\ensuremath{\Varid{readMVar}\mathbin{::}\Conid{MVar}\;\Varid{a}\to \Conid{IO}\;\Varid{a}} or emptied with
\ensuremath{\Varid{takeMVar}\mathbin{::}\Conid{MVar}\;\Varid{a}\to \Conid{IO}\;\Varid{a}}. Values can be placed inside with
\ensuremath{\Varid{putMVar}\mathbin{::}\Conid{MVar}\;\Varid{a}\to \Varid{a}\to \Conid{IO}\;()}. \ensuremath{\Conid{MVar}}s are thread-safe and all
reading operations block until some content is placed in them. We will
see them used in other places of this backend as well.

In the \ensuremath{\Conid{State}} type, \ensuremath{\Varid{workers}\mathbin{::}\Conid{MVar}\;[\mskip1.5mu \Conid{NodeId}\mskip1.5mu]} holds information about
all available slave nodes, \ensuremath{\Varid{shutdown}\mathbin{::}\Conid{MVar}\;\Conid{Bool}} determines whether
the backend is to be shut down, \ensuremath{\Varid{started}\mathbin{::}\Conid{MVar}\;()} returns a
signalling \ensuremath{()} if the backend has properly started when accessed with
\ensuremath{\Varid{readMVar}}. \ensuremath{\Varid{localNode}\mathbin{::}\Conid{LocalNode}} and \ensuremath{\Varid{serializeBufferSize}\mathbin{::}\Conid{Int}}
store information about all Cloud Haskell internals for the master node
and the buffer size for serialization (we will discuss serialization
system itself separately), respectively.

Note that as we will later use the \ensuremath{\Conid{State}} type as the \ensuremath{\Varid{conf}} parameter
in the \linebreak \ensuremath{\Conid{ArrowParallel}} instance, we use the type synonym
\ensuremath{\mathbf{type}\;\Conid{Conf}\mathrel{=}\Conid{State}} in the following code chapters. Furthermore, an
initial config can be created with the function
\ensuremath{\Varid{initialConf}\mathbin{::}\Conid{Int}\to \Conid{LocalNode}\to \Conid{IO}\;\Conid{Conf}} where the resulting config
contains a \ensuremath{\Varid{serializeBufferSize}} as specified by the first parameter and
the \ensuremath{\Conid{LocalNode}} specified by the second parameter. Additionally, the
list of workers, \ensuremath{\Varid{workers}\mathbin{::}\Conid{MVar}\;[\mskip1.5mu \Conid{NodeId}\mskip1.5mu]}, is initialized with an
empty list, \ensuremath{\Varid{shutdown}\mathbin{::}\Conid{MVar}\;\Conid{Bool}} is set to \ensuremath{\Conid{False}} and
\ensuremath{\Varid{started}\mathbin{::}\Conid{MVar}\;()} is created as an empty \ensuremath{\Conid{MVar}} so that it can be
populated with the signalling \ensuremath{()} when the startup is finished. The
complete code for \ensuremath{\Varid{initialConf}} is the following:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{initialConf}\mathbin{::}\Conid{Int}\to \Conid{LocalNode}\to \Conid{IO}\;\Conid{Conf}{}\<[E]%
\\
\>[B]{}\Varid{initialConf}\;\Varid{serializeBufferSize}\;\Varid{localNode}\mathrel{=}\mathbf{do}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Varid{workersMVar}\leftarrow \Varid{newMVar}\;[\mskip1.5mu \mskip1.5mu]{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Varid{shutdownMVar}\leftarrow \Varid{newMVar}\;\Conid{False}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Varid{startedMVar}\leftarrow \Varid{newEmptyMVar}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Varid{return}\;\Conid{State}\;\{\mskip1.5mu {}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\Varid{workers}\mathrel{=}\Varid{workersMVar},{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\Varid{shutdown}\mathrel{=}\Varid{shutdownMVar},{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\Varid{started}\mathrel{=}\Varid{startedMVar},{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\Varid{localNode}\mathrel{=}\Varid{localNode},{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\Varid{serializeBufferSize}\mathrel{=}\Varid{serializeBufferSize}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mskip1.5mu\}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

A utility function \ensuremath{\Varid{defaultInitConf}} using a default serialization
buffer size of 10MB is also defined as:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{defaultBufSize}\mathbin{::}\Conid{Int}{}\<[E]%
\\
\>[B]{}\Varid{defaultBufSize}\mathrel{=}\mathrm{10}\mathbin{*}\mathrm{2}\mathbin{\uparrow}\mathrm{20}\mbox{\onelinecomment  10 MB}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{defaultInitConf}\mathbin{::}\Conid{LocalNode}\to \Conid{IO}\;\Conid{Conf}{}\<[E]%
\\
\>[B]{}\Varid{defaultInitConf}\mathrel{=}\Varid{initialConf}\;\Varid{defaultBufSize}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

\hypertarget{starting-slave-nodes}{%
\subsection{Starting slave nodes}\label{starting-slave-nodes}}

\label{sec:cloudhaskellslaves}

With the \ensuremath{\Conid{State}}/\ensuremath{\Conid{Conf}} data structure, we can now implement our
node-discovery scheme.

For the slave nodes, we can just use the basic utilities for a slave
backend in the SimpleLocalNet library. The code to start a node for the
\ensuremath{\Conid{Slave}} backend is then:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{type}\;\Conid{Host}\mathrel{=}\Conid{String}{}\<[E]%
\\
\>[B]{}\mathbf{type}\;\Conid{Port}\mathrel{=}\Conid{String}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{initializeSlave}\mathbin{::}\Conid{RemoteTable}\to \Conid{Host}\to \Conid{Port}\to \Conid{IO}\;(){}\<[E]%
\\
\>[B]{}\Varid{initializeSlave}\;\Varid{remoteTable}\;\Varid{host}\;\Varid{port}\mathrel{=}\mathbf{do}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Varid{backend}\leftarrow \Varid{initializeBackend}\;\Varid{host}\;\Varid{port}\;\Varid{remoteTable}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Varid{startSlave}\;\Varid{backend}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

We here start by initializing the Cloud Haskell backend with a given
\ensuremath{\Varid{host}}, \ensuremath{\Varid{port}} and \ensuremath{\Varid{remoteTable}} via
\ensuremath{\Varid{initializeBackend}\mathbin{::}\Conid{String}\to \Conid{String}\to \Conid{RemoteTable}} to then delegate
the logic completely to the library function
\ensuremath{\Varid{startSlave}\mathbin{::}\Conid{Backend}\to \Conid{IO}\;()} which does not return unless the slave
is shutdown manually from the master node. The \ensuremath{\Conid{RemoteTable}} contains
all serialization information about static values required by Cloud
Haskell. We will later see how we can automatically generate such a
table.

\hypertarget{starting-master-nodes}{%
\subsection{Starting master nodes}\label{starting-master-nodes}}

\label{sec:cloudhaskellmasters}

For master nodes, the implementation of the node-discovery scheme is a
bit more involved. The actual
\ensuremath{\Varid{startMaster}\mathbin{::}\Conid{Backend}\to \Conid{Process}\to \Conid{IO}\;()} supplied by SimpleLocalNet
is meant to start a computation represented by a \ensuremath{\Conid{Process}} Monad and
then return. In our use-case, we want to be able to spawn functions
outside of the \ensuremath{\Conid{Process}} Monad, though. We therefore use the following
function to build a process \ensuremath{\Conid{Process}\;()} which will be passed into
\ensuremath{\Varid{startMaster}} only for the sake of slave-node discovery and management:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{7}{@{}>{\hspre}l<{\hspost}@{}}%
\column{9}{@{}>{\hspre}l<{\hspost}@{}}%
\column{11}{@{}>{\hspre}l<{\hspost}@{}}%
\column{15}{@{}>{\hspre}l<{\hspost}@{}}%
\column{17}{@{}>{\hspre}l<{\hspost}@{}}%
\column{19}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{master}\mathbin{::}\Conid{Conf}\to \Conid{Backend}\to [\mskip1.5mu \Conid{NodeId}\mskip1.5mu]\to \Conid{Process}\;(){}\<[E]%
\\
\>[B]{}\Varid{master}\;\Varid{conf}\;\Varid{backend}\;\Varid{slaves}\mathrel{=}\mathbf{do}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{forever}\mathbin{\$}\mathbf{do}{}\<[E]%
\\
\>[5]{}\hsindent{2}{}\<[7]%
\>[7]{}\Varid{shutdown}\leftarrow \Varid{liftIO}\mathbin{\$}\Varid{readMVar}\mathbin{\$}\Varid{shutdown}\;\Varid{conf}{}\<[E]%
\\
\>[5]{}\hsindent{2}{}\<[7]%
\>[7]{}\mathbf{if}\;\Varid{shutdown}{}\<[E]%
\\
\>[7]{}\hsindent{2}{}\<[9]%
\>[9]{}\mathbf{then}\;\mathbf{do}{}\<[E]%
\\
\>[9]{}\hsindent{2}{}\<[11]%
\>[11]{}\Varid{terminateAllSlaves}\;\Varid{backend}{}\<[E]%
\\
\>[9]{}\hsindent{2}{}\<[11]%
\>[11]{}\Varid{die}\;\text{\tt \char34 terminated\char34}{}\<[E]%
\\
\>[7]{}\hsindent{2}{}\<[9]%
\>[9]{}\mathbf{else}\;\mathbf{do}{}\<[E]%
\\
\>[9]{}\hsindent{2}{}\<[11]%
\>[11]{}\Varid{slaveProcesses}\leftarrow \Varid{findSlaves}\;\Varid{backend}{}\<[E]%
\\
\>[9]{}\hsindent{2}{}\<[11]%
\>[11]{}\Varid{redirectLogsHere}\;\Varid{backend}\;\Varid{slaveProcesses}{}\<[E]%
\\
\>[9]{}\hsindent{2}{}\<[11]%
\>[11]{}\mathbf{let}\;\Varid{slaveNodes}\mathrel{=}\Varid{map}\;\Varid{processNodeId}\;\Varid{slaveProcesses}{}\<[E]%
\\
\>[9]{}\hsindent{2}{}\<[11]%
\>[11]{}\Varid{liftIO}\mathbin{\$}\mathbf{do}{}\<[E]%
\\
\>[11]{}\hsindent{4}{}\<[15]%
\>[15]{}\Varid{modifyMVar\char95 }\;(\Varid{workers}\;\Varid{conf})\;(\mathbin{\char92 \char95 }\to \Varid{return}\;\Varid{slaveNodes}){}\<[E]%
\\
\>[11]{}\hsindent{4}{}\<[15]%
\>[15]{}\Varid{isEmpty}\leftarrow \Varid{isEmptyMVar}\mathbin{\$}\Varid{started}\;\Varid{conf}{}\<[E]%
\\
\>[11]{}\hsindent{4}{}\<[15]%
\>[15]{}\mathbf{if}\;(\Varid{isEmpty}\mathrel{\wedge}\Varid{length}\;\Varid{slaveNodes}\mathbin{>}\mathrm{0})\;\mathbf{then}{}\<[E]%
\\
\>[15]{}\hsindent{4}{}\<[19]%
\>[19]{}\Varid{putMVar}\;(\Varid{started}\;\Varid{conf})\;(){}\<[E]%
\\
\>[11]{}\hsindent{4}{}\<[15]%
\>[15]{}\mathbf{else}{}\<[E]%
\\
\>[15]{}\hsindent{2}{}\<[17]%
\>[17]{}\Varid{return}\;(){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

Basically, this continuously updates the list of slaves inside the
configuration by first querying for all slave processes with
\ensuremath{\Varid{findSlaves}\;\Varid{backend}} and redirecting the log output to the master node
with \ensuremath{\Varid{redirectLogsHere}\;\Varid{backend}\;\Varid{slaveProcesses}}, to then finally update
\ensuremath{\Varid{workers}\mathbin{::}\Conid{MVar}\;[\mskip1.5mu \Conid{NodeId}\mskip1.5mu]} inside the configuration. Additionally, as
soon as one slave is found, \ensuremath{\Varid{started}\mathbin{::}\Conid{MVar}\;()} is supplied with the
signalling \ensuremath{()}, so that any thread waiting for node discovery can start
its actual computation.\footnote{Notice that while we could add an
  additional sleep here to not generate too much network noise in this
  function, we leave it out for the sake of simplicity.} All of this is
embedded in a check whether a shutdown is requested with
\ensuremath{\Varid{liftIO}\mathbin{\$}\Varid{readMVar}\mathbin{\$}\Varid{shutdown}\;\Varid{conf}}. If instructed to do so, the program
does the necessary cleanup -- terminating all slaves with
\ensuremath{\Varid{terminateAllSlaves}\;\Varid{backend}} and shutting itself down with
\ensuremath{\Varid{die}\;\text{\tt \char34 terminated\char34}} - otherwise continuing with the updating process.

With this \ensuremath{\Varid{master}} function, we can now define our initialization
function \linebreak \ensuremath{\Varid{initializeMaster}\mathbin{::}\Conid{RemoteTable}\to \Conid{Host}\to \Conid{Port}\to \Conid{IO}\;\Conid{Conf}}:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{initializeMaster}\mathbin{::}\Conid{RemoteTable}\to \Conid{Host}\to \Conid{Port}\to \Conid{IO}\;\Conid{Conf}{}\<[E]%
\\
\>[B]{}\Varid{initializeMaster}\;\Varid{remoteTable}\;\Varid{host}\;\Varid{port}\mathrel{=}\mathbf{do}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{backend}\leftarrow \Varid{initializeBackend}\;\Varid{host}\;\Varid{port}\;\Varid{remoteTable}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{localNode}\leftarrow \Varid{newLocalNode}\;\Varid{backend}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{conf}\leftarrow \Varid{defaultInitConf}\;\Varid{localNode}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{forkIO}\mathbin{\$}\Varid{startMaster}\;\Varid{backend}\;(\Varid{master}\;\Varid{conf}\;\Varid{backend}){}\<[E]%
\\[\blanklineskip]%
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{waitForStartup}\;\Varid{conf}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{return}\;\Varid{conf}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

Similar to the slave code, we initialize the Cloud Haskell backend via
\ensuremath{\Varid{initializeBackend}\mathbin{::}\Conid{String}\to \Conid{String}\to \Conid{RemoteTable}}, but also create
a new local node that is used to start computations outside of the
initialization logic. With this node we can create a default initial
config via \ensuremath{\Varid{defaultInitConf}\mathbin{::}\Conid{LocalNode}\to \Conid{Conf}} which is then passed
into the discovery function with
\ensuremath{\Varid{startMaster}\;\Varid{backend}\;(\Varid{master}\;\Varid{conf}\;\Varid{backend})}. We have to fork this \ensuremath{\Conid{IO}}
action away with \ensuremath{\Varid{forkIO}}, because the \ensuremath{\Conid{IO}} action will run forever as
long as the program has not been manually shutdown via the corresponding
variable in the \ensuremath{\Conid{State}}. Finally, we wait for the startup to finish via
\ensuremath{\Varid{waitForStartup}\mathbin{::}\Conid{Conf}\to \Conid{IO}\;()} to end with returning a \ensuremath{\Conid{IO}\;\Conid{Conf}}
action containing the initial config/state. \ensuremath{\Varid{waitForStartup}} can simply
be defined as


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{waitForStartup}\mathbin{::}\Conid{Conf}\to \Conid{IO}\;(){}\<[E]%
\\
\>[B]{}\Varid{waitForStartup}\;\Varid{conf}\mathrel{=}\Varid{readMVar}\;(\Varid{started}\;\Varid{conf}){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

because of the blocking behaviour of empty \ensuremath{\Conid{MVar}}s and the fact that we
are signalling the startup with a simple dummy value \ensuremath{()}, as described
earlier.

\hypertarget{startup-harness}{%
\subsection{Startup harness}\label{startup-harness}}

\label{sec:cloudhaskellstartupharness}

If we put together all of the startup logic we have discussed until now,
we can easily write a startup harness in which we simply delegate to the
proper initialization code depending on the command line arguments:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{7}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{myRemoteTable}\mathbin{::}\Conid{RemoteTable}{}\<[E]%
\\
\>[B]{}\Varid{myRemoteTable}\mathrel{=}\mathbin{\Conid{Main}.\char95 \char95 }\Varid{remoteTable}\;\Varid{initRemoteTable}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{main}\mathbin{::}\Conid{IO}\;(){}\<[E]%
\\
\>[B]{}\Varid{main}\mathrel{=}\mathbf{do}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Varid{args}\leftarrow \Varid{getArgs}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{case}\;\Varid{args}\;\mathbf{of}{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}[\mskip1.5mu \text{\tt \char34 master\char34},\Varid{host},\Varid{port}\mskip1.5mu]\to \mathbf{do}{}\<[E]%
\\
\>[5]{}\hsindent{2}{}\<[7]%
\>[7]{}\Varid{conf}\leftarrow \Varid{initializeMaster}\;\Varid{myRemoteTable}\;\Varid{host}\;\Varid{port}{}\<[E]%
\\[\blanklineskip]%
\>[5]{}\hsindent{2}{}\<[7]%
\>[7]{}\mbox{\onelinecomment  read and print the list of available workers}{}\<[E]%
\\
\>[5]{}\hsindent{2}{}\<[7]%
\>[7]{}\Varid{readMVar}\;(\Varid{workers}\;\Varid{conf})\bind \Varid{print}{}\<[E]%
\\[\blanklineskip]%
\>[5]{}\hsindent{2}{}\<[7]%
\>[7]{}\mbox{\onelinecomment  TODO: parallel computation here}{}\<[E]%
\\[\blanklineskip]%
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}[\mskip1.5mu \text{\tt \char34 slave\char34},\Varid{host},\Varid{port}\mskip1.5mu]\to \mathbf{do}{}\<[E]%
\\
\>[5]{}\hsindent{2}{}\<[7]%
\>[7]{}\Varid{initializeSlave}\;\Varid{myRemoteTable}\;\Varid{host}\;\Varid{port}{}\<[E]%
\\
\>[5]{}\hsindent{2}{}\<[7]%
\>[7]{}\Varid{print}\;\text{\tt \char34 slave~shutdown.\char34}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

In order to launch a program using this harness, we have to start slave
nodes for each cpu core with commands like
\enquote{<executable> slave 127.0.0.1 8000} where the last parameter
determines the port the slave will listen to and wait for requests on.
Similarly, a single master node can be started with
\enquote{<executable> master 127.0.0.1 7999} where, once again, the last
parameter determines the communication port.

This example also shows how a \ensuremath{\Conid{RemoteTable}} is obtained so that it can
be used inside \ensuremath{\Varid{main}\mathbin{::}\Conid{IO}\;()}. Note, that the definition of
\ensuremath{\mathbin{\Conid{Main}.\char95 \char95 }\Varid{remoteTable}\mathbin{::}\Conid{RemoteTable}\to \Conid{RemoteTable}} used in
\ensuremath{\Varid{myRemoteTable}\mathbin{::}\Conid{RemoteTable}} is a function automatically generated by
Template-Haskell\footnote{Template-Haskell is a code generator for
  Haskell written in Haskell, and can be enabled with a language pragma
  \ensuremath{\mbox{\enskip\{-\# LANGUAGE TemplateHaskell  \#-\}\enskip}} at the top of the source file.}
building a \ensuremath{\Conid{RemoteTable}} on top of the \ensuremath{\Varid{initRemoteTable}} supplied by
Cloud Haskell by adding all relevant static declarations of the program.
In Cloud Haskell, we can for example generate such a declaration for
some function \ensuremath{\Varid{f}\mathbin{::}\Conid{Int}\to \Conid{Int}}, with a call to \ensuremath{\Varid{remotable}} inside a
Template-Haskell splice as \ensuremath{\mathbin{\$}(\Varid{remotable}\;[\mskip1.5mu }'\ensuremath{\Varid{f}\mskip1.5mu])}.

As can be seen from this, any function passed to \ensuremath{\Varid{remotable}} must have a
top-level declaration. Furthermore, we must also add any function
manually. This is usually okay for basic applications where the user
knows which functions/values need to be serialized statically at compile
time, but not in our use case, as we want to be able to evaluate
arbitrary functions/Arrows on remote nodes. In Chapter
\ref{sec:parEvalCloudHaskell}, we will see how to resolve this problem.

\hypertarget{parallel-evaluation-with-cloud-haskell}{%
\section{Parallel evaluation with Cloud
Haskell}\label{parallel-evaluation-with-cloud-haskell}}

\label{sec:parEvalCloudHaskell}

As we have seen in the previous chapter, we can not send arbitrary
functions or Arrows to the slave nodes. Thankfully, there is an
alternative: Eden's serialization mechanism has been made available
separately in a package called \enquote{packman}.\footnote{See
  \url{https://hackage.haskell.org/package/packman}.} This allows for
values to be serialized in the exact evaluation state they are currently
in.

We can use this to our advantage. Instead of sending inputs and
functions/Arrows to the slave nodes and sending the result back (which
does not work with the current Cloud Haskell API), we can instead apply
the function, serialize this unevaluated thunk, send it to the
evaluating slave, and communicate the fully evaluated value back to the
master.

With this idea in mind, we will now explain how to achieve parallel
evaluation of Arrows with Cloud Haskell. We start by explaining the
communication basics in Chapter \ref{sec:cloudhaskellCommBetweenNodes}.
Next, we describe how to achieve evaluation of single values on slave
nodes in Chapter \ref{sec:cloudhaskellEvaluationOnSlaves}. Finally, we
use these results to implement a parallel evaluation scheme in Chapter
\ref{sec:cloudhaskellParallelEvaluation}.

\hypertarget{communication-basics}{%
\subsection{Communication basics}\label{communication-basics}}

We will now go over the communication basics we require in the later
parts of this chapter. This includes a quick introduction on how we
actually send the data in Cloud Haskell and also a quick definition of
our serialized data wrapper we use to send unevaluated data between
nodes.

\label{sec:cloudhaskellCommBetweenNodes}

\hypertarget{sending-and-receiving-data}{%
\subsubsection{Sending and receiving
data}\label{sending-and-receiving-data}}

\label{sec:sendRecCloud}

Cloud Haskell uses typed channels to send and receive data between
nodes. A typed channel consists of a \ensuremath{\Conid{SendPort}\;\Varid{a}} and a \ensuremath{\Conid{ReceivePort}\;\Varid{a}}.
We can create a new typed cannel with the help of
\ensuremath{\Varid{newChan}\mathbin{::}\Conid{Serializable}\;\Varid{a}\Rightarrow \Conid{Process}\;(\Conid{SendPort}\;\Varid{a}, \linebreak \Conid{ReceivePort}\;\Varid{a})}:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{myProc}\mathbin{::}\Conid{Process}\;(){}\<[E]%
\\
\>[B]{}\Varid{myProc}\mathrel{=}\mathbf{do}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}(\Varid{sendPort},\Varid{receivePort})\leftarrow \Varid{newChan}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\mbox{\onelinecomment  do stuff}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

Data can be sent with
\ensuremath{\Varid{sendChan}\mathbin{::}\Conid{Serializable}\;\Varid{a}\Rightarrow \Conid{SendPort}\;\Varid{a}\to \Varid{a}\to \Conid{Process}\;()}:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{sendTen}\mathbin{::}\Conid{SendPort}\;\Conid{Int}\to \Conid{Process}\;(){}\<[E]%
\\
\>[B]{}\Varid{sendTen}\;\Varid{sendPort}\mathrel{=}\Varid{sendChan}\;\Varid{sendPort}\;\mathrm{10}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

Values are received in a blocking manner with
\ensuremath{\Varid{receiveChan}\mathbin{::}\Conid{Serializable}\;\Varid{a}\Rightarrow \Conid{ReceivePort}\;\Varid{a}\to \Conid{Process}\;\Varid{a}}:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{receiveVal}\mathbin{::}\Conid{ReceivePort}\;\Conid{Int}\to \Conid{Process}\;\Conid{Int}{}\<[E]%
\\
\>[B]{}\Varid{receiveVal}\;\Varid{receivePort}\mathrel{=}\Varid{receiveChan}\;\Varid{receivePort}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

One thing to keep in mind is that only \ensuremath{\Conid{SendPort}} are serializable. So
in order to have a two way communication where the master sends some
input to the slave and awaits its result, like we require in our use
case, we have to first receive a \ensuremath{\Conid{SendPort}\;\Varid{a}} in the master via the
\ensuremath{\Conid{ReceivePort}\;(\Conid{SendPort}\;\Varid{a}))} of some channel
\ensuremath{(\Conid{SendPort}\;(\Conid{SendPort}\;\Varid{a}),\Conid{ReceivePort}\;(\Conid{SendPort}\;\Varid{a}))}. This \ensuremath{\Conid{SendPort}\;\Varid{a}}
is sent the slave and belongs to the channel
\ensuremath{(\Conid{SendPort}\;\Varid{a},\Conid{ReceivePort}\;\Varid{a})} where it expects its input to come through
the corresponding \ensuremath{\Conid{ReceivePort}\;\Varid{a}}. Additionally, we also require a
channel \ensuremath{(\Conid{SendPort}\;\Varid{b},\Conid{ReceivePort}\;\Varid{b})} on which the slave sends its
result through the \ensuremath{\Conid{SendPort}\;\Varid{b}} and the master awaits its result on the
\ensuremath{\Conid{ReceivePort}\;\Varid{b}}. We depict this process with schematically in Figure
\ref{fig:cloudHaskellGeneralComm}.

\begin{figure}
\centering
\includegraphics{src/img/CloudHaskellCommunication.pdf}
\caption[Required communication scheme for our Cloud Haskell backend.]{Required communication scheme for our Cloud Haskell backend.
Actions corresponding to specific channels are marked with their
respective colour.\label{fig:cloudHaskellGeneralComm}}
\end{figure}

This idea is executed in the following code example. The master looks
like


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{master}\mathbin{::}\Conid{ReceivePort}\;(\Conid{SendPort}\;\Varid{a})\to \Conid{ReceivePort}\;\Varid{b}\to \Conid{Process}\;(){}\<[E]%
\\
\>[B]{}\Varid{master}\;\Varid{aSenderReceiver}\;\Varid{bReceiver}\mathrel{=}\mathbf{do}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{aSender}\leftarrow \Varid{receiveChan}\;\Varid{aSenderReceiver}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\mathbf{let}\;\Varid{someA}\mathrel{=}\mathbin{...}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{sendChan}\;\Varid{aSender}\;\Varid{someA}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{someB}\leftarrow \Varid{receiveChan}\;\Varid{bReceiver}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\mathbin{...}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{return}\;(){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

while the slave is schematically defined as


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{slave}\mathbin{::}\Conid{SendPort}\;(\Conid{SendPort}\;\Varid{a})\to \Conid{SendPort}\;\Varid{b}\to \Conid{Process}\;(){}\<[E]%
\\
\>[B]{}\Varid{slave}\;\Varid{aSenderSender}\;\Varid{bSender}\mathrel{=}\mathbf{do}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}(\Varid{aSender},\Varid{aReceiver})\leftarrow \Varid{newChan}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{sendChan}\;\Varid{aSenderSender}\;\Varid{aSender}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{someA}\leftarrow \Varid{receiveChan}\;\Varid{aReceiver}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\mathbf{let}\;\Varid{someB}\mathrel{=}\Varid{useAToMakeB}\;\Varid{someA}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{sendChan}\;\Varid{bSender}\;\Varid{someB}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

\hypertarget{serialized-data-type}{%
\subsubsection{Serialized data type}\label{serialized-data-type}}

The packman package comes with a serialization function
\ensuremath{\Varid{trySerializeWith}\mathbin{::}\Varid{a}\to \Conid{Int}\to \Conid{IO}\;(\Conid{Serialized}\;\Varid{a})} (the second
parameter is the buffer size) and a deserialization function
\ensuremath{\Varid{deserialize}\mathbin{::}\Conid{Serialized}\;\Varid{a}\to \Conid{IO}\;\Varid{a}}. Here, \ensuremath{\Conid{Serialized}\;\Varid{a}} is the type
containing the serialized value of \ensuremath{\Varid{a}}.

In order to have a clean slate in terms of type class instances, we
define a wrapper type \ensuremath{\Conid{Thunk}\;\Varid{a}} around \ensuremath{\Conid{Serialized}\;\Varid{a}} as


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mbox{\onelinecomment  Wrapper for the packman type Serialized}{}\<[E]%
\\
\>[B]{}\mathbf{newtype}\;\Conid{Thunk}\;\Varid{a}\mathrel{=}\Conid{Thunk}\;\{\mskip1.5mu \Varid{fromThunk}\mathbin{::}\Conid{Serialized}\;\Varid{a}\mskip1.5mu\}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\mathbf{deriving}\;(\Conid{Typeable}){}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{toThunk}\;\Varid{a}\mathrel{=}\Conid{Thunk}\;\{\mskip1.5mu \Varid{fromThunk}\mathrel{=}\Varid{a}\mskip1.5mu\}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

Additionally, we require a \ensuremath{\Conid{Binary}} for our wrapper in order to be able
to send it with Cloud Haskell. This only delegates to the implementation
of the actual \ensuremath{\Conid{Serialized}} we wrap:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{instance}\;(\Conid{Typeable}\;\Varid{a})\Rightarrow \Conid{Binary}\;(\Conid{Thunk}\;\Varid{a})\;\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Varid{put}\mathrel{=}\Varid{\Conid{Data}.\Conid{Binary}.put}\mathbin{\circ}\Varid{fromThunk}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Varid{get}\mathrel{=}\mathbf{do}{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}(\Varid{ser}\mathbin{::}\Conid{Serialized}\;\Varid{a})\leftarrow \Varid{\Conid{Data}.\Conid{Binary}.get}{}\<[E]%
\\
\>[B]{}\Varid{return}\mathbin{\$}\Conid{Thunk}\;\{\mskip1.5mu \Varid{fromThunk}\mathrel{=}\Varid{ser}\mskip1.5mu\}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

\hypertarget{evaluation-of-values-on-slave-nodes}{%
\subsection{Evaluation of values on slave
nodes}\label{evaluation-of-values-on-slave-nodes}}

\label{sec:cloudhaskellEvaluationOnSlaves}

Having discussed the general communication scheme and serialization
mechanism we want to use, we can explain how the evaluation of values on
slave nodes works with Cloud Haskell, next. We give the master node's
code for evaluation of a single value on a slave node and also the slave
nodes' code.

\hypertarget{master-node}{%
\subsubsection{Master node}\label{master-node}}

\label{sec:cloudhaskellparEvalMasterNode}

On the master node, the function
\ensuremath{\Varid{forceSingle}\mathbin{::}\Conid{NodeId}\to \Conid{MVar}\;\Varid{a}\to \Varid{a}\to \Conid{Process}\;()} is used to evaluate
a single value \ensuremath{\Varid{a}}. It returns a monadic action \ensuremath{\Conid{Process}\;()} that
evaluates a value of type \ensuremath{\Varid{a}} on the node with the given \ensuremath{\Conid{NodeId}} and
stores the evaluated result in the given \ensuremath{\Conid{MVar}\;\Varid{a}}.

Unlike the master from Chapter \ref{sec:sendRecCloud}, it starts by
creating the top level communication channels


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}(\Conid{SendPort}\;\Varid{a},\Conid{ReceivePort}\;\Varid{a}){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

and


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}(\Conid{SendPort}\;(\Conid{SendPort}\;(\Conid{Thunk}\;\Varid{a})),\Conid{ReceivePort}\;(\Conid{SendPort}\;(\Conid{Thunk}\;\Varid{a}))){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

as we have to spawn the slave node from the master. This is different
from the schematic depiction we have seen earlier where the two main
channels were created outside of both functions. The communication
scheme is the same, nevertheless. Furthermore, we intentionally do not
have some \ensuremath{(\Conid{SendPort}\;\Varid{b},\Conid{ReceivePort}\;\Varid{b})} as we only evaluate some \ensuremath{\Varid{a}} and
not transform it. Then, the master spawns the actual evaluation task
(the slave from Chapter \ref{sec:sendRecCloud})


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{evalTask}\mathbin{::}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}(\Conid{SendPort}\;(\Conid{SendPort}\;(\Conid{Thunk}\;\Varid{a})),\Conid{SendPort}\;\Varid{a})\to \Conid{Process}\;(){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

with the necessary \ensuremath{\Conid{SendPort}}s for input communication \linebreak
(\ensuremath{\Conid{SendPort}\;(\Conid{SendPort}\;(\Conid{Thunk}\;\Varid{a}))}) and result communication
(\ensuremath{\Conid{SendPort}\;\Varid{a}}\footnote{Again, the type is \ensuremath{\Varid{a}} instead of some
  potentially other \ensuremath{\Varid{b}} because we only evaluate some \ensuremath{\Varid{a}}.}) on the
given node via


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{spawn}\;\Varid{node}\;(\Varid{evalTask}\;(\Varid{inputSenderSender},\Varid{outputSender})){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

where \ensuremath{\Varid{spawn}} is of type


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{spawn}\mathbin{::}\Conid{NodeId}\to \Conid{Closure}\;(\Conid{Process}\;())\to \Conid{Process}\;\Conid{ProcessId}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

Next, like the master from Chapter \ref{sec:sendRecCloud}, \ensuremath{\Varid{forceSingle}}
waits for the input \ensuremath{\Conid{SendPort}\;\Varid{a}} of the evaluation task with
\ensuremath{\Varid{receiveChan}\;\Varid{inputSenderReceiver}}. After that, it sends the not yet
evaluated, serialized version of \ensuremath{\Varid{a}},
\ensuremath{\Varid{serialized}\leftarrow \Varid{liftIO}\mathbin{\$}\Varid{trySerialize}\;\Varid{a}} over that \ensuremath{\Conid{SendPort}} with
\ensuremath{\Varid{sendChan}\;\Varid{inputSender}\mathbin{\$}\Varid{toThunk}\;\Varid{serialized}} to the evaluating slave
node. Finally, it awaits the result of the evaluation with
\ensuremath{\Varid{forcedA}\leftarrow \Varid{receiveChan}\;\Varid{outputReceiver}} and puts it inside the passed
\ensuremath{\Conid{MVar}\;\Varid{a}} with \ensuremath{\Varid{liftIO}\mathbin{\$}\Varid{putMVar}\;\Varid{out}\;\Varid{forcedA}}. The complete definition
is:

\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{forceSingle}\mathbin{::}(\Conid{Evaluatable}\;\Varid{a})\Rightarrow \Conid{NodeId}\to \Conid{MVar}\;\Varid{a}\to \Varid{a}\to \Conid{Process}\;(){}\<[E]%
\\
\>[B]{}\Varid{forceSingle}\;\Varid{node}\;\Varid{out}\;\Varid{a}\mathrel{=}\mathbf{do}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mbox{\onelinecomment  create the Channel that we use to send the }{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mbox{\onelinecomment  sender of the input from the slave node from}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}(\Varid{inputSenderSender},\Varid{inputSenderReceiver})\leftarrow \Varid{newChan}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mbox{\onelinecomment  create the channel to receive the output from}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}(\Varid{outputSender},\Varid{outputReceiver})\leftarrow \Varid{newChan}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mbox{\onelinecomment  spawn the actual evaluation task on the given node}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mbox{\onelinecomment  and pass the two sender objects we created above}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Varid{spawn}\;\Varid{node}\;(\Varid{evalTask}\;(\Varid{inputSenderSender},\Varid{outputSender})){}\<[E]%
\\[\blanklineskip]%
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mbox{\onelinecomment  wait for the slave to send the input sender}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Varid{inputSender}\leftarrow \Varid{receiveChan}\;\Varid{inputSenderReceiver}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Varid{serialized}\leftarrow \Varid{liftIO}\mathbin{\$}\Varid{trySerialize}\;\Varid{a}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mbox{\onelinecomment  send the input to the slave}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Varid{sendChan}\;\Varid{inputSender}\mathbin{\$}\Varid{toThunk}\;\Varid{serialized}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mbox{\onelinecomment  wait for the result from the slave}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Varid{forcedA}\leftarrow \Varid{receiveChan}\;\Varid{outputReceiver}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mbox{\onelinecomment  put the output back into the passed MVar}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Varid{liftIO}\mathbin{\$}\Varid{putMVar}\;\Varid{out}\;\Varid{forcedA}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks

\hypertarget{slave-node}{%
\subsubsection{Slave node}\label{slave-node}}

\label{sec:cloudhaskellparEvalSlaveNode}

In the definition of \ensuremath{\Varid{forceSingle}}, we use a function


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{evalTask}\mathbin{::}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}(\Conid{SendPort}\;(\Conid{SendPort}\;(\Conid{Thunk}\;\Varid{a})),\Conid{SendPort}\;\Varid{a})\to \Conid{Closure}\;(\Conid{Process}\;()){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

As indicated by the \ensuremath{\Conid{Evaluatable}\;\Varid{a}} in the type signature, this function
is hosted on a \ensuremath{\Conid{Evaluatable}\;\Varid{a}} type class:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{9}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{class}\;(\Conid{Binary}\;\Varid{a},\Conid{Typeable}\;\Varid{a},\Conid{NFData}\;\Varid{a})\Rightarrow \Conid{Evaluatable}\;\Varid{a}\;\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{evalTask}\mathbin{::}(\Conid{SendPort}\;(\Conid{SendPort}\;(\Conid{Thunk}\;\Varid{a})),\Conid{SendPort}\;\Varid{a})\to {}\<[E]%
\\
\>[5]{}\hsindent{4}{}\<[9]%
\>[9]{}\Conid{Closure}\;(\Conid{Process}\;()){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

This abstraction is required because of the way Cloud Haskell does
serialization. We can not write a single definition \ensuremath{\Varid{evalTask}} and
expect it to work even though it would be a valid definition. This is
because for Cloud Haskell to be able to create the required
serialization code, at least in our tests, we require a fixed type,
e.g.~for \ensuremath{\Conid{Int}}s:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{evalTaskInt}\mathbin{::}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}(\Conid{SendPort}\;(\Conid{SendPort}\;(\Conid{Thunk}\;\Conid{Int})),\Conid{SendPort}\;\Conid{Int})\to \Conid{Closure}\;(\Conid{Process}\;()){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

This function can be made remotable with
\ensuremath{\mathbin{\$}(\Varid{remotable}\;[\mskip1.5mu }'\ensuremath{\Varid{evalTaskInt}\mskip1.5mu])}. We can now write a valid Cloud Haskell
compatible instance \ensuremath{\Conid{Evaluatable}\;\Conid{Int}} simply as


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{instance}\;\Conid{Evaluatable}\;\Conid{Int}\;\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{evalTask}\mathrel{=}\Varid{evalTaskInt}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

We do not have to write these manually, though, as as they can easily be
generated with the Template Haskell code generator in Figure
\ref{fig:evalGen} from the Appendix via calls to the following three
Template Haskell functions:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbin{\$}(\Varid{mkEvalTasks}\;[\mskip1.5mu \mathbin{\mathtt{\textquotesingle\textquotesingle}\!\!}\;\Conid{Int}\mskip1.5mu]){}\<[E]%
\\
\>[B]{}\mathbin{\$}(\Varid{mkRemotables}\;[\mskip1.5mu \mathbin{\mathtt{\textquotesingle\textquotesingle}\!\!}\;\Conid{Int}\mskip1.5mu]){}\<[E]%
\\
\>[B]{}\mathbin{\$}(\Varid{mkEvaluatables}\;[\mskip1.5mu \mathbin{\mathtt{\textquotesingle\textquotesingle}\!\!}\;\Conid{Int}\mskip1.5mu]){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

This is possible because \ensuremath{\Varid{evalTaskInt}}, just like any other function on
types that have instances for \ensuremath{\Conid{Binary}\;\Varid{a}}, \ensuremath{\Conid{Typeable}\;\Varid{a}}, and \ensuremath{\Conid{NFData}\;\Varid{a}},
can be just delegated to \ensuremath{\Varid{evalTaskBase}}, which behaves as follows: It
starts by creating the channel that it wants to receive its input from
with \ensuremath{(\Varid{sendMaster},\Varid{rec})\leftarrow \Varid{newChan}}. Next, it sends the
\ensuremath{\Conid{SendPort}\;(\Conid{Thunk}\;\Varid{a})} of this channel back to the master process via
\ensuremath{\Varid{sendChan}\;\Varid{inputPipe}\;\Varid{sendMaster}} to then receive its actual input on the \linebreak
\ensuremath{\Conid{ReceivePort}\;(\Conid{Thunk}\;\Varid{a})} end with \ensuremath{\Varid{thunkA}\leftarrow \Varid{receiveChan}\;\Varid{rec}}. It finally
deserializes this thunk with
\ensuremath{\Varid{a}\leftarrow \Varid{liftIO}\mathbin{\$}\Varid{deserialize}\mathbin{\$}\Varid{fromThunk}\;\Varid{thunkA}} and sends the fully
evaluated result back with \ensuremath{\Varid{sendChan}\;\Varid{output}\;(\Varid{seq}\;(\Varid{rnf}\;\Varid{a})\;\Varid{a})}. Its
complete definition is


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{evalTaskBase}\mathbin{::}(\Conid{Binary}\;\Varid{a},\Conid{Typeable}\;\Varid{a},\Conid{NFData}\;\Varid{a})\Rightarrow {}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}(\Conid{SendPort}\;(\Conid{SendPort}\;(\Conid{Thunk}\;\Varid{a})),\Conid{SendPort}\;\Varid{a})\to \Conid{Process}\;(){}\<[E]%
\\
\>[B]{}\Varid{evalTaskBase}\;(\Varid{inputPipe},\Varid{output})\mathrel{=}\mathbf{do}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}(\Varid{sendMaster},\Varid{rec})\leftarrow \Varid{newChan}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mbox{\onelinecomment  send the master the SendPort, that we}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mbox{\onelinecomment  want to listen the other end on for the input}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Varid{sendChan}\;\Varid{inputPipe}\;\Varid{sendMaster}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mbox{\onelinecomment  receive the actual input}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Varid{thunkA}\leftarrow \Varid{receiveChan}\;\Varid{rec}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mbox{\onelinecomment  and deserialize}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Varid{a}\leftarrow \Varid{liftIO}\mathbin{\$}\Varid{deserialize}\mathbin{\$}\Varid{fromThunk}\;\Varid{thunkA}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mbox{\onelinecomment  force the input and send it back to master}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Varid{sendChan}\;\Varid{output}\;(\Varid{seq}\;(\Varid{rnf}\;\Varid{a})\;\Varid{a}){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

\hypertarget{parallel-evaluation-scheme}{%
\subsection{Parallel evaluation
scheme}\label{parallel-evaluation-scheme}}

\label{sec:cloudhaskellParallelEvaluation}

Since we now know how to evaluate a value on slave nodes via
\ensuremath{\Varid{forceSingle}\mathbin{::}(\Conid{Evaluatable}\;\Varid{a})\Rightarrow \Conid{NodeId}\to \Conid{MVar}\;\Varid{a}\to \Varid{a}\to \Conid{Process}\;()},
we can use this to build up an internal parallel evaluation scheme. For
this, we start by defining an abstraction of a computation as


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{data}\;\Conid{Computation}\;\Varid{a}\mathrel{=}\Conid{Comp}\;\{\mskip1.5mu {}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Varid{computation}\mathbin{::}\Conid{IO}\;(),{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Varid{result}\mathbin{::}\Conid{IO}\;\Varid{a}{}\<[E]%
\\
\>[B]{}\mskip1.5mu\}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

where \ensuremath{\Varid{computation}\mathbin{::}\Conid{IO}\;()} is the \ensuremath{\Conid{IO}\;()} action that has to be
evaluated in order to get a result from \ensuremath{\Varid{result}\mathbin{::}\Conid{IO}\;\Varid{a}}.

Next is the definition of
\ensuremath{\Varid{evalSingle}\mathbin{::}\Conid{Evaluatable}\Rightarrow \Conid{Conf}\to \Conid{NodeId}\to \Varid{a}\to \Conid{IO}\;(\Conid{Computation}\;\Varid{a})}.
Its resulting \ensuremath{\Conid{IO}} action starts by creating an empty \ensuremath{\Conid{MVar}\;\Varid{a}} with
\ensuremath{\Varid{mvar}\leftarrow \Varid{newEmptyMVar}}. Then, it creates an \ensuremath{\Conid{IO}} action that forks away
the evaluation process of \ensuremath{\Varid{forceSingle}} on the single passed value \ensuremath{\Varid{a}}
by means of \ensuremath{\Varid{forkProcess}\mathbin{::}\Conid{LocalNode}\to \Conid{Process}\;()\to \Conid{IO}\;\Conid{ProcessId}} on
the the master node with


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{forkProcess}\;(\Varid{localNode}\;\Varid{conf})\mathbin{\$}\Varid{forceSingle}\;\Varid{node}\;\Varid{mvar}\;\Varid{a}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

The action concludes by returning a \ensuremath{\Conid{Computation}\;\Varid{a}} encapsulating the
evaluation \ensuremath{\Conid{IO}\;()} and the result communication action
\ensuremath{\Varid{takeMVar}\;\Varid{mvar}\mathbin{::}\Conid{IO}\;\Varid{a}}:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}c<{\hspost}@{}}%
\column{5E}{@{}l@{}}%
\column{9}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{evalSingle}\mathbin{::}\Conid{Evaluatable}\;\Varid{a}\Rightarrow \Conid{Conf}\to \Conid{NodeId}\to \Varid{a}\to \Conid{IO}\;(\Conid{Computation}\;\Varid{a}){}\<[E]%
\\
\>[B]{}\Varid{evalSingle}\;\Varid{conf}\;\Varid{node}\;\Varid{a}\mathrel{=}\mathbf{do}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Varid{mvar}\leftarrow \Varid{newEmptyMVar}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{let}\;\Varid{comp}\mathrel{=}\Varid{forkProcess}\;(\Varid{localNode}\;\Varid{conf})\mathbin{\$}\Varid{forceSingle}\;\Varid{node}\;\Varid{mvar}\;\Varid{a}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Varid{return}\mathbin{\$}\Conid{Comp}\;\{\mskip1.5mu {}\<[E]%
\\
\>[3]{}\hsindent{6}{}\<[9]%
\>[9]{}\Varid{computation}\mathrel{=}\Varid{comp}\sequ \Varid{return}\;(){}\<[E]%
\\
\>[3]{}\hsindent{6}{}\<[9]%
\>[9]{}\Varid{result}\mathrel{=}\Varid{takeMVar}\;\Varid{mvar}{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\mskip1.5mu\}{}\<[5E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

With this, we can easily define a function
\ensuremath{\Varid{evalParallel}\mathbin{::}\Conid{Evaluatable}\;\Varid{a}\Rightarrow \Conid{Conf}\to [\mskip1.5mu \Varid{a}\mskip1.5mu]\to \Conid{IO}\;(\Conid{Computation}\;[\mskip1.5mu \Varid{a}\mskip1.5mu])}
that builds an \ensuremath{\Conid{IO}} action containing a parallel \linebreak \ensuremath{\Conid{Computation}\;[\mskip1.5mu \Varid{a}\mskip1.5mu]} from
an input list \ensuremath{[\mskip1.5mu \Varid{a}\mskip1.5mu]}. This \ensuremath{\Conid{IO}} action starts by retrieving the current
list of workers with \ensuremath{\Varid{workers}\leftarrow \Varid{readMVar}\mathbin{\$}\Varid{workers}\;\Varid{conf}}. It continues
by shuffling this list of workers with
\ensuremath{\Varid{shuffledWorkers}\leftarrow \Varid{randomShuffle}\;\Varid{workers}}\footnote{\ensuremath{\Varid{randomShuffle}\mathbin{::}[\mskip1.5mu \Varid{a}\mskip1.5mu]\to \Conid{IO}\;[\mskip1.5mu \Varid{a}\mskip1.5mu]}
  is from \url{https://wiki.haskell.org/Random_shuffle}.} to ensure at
least some level of equal work distribution between multiple calls to
\ensuremath{\Varid{evalParallel}}. Then, the input values \ensuremath{[\mskip1.5mu \Varid{a}\mskip1.5mu]} are assigned to their
corresponding workers to finally build the list of parallel computations
\ensuremath{[\mskip1.5mu \Conid{Computation}\;\Varid{a}\mskip1.5mu]} with
\ensuremath{\Varid{comps}\leftarrow \Varid{sequence}\mathbin{\$}\Varid{map}\;(\Varid{uncurry}\mathbin{\$}\Varid{evalSingle}\;\Varid{conf})\;\Varid{workAssignment}}.
The action concludes by turning this list \ensuremath{[\mskip1.5mu \Conid{Computation}\;\Varid{a}\mskip1.5mu]} into a
computation of a list \ensuremath{\Conid{Computation}\;[\mskip1.5mu \Varid{a}\mskip1.5mu]} with
\ensuremath{\Varid{return}\mathbin{\$}\Varid{sequenceComp}\;\Varid{comps}}.


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{evalParallel}\mathbin{::}\Conid{Evaluatable}\;\Varid{a}\Rightarrow \Conid{Conf}\to [\mskip1.5mu \Varid{a}\mskip1.5mu]\to \Conid{IO}\;(\Conid{Computation}\;[\mskip1.5mu \Varid{a}\mskip1.5mu]){}\<[E]%
\\
\>[B]{}\Varid{evalParallel}\;\Varid{conf}\;\Varid{as}\mathrel{=}\mathbf{do}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Varid{workers}\leftarrow \Varid{readMVar}\mathbin{\$}\Varid{workers}\;\Varid{conf}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mbox{\onelinecomment  shuffle the list of workers, so we don not end up spawning}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mbox{\onelinecomment  all tasks in the same order everytime}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Varid{shuffledWorkers}\leftarrow \Varid{randomShuffle}\;\Varid{workers}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mbox{\onelinecomment  complete the work assignment node to task (NodeId, a)}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{let}\;\Varid{workAssignment}\mathrel{=}\Varid{zipWith}\;(,)\;(\Varid{cycle}\;\Varid{shuffledWorkers})\;\Varid{as}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mbox{\onelinecomment  build the parallel computation with sequence}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Varid{comps}\leftarrow \Varid{sequence}\mathbin{\$}\Varid{map}\;(\Varid{uncurry}\mathbin{\$}\Varid{evalSingle}\;\Varid{conf})\;\Varid{workAssignment}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Varid{return}\mathbin{\$}\Varid{sequenceComp}\;\Varid{comps}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

Here, the definition of
\ensuremath{\Varid{sequenceComp}\mathbin{::}[\mskip1.5mu \Conid{Computation}\;\Varid{a}\mskip1.5mu]\to \Conid{Computation}\;[\mskip1.5mu \Varid{a}\mskip1.5mu]} is


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{9}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{sequenceComp}\mathbin{::}[\mskip1.5mu \Conid{Computation}\;\Varid{a}\mskip1.5mu]\to \Conid{Computation}\;[\mskip1.5mu \Varid{a}\mskip1.5mu]{}\<[E]%
\\
\>[B]{}\Varid{sequenceComp}\;\Varid{comps}\mathrel{=}\Conid{Comp}\;\{\mskip1.5mu \Varid{computation}\mathrel{=}\Varid{newComp},\Varid{result}\mathrel{=}\Varid{newRes}\mskip1.5mu\}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{where}\;\Varid{newComp}\mathrel{=}\Varid{sequence\char95 }\mathbin{\$}\Varid{map}\;\Varid{computation}\;\Varid{comps}{}\<[E]%
\\
\>[3]{}\hsindent{6}{}\<[9]%
\>[9]{}\Varid{newRes}\mathrel{=}\Varid{sequence}\mathbin{\$}\Varid{map}\;\Varid{result}\;\Varid{comps}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

Now, in order to start the actual computation from a blueprint in
\ensuremath{\Conid{Computation}\;\Varid{a}} and get the result back as a pure value \ensuremath{\Varid{a}}, we have to
use the function \ensuremath{\Varid{runComputation}\mathbin{::}\Conid{IO}\;(\Conid{Computation}\;\Varid{a})\to \Varid{a}} defined as
follows:

Internally, it uses an \ensuremath{\Conid{IO}\;\Varid{a}} action that starts by unwrapping
\ensuremath{\Conid{Computation}\;\Varid{a}} from the input \ensuremath{\Conid{IO}\;(\Conid{Computation}\;\Varid{a})} with \ensuremath{\Varid{comp}\leftarrow \Varid{x}} in
order to launch the actual evaluation with \ensuremath{\Varid{computation}\;\Varid{comp}}. The \ensuremath{\Conid{IO}}
action concludes by returning the result with \ensuremath{\Varid{result}\;\Varid{comp}}. Now, in
order to turn the \ensuremath{\Conid{IO}\;\Varid{a}} action into \ensuremath{\Varid{a}}, we have to use
\ensuremath{\Varid{unsafePerformIO}\mathbin{::}\Conid{IO}\;\Varid{a}\to \Varid{a}} which allows us to unwrap the pure values
\enquote{contained} in \ensuremath{\Conid{IO}} actions. Generally, the use of this function
is discouraged, because it can introduce severe bugs if not handled with
utmost care. Here, its use is necessary and absolutely fine, though,
since we only do evaluation inside the \ensuremath{\Conid{IO}} Monad and if this were to
fail, the computation would be wrong anyways. Also in order to force the
compiler to not inline the result as we do not want to spawn the
computation multiple times, we protect the definition of
\ensuremath{\Varid{runComputation}} with a \ensuremath{\Conid{NOINLINE}} pragma:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mbox{\enskip\{-\# NOINLINE runComputation  \#-\}\enskip}{}\<[E]%
\\
\>[B]{}\Varid{runComputation}\mathbin{::}\Conid{IO}\;(\Conid{Computation}\;\Varid{a})\to \Varid{a}{}\<[E]%
\\
\>[B]{}\Varid{runComputation}\;\Varid{x}\mathrel{=}\Varid{unsafePerformIO}\mathbin{\$}\mathbf{do}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Varid{comp}\leftarrow \Varid{x}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Varid{computation}\;\Varid{comp}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Varid{result}\;\Varid{comp}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

\hypertarget{implementing-the-parrows-api}{%
\section{Implementing the PArrows
API}\label{implementing-the-parrows-api}}

\label{sec:CloudHaskellArrowParallel}

Finally, we describe in this chapter how to implement the PArrows API
with Cloud Haskell and evaluate our results.

We start by explaining how to implement \ensuremath{\Conid{ArrowParallel}} in Chapter
\ref{sec:CloudHaskellArrowParallelInstance}. Then, we discuss the limits
of the current code and explain why we can not yet give a proper
instance for \ensuremath{\Conid{ArrowLoopParallel}} or a proper \ensuremath{\Conid{Future}} implementation in
Chapter \ref{sec:CloudHaskellArrowParallelLimits}. Finally, we lay out a
possible solution to this which could be implemented in the future in
Chapter \ref{sec:CloudHaskellArrowParallelLimitsMitigation}.

\hypertarget{arrowparallel-instance}{%
\subsection{\texorpdfstring{\ensuremath{\Conid{ArrowParallel}}
instance}{ instance}}\label{arrowparallel-instance}}

\label{sec:CloudHaskellArrowParallelInstance}

We will now give an experimental implementation of the \ensuremath{\Conid{ArrowParallel}}
type class with Cloud Haskell. Obviously, as already mentioned earlier,
here the additional conf parameter is the \ensuremath{\Conid{State}\mathbin{/}\Conid{Conf}} type.

We implement \ensuremath{\Varid{parEvalN}} for our \ensuremath{\Conid{ArrowParallel}\;\Varid{arr}\;\Varid{a}\;\Varid{b}\;\Conid{Conf}} instance as
follows: We start off by forcing the input \ensuremath{[\mskip1.5mu \Varid{a}\mskip1.5mu]} into normal form with
\ensuremath{\Varid{arr}\;\Varid{force}}, as during testing this was found necessary because a not
fully evaluated value \ensuremath{\Varid{a}} can still have attached things like a file
handle which may be not serializable. Then, the parallel Arrow goes on
to feed the now fully forced input list \ensuremath{[\mskip1.5mu \Varid{a}\mskip1.5mu]} into the evaluation Arrow
obtained by applying \ensuremath{\Varid{evalN}\mathbin{::}[\mskip1.5mu \Varid{arr}\;\Varid{a}\;\Varid{b}\mskip1.5mu]\to \Varid{arr}\;[\mskip1.5mu \Varid{a}\mskip1.5mu]\;[\mskip1.5mu \Varid{b}\mskip1.5mu]} to the list of
Arrows to be parallelized \ensuremath{[\mskip1.5mu \Varid{arr}\;\Varid{a}\;\Varid{b}\mskip1.5mu]} with \ensuremath{\Varid{evalN}\;\Varid{fs}}. This results in a
not yet evaluated list of results \ensuremath{[\mskip1.5mu \Varid{b}\mskip1.5mu]} which is prepared to be forked
away with \ensuremath{\Varid{arr}\;(\Varid{evalParallel}\;\Varid{conf})\mathbin{::}\Varid{arr}\;[\mskip1.5mu \Varid{a}\mskip1.5mu]\;(\Conid{Computation}\;[\mskip1.5mu \Varid{b}\mskip1.5mu])}. The
resulting computation blueprint is finally executed with
\ensuremath{\Varid{arr}\;\Varid{runComputation}\mathbin{::}\Varid{arr}\;(\Conid{Computation}\;[\mskip1.5mu \Varid{b}\mskip1.5mu])\;[\mskip1.5mu \Varid{b}\mskip1.5mu]}.


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{4}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{9}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{instance}\;(\Conid{NFData}\;\Varid{a},\Conid{Evaluatable}\;\Varid{b},\Conid{ArrowChoice}\;\Varid{arr})\Rightarrow {}\<[E]%
\\
\>[B]{}\hsindent{4}{}\<[4]%
\>[4]{}\Conid{ArrowParallel}\;\Varid{arr}\;\Varid{a}\;\Varid{b}\;\Conid{Conf}\;\mathbf{where}{}\<[E]%
\\
\>[4]{}\hsindent{1}{}\<[5]%
\>[5]{}\Varid{parEvalN}\;\Varid{conf}\;\Varid{fs}\mathrel{=}{}\<[E]%
\\
\>[5]{}\hsindent{4}{}\<[9]%
\>[9]{}\Varid{arr}\;\Varid{force}\mathbin{>\!\!>\!\!>}{}\<[E]%
\\
\>[5]{}\hsindent{4}{}\<[9]%
\>[9]{}\Varid{evalN}\;\Varid{fs}\mathbin{>\!\!>\!\!>}{}\<[E]%
\\
\>[5]{}\hsindent{4}{}\<[9]%
\>[9]{}\Varid{arr}\;(\Varid{evalParallel}\;\Varid{conf})\mathbin{>\!\!>\!\!>}{}\<[E]%
\\
\>[5]{}\hsindent{4}{}\<[9]%
\>[9]{}\Varid{arr}\;\Varid{runComputation}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

\hypertarget{limits-of-the-current-implementation}{%
\subsection{Limits of the current
implementation}\label{limits-of-the-current-implementation}}

\label{sec:CloudHaskellArrowParallelLimits}

Similar to the GpH and \ensuremath{\Conid{Par}} Monad backends, our experimental Cloud
Haskell backend suffers from the problem that it does not work in
conjunction with the looping skeletons \ensuremath{\Varid{pipe}}, \ensuremath{\Varid{ring}} and \ensuremath{\Varid{torus}}. All
testing programs would refuse to compute anything and hang indefinitely.
While this is no big problem for the shared-memory backends where we
could just implement a workaround with the help of an
\ensuremath{\Conid{ArrowLoopParallel}} instance


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{instance}\;(\Conid{ArrowChoice}\;\Varid{arr},\Conid{ArrowParallel}\;\Varid{arr}\;\Varid{a}\;\Varid{b}\;\Conid{Conf})\Rightarrow {}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Conid{ArrowLoopParallel}\;\Varid{arr}\;\Varid{a}\;\Varid{b}\;\Conid{Conf}\;\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{loopParEvalN}\;\anonymous \mathrel{=}\Varid{evalN}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{postLoopParEvalN}\mathrel{=}\Varid{parEvalN}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

a similar solution would not be feasible here because we are in a
distributed-memory setting. The topology skeletons would become
meaningless as all benefits of using a sophisticated distributed
evaluation scheme would be lost.

Since it would not make sense to have a \ensuremath{\Conid{Future}} instance without proper
support for skeletons that could make use of it, we also do not give an
implementation for a \ensuremath{\Conid{CloudFuture}} in this thesis.

\hypertarget{possible-mitigation-of-the-limits}{%
\subsection{Possible mitigation of the
limits}\label{possible-mitigation-of-the-limits}}

\label{sec:CloudHaskellArrowParallelLimitsMitigation}

While investigating the problem with the looping skeletons, we noticed a
difference in behaviour between Eden and all other backends including
our experimental Cloud Haskell backend: Eden streams lists of data \ensuremath{[\mskip1.5mu \Varid{a}\mskip1.5mu]}
instead of sending the complete list as one big serialized chunk.
Another difference is that tuples of data \ensuremath{(\Varid{a},\Varid{b},\mathbin{...})} are sent in
parallel on \(n\) threads for a tuple of \(n\) entries. When
investigating the \ensuremath{\Varid{torus}} or \ensuremath{\Varid{ring}} skeletons we ported from Eden, we
notice how these two specialities are important. For example, in the
\ensuremath{\Varid{ring}} skeleton we build up the resulting Arrow so that it calculates
the result in multiple rounds:


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{9}{@{}>{\hspre}l<{\hspost}@{}}%
\column{13}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{ring}\;\Varid{conf}\;\Varid{f}\mathrel{=}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{loop}\;(\Varid{second}\;(\Varid{rightRotate}\mathbin{>\!\!>\!\!>}\Varid{lazy})\mathbin{>\!\!>\!\!>}{}\<[E]%
\\
\>[5]{}\hsindent{4}{}\<[9]%
\>[9]{}\mbox{\onelinecomment  convert the current input into a form we can process in this round}{}\<[E]%
\\
\>[5]{}\hsindent{4}{}\<[9]%
\>[9]{}\Varid{arr}\;(\Varid{uncurry}\;\Varid{zip})\mathbin{>\!\!>\!\!>}{}\<[E]%
\\
\>[5]{}\hsindent{4}{}\<[9]%
\>[9]{}\mbox{\onelinecomment  here, we evaluate the current round}{}\<[E]%
\\
\>[5]{}\hsindent{4}{}\<[9]%
\>[9]{}\Varid{loopParEvalN}\;\Varid{conf}\;{}\<[E]%
\\
\>[9]{}\hsindent{4}{}\<[13]%
\>[13]{}(\Varid{repeat}\;(\Varid{second}\;(\Varid{get}\;\Varid{conf})\mathbin{>\!\!>\!\!>}\Varid{f}\mathbin{>\!\!>\!\!>}\Varid{second}\;(\Varid{put}\;\Varid{conf})))\mathbin{>\!\!>\!\!>}{}\<[E]%
\\
\>[5]{}\hsindent{4}{}\<[9]%
\>[9]{}\mbox{\onelinecomment  put the current result back into the original input form}{}\<[E]%
\\
\>[5]{}\hsindent{4}{}\<[9]%
\>[9]{}\Varid{arr}\;\Varid{unzip})\mathbin{>\!\!>\!\!>}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{postLoopParEvalN}\;\Varid{conf}\;(\Varid{repeat}\;(\Varid{arr}\;\Varid{id})){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

Here, anything other than the exact same behaviour as Eden will result
in a dead-lock when using \ensuremath{\Varid{loopParEvalN}\mathrel{=}\Varid{parEvalN}}. We therefore
believe that it is crucial for a proper Cloud Haskell backend to have
the same streaming behaviour as Eden does. While we are confident that
this is definitely possible to achieve with Cloud Haskell, as early
experiments on this suggest, we have to date not been able to achieve
proper streaming behaviour, though. We stopped further development here,
as this would have bursted the scope of this thesis.

Nevertheless, the most promising idea to implement the correct behaviour
is to use a more sophisticated mechanism to stream data back to the
master node by using pipes along the lines of


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{type}\;\Conid{PipeIn}\;\Varid{a}\mathrel{=}\Conid{SendPort}\;(\Conid{SendPort}\;(\Conid{Maybe}\;(\Conid{SendPort}\;(\Conid{Maybe}\;\Varid{a})))){}\<[E]%
\\
\>[B]{}\mathbf{type}\;\Conid{PipeOut}\;\Varid{a}\mathrel{=}\Conid{ReceivePort}\;(\Conid{SendPort}\;(\Conid{Maybe}\;(\Conid{SendPort}\;(\Conid{Maybe}\;\Varid{a})))){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

where \ensuremath{\Conid{PipeIn}\;\Varid{a}} would be the port where the evaluating process on the
slave node sends its result through to the corresponding \ensuremath{\Conid{PipeOut}\;\Varid{a}} on
the master node. Note that we here encode a \enquote{stream} of some \ensuremath{\Varid{a}}
with \ensuremath{\Conid{SendPort}\;(\Conid{Maybe}\;\Varid{a})}: For types with singular values, we just
request one value. And on types like e.g.~a list \ensuremath{[\mskip1.5mu \Varid{a}\mskip1.5mu]} we expect
multiple singleton lists \ensuremath{\Conid{Just}\;[\mskip1.5mu \Varid{a}\mskip1.5mu]}, on the \ensuremath{\Conid{SendPort}} and the end of
the input with \ensuremath{\Conid{Nothing}}. For other multi-valued types, this would work
similar even if some hacks would be required.

Then, in order to communicate the result from the slave node, we would
first send \ensuremath{\Conid{SendPort}\;(\Conid{Maybe}\;(\Conid{SendPort}\;(\Conid{Maybe}\;\Varid{a})))} on which the
slave-node would want to receive the stream of \ensuremath{\Conid{SendPort}\;(\Conid{Maybe}\;\Varid{a})}.
This stream of \ensuremath{\Conid{SendPort}}s is required instead of a singular \ensuremath{\Conid{SendPort}}
because of types that have to be sent by multiple threads like
e.g.~tuples. Via these \ensuremath{\Conid{SendPort}\;(\Conid{Maybe}\;\Varid{a})}s the slave can then finally
communicate the stream of evaluated results back to the master node. A
corresponding communication scheme doing the necessary opposite tasks
would obviously be required on the master node.

During testing, as already mentioned, we were not successful in making
this idea work with the looping skeletons.\footnote{It did however still
  work with non-looping skeletons.} We still believe that this path is
worth exploring further in the future, though.

\hypertarget{experimental-performance-results}{%
\chapter{Experimental performance
results}\label{experimental-performance-results}}

\label{sec:benchmarks}

The preceding chapters have shown what PArrows are and how expressive
they are. In this chapter, we will now evaluate the performance overhead
of our compositional abstraction in comparison to GpH and the \ensuremath{\Conid{Par}}
Monad on shared memory architectures and Eden on a distributed memory
cluster.\footnote{We do not include the Cloud Haskell backend here, as
  it is still a work-in-progress.} We describe our measurement platform
(Chapter \ref{sec:benchmarksMeasurementPlatform}), the benchmark results
(Chapter \ref{sec:benchmarkResults}) -- the shared-memory variants (GpH,
\ensuremath{\Conid{Par}} Monad and Eden CP) followed by Eden in a distributed-memory
setting, and conclude that PArrows hold up in terms of performance when
compared to the original parallel Haskells (Chapter
\ref{sec:benchmarksEvaluation}).

\hypertarget{measurement-platform}{%
\section{Measurement platform}\label{measurement-platform}}

\label{sec:benchmarksMeasurementPlatform}

We will now discuss our measurement platform. We start by explaining the
hardware and software stack in Chapter \ref{sec:hardwareSoftware} and
outline the benchmark programs and motivation for choosing them in
Chapter \ref{sec:benchmarksBenchmarks}. Chapter
\ref{sec:whichHaskellWhere} goes over the specifics of where each
benchmark was run. We also shortly address hyper-threading and why we do
not use it in our benchmarks in Chapter \ref{sec:effect-hyper-thread}.

\hypertarget{hardware-and-software}{%
\subsection{Hardware and software}\label{hardware-and-software}}

\label{sec:hardwareSoftware}

The benchmarks are executed both in a shared and in a distributed memory
setting using the Glasgow GPG Beowulf cluster, consisting of 16 machines
with 2 Intel® Xeon® E5-2640 v2 and 64 GB of DDR3 RAM each. Each
processor has 8 cores and 16 (hyper-threaded) threads with a base
frequency of 2 GHz and a turbo frequency of 2.50 GHz. This results in a
total of 256 cores and 512 threads for the whole cluster. The operating
system was Ubuntu 14.04 LTS with Kernel 3.19.0-33. We found that
hyper-threading does not provide any particular interesting insight over
using real 16 cores in terms of performance in our benchmarks (numbers
here for a single machine) as will be discussed in Chapter
\ref{sec:effect-hyper-thread}. We therefore disregard the
hyper-threading ability in most of the cases.

Apart from Eden, all benchmarks and libraries were compiled with
Stack's\footnote{See \url{https://www.haskellstack.org/}.} lts-7.1 GHC
compiler which is equivalent to a standard GHC 8.0.1 with the base
package in version 4.9.0.0. Stack itself was used in version 1.3.2. For
GpH in its Multicore variant we used the \emph{parallel} package in
version 3.2.1.0\footnote{See
  \url{https://hackage.haskell.org/package/parallel-3.2.1.0}.}, while for
the \ensuremath{\Conid{Par}} Monad we used \emph{monad-par} in version 0.3.4.8\footnote{See
  \url{https://hackage.haskell.org/package/monad-par-0.3.4.8}.}. For all
Eden tests, we used a manually built GHC-Eden compiler in version
7.8.2\footnote{See
  \url{http://www.mathematik.uni-marburg.de/~eden/?content=build_eden_7_&navi=build}.}
together with OpenMPI 1.6.5\footnote{See
  \url{https://www.open-mpi.org/software/ompi/v1.6/}.}.

Furthermore, all benchmarks were done with help of the bench\footnote{See
  \url{https://hackage.haskell.org/package/bench}.} tool in version 1.0.2
which uses criterion (\textgreater{}=1.1.1.0 \&\& \textless{}
1.2)\footnote{See
  \url{https://hackage.haskell.org/package/criterion-1.1.1.0}.}
internally. All runtime data (mean runtime, max stddev, etc.) was
collected by this tool.

We used a single node with 16 real cores as a shared memory test-bed and
the whole grid with 256 real cores as a device to test our distributed
memory software.

\hypertarget{benchmarks}{%
\subsection{Benchmarks}\label{benchmarks}}

\label{sec:benchmarksBenchmarks}

We measure four benchmarks from different sources. Most of them are
parallel mathematical computations, initially implemented in Eden. Table
\ref{tbl:benches} summarises.

\begin{longtable}[]{@{}lcccc@{}}
\caption{The benchmarks we use in this thesis.
\label{tbl:benches}}\tabularnewline
\toprule
Name & Area & Type & Origin & Source\tabularnewline
\midrule
\endfirsthead
\toprule
Name & Area & Type & Origin & Source\tabularnewline
\midrule
\endhead
Rabin--Miller test & Mathematics & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\ensuremath{\Varid{parMap}}\\ \& \ensuremath{\Varid{reduce}}\end{tabular}} & Eden & Lobachev
(\protect\hyperlink{ref-Lobachev2012}{2012})\tabularnewline
Jacobi sum test & Mathematics & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}\ensuremath{\Varid{workpool}}\\ \& \ensuremath{\Varid{reduce}}\end{tabular}} & Eden & Lobachev
(\protect\hyperlink{ref-Lobachev2012}{2012})\tabularnewline
Gentleman & Mathematics & \ensuremath{\Varid{torus}} & Eden & Loogen et al.
(\protect\hyperlink{ref-Eden:SkeletonBookChapter02}{2003})\tabularnewline
Sudoku & Puzzle & \ensuremath{\Varid{parMap}} & \ensuremath{\Conid{Par}} Monad & Marlow et al.
(\protect\hyperlink{ref-par-monad}{2011})\footnote{actual code from:
  \url{http://community.haskell.org/\~simonmar/par-tutorial.pdf} and
  \url{https://github.com/simonmar/parconc-examples}.}\tabularnewline
\bottomrule
\end{longtable}

The Rabin--Miller test is a probabilistic primality test that iterates
multiple (here: 32--256) \enquote{subtests}. Should a subtest fail, the
input is definitely not a prime. If all \(n\) subtest pass, the input is
composite with the probability of \(1/4^{n}\).

The Jacobi sum test or APRCL is also a primality test, that however,
guarantees the correctness of the result. It is probabilistic in the
sense that its run time is not certain. Unlike Rabin--Miller test, the
subtests of Jacobi sum test have very different durations. Lobachev
(\protect\hyperlink{ref-lobachev-phd}{2011}) discusses some
optimisations of parallel APRCL. Generic parallel implementations of
Rabin--Miller test and APRCL were presented in Lobachev
(\protect\hyperlink{ref-Lobachev2012}{2012}).

Gentleman is a standard Eden test program, developed for their \ensuremath{\Varid{torus}}
skeleton. It implements a Gentleman's algorithm for parallel matrix
multiplication (Gentleman, \protect\hyperlink{ref-Gentleman1978}{1978}).
We ported an Eden-based version (Loogen et al.,
\protect\hyperlink{ref-Eden:SkeletonBookChapter02}{2003}) to PArrows and
completed the necessary implementations for all sequential parts that
were left out.

A parallel Sudoku solver was used by Marlow et al.
(\protect\hyperlink{ref-par-monad}{2011}) to compare \ensuremath{\Conid{Par}} Monad to GpH.
We ported it to our PArrows DSL.

\hypertarget{which-parallel-haskells-run-where}{%
\subsection{Which parallel Haskells run
where}\label{which-parallel-haskells-run-where}}

\label{sec:whichHaskellWhere}

The \ensuremath{\Conid{Par}} Monad and GpH -- in its multicore version (Marlow et al.,
\protect\hyperlink{ref-Marlow2009}{2009}) -- can be executed on shared
memory machines only. Although GpH is available on distributed memory
clusters, current support of distributed memory in PArrows is limited to
Eden. We used the MPI backend of Eden in a distributed memory setting.
However, Eden features a \enquote{CP} backend for shared memory that
merely copies the memory blocks between disjoint heaps. In this mode,
Eden still operates in the \enquote{nothing shared} setting, but is
adapted better to multicore machines. We call this version of Eden
\enquote{Eden CP}.

\hypertarget{effect-of-hyper-threading}{%
\subsection{Effect of hyper-threading}\label{effect-of-hyper-threading}}

\label{sec:effect-hyper-thread}

In preliminary tests, the PArrows version of the Rabin-Miller test on a
single node of the Glasgow cluster showed almost linear speedup on up to
16 shared-memory cores (as supplementary materials show). The speedup of
64-task PArrows/Eden at 16 real cores version was 13.65 giving a
parallel efficiency of 85.3\%. However, if we increased the number of
requested cores to 32 -- i.e.~if we use hyper-threading on 16 real cores
-- the speedup did not increase that well. It was merely 15.99 for 32
tasks with PArrows/Eden. This was worse for other implementations. As
for 64 tasks, we obtained a speedup of 16.12 with PArrows/Eden at 32
hyper-threaded cores and only 13.55 with PArrows/GpH.

While this shows that hyper-threading can be of benefit in real-world
scenarios running similar workloads to the ones presented in the
benchmarks, we only use real cores for the performance measurements in
Chapter \ref{sec:benchmarkResults} as the purpose of this chapter is to
show the performance of PArrows and not to investigate parallel
behaviour with hyper-threading.

\hypertarget{benchmark-results}{%
\section{Benchmark results}\label{benchmark-results}}

\label{sec:benchmarkResults}

We compare the PArrow performance with direct implementations of the
benchmarks in Eden, GpH and the \ensuremath{\Conid{Par}} Monad. We start with the
definition of speedup (Chapter \ref{sec:defSpeedup}) and mean overhead
(Chapter \ref{sec:defOverhead}) to evaluate PArrows-enabled and standard
benchmark implementations. We continue by comparing speedups and
overheads for the shared memory implementations in Chapter
\ref{sec:benchmarksSharedMem} and then study OpenMPI variants of the
Eden-enabled PArrows as a representative of a distributed memory backend
in Chapter \ref{sec:benchmarksDistMem}. We plot all speedup curves and
all overhead values in the Appendix in \ref{sec:benchmarkSharedPlots}
and \ref{sec:benchmarkDistPlots} for the shared memory and distributed
memory benchmarks, respectively.

\hypertarget{defining-speedup}{%
\subsection{Defining speedup}\label{defining-speedup}}

\label{sec:defSpeedup}

In the following, when we talk about speedup, we use the common
definition

\[
S = \frac{T_1}{T_p}
\]

where \(T_1\) denotes the sequential and \(T_p\) the parallel runtime of
the program. Note that we do not use a separate sequential program,
though. Instead we simply use the same binary with only 1 computation
thread enabled.

\hypertarget{defining-overhead}{%
\subsection{Defining overhead}\label{defining-overhead}}

\label{sec:defOverhead}

We compare the mean overhead, i.e.~the difference of mean relative
wall-clock run time between the PArrow and direct benchmark
implementations executed multiple times with the same settings. The
error margins of the time measurements, supplied by criterion
package\footnote{See
  \url{https://hackage.haskell.org/package/criterion-1.1.1.0}.}, yield
the error margin of the mean overhead.

Quite often the zero value lies in the error margin of the mean
overhead. This means that even though we have measured some difference
(against or even in favour of PArrows), it could be merely the error
margin of the measurement and the difference might not be existent. We
are mostly interested in the cases where above issue does not persist,
we call them \emph{significant}. We usually denote the error margin with
\(\pm\) after the mean overhead value.

\hypertarget{shared-memory}{%
\subsection{Shared memory}\label{shared-memory}}

\label{sec:benchmarksSharedMem}

\hypertarget{speedup}{%
\subsubsection{Speedup}\label{speedup}}

The Rabin--Miller benchmark showed almost linear speedup for both 32 and
64 tasks, the performance is slightly better in the latter case: 13.7 at
16 cores for input \(2^{11213}-1\) and 64 tasks in the best case
scenario with Eden CP. The performance of the Sudoku benchmark merely
reaches a speedup of 9.19 (GpH), 8.78 (\ensuremath{\Conid{Par}} Monad), 8.14 (Eden CP) for
16 cores and 1000 Sudokus. In contrast to Rabin--Miller, here the \ensuremath{\Conid{GpH}}
seems to be the best of all, while Rabin--Miller profited most from Eden
CP (i.e.~Eden with direct memory copy) implementation of PArrows.
Gentleman on shared memory has a plummeting speedup curve with GpH and
\ensuremath{\Conid{Par}} Monad and logarithmically increasing speedup for the Eden-based
version. The latter reached a speedup of 6.56 at 16 cores.

\hypertarget{overhead}{%
\subsubsection{Overhead}\label{overhead}}

For the shared memory Rabin--Miller benchmark, implemented with PArrows
using Eden CP, GpH, and \ensuremath{\Conid{Par}} Monad, the overhead values are within
single percents range, but also negative overhead (i.e.~PArrows are
better) and larger error margins happen. To give a few examples, the
overhead for Eden CP with input value \(2^{11213}-1\), 32 tasks, and 16
cores is \(1.5\%\), but the error margin is around \(5.2\%\)! Same
implementation in the same setting with 64 tasks reaches \(-0.2\%\)
overhead, PArrows apparently fare better than Eden -- but the error
margin of \(1.9\%\) disallows this interpretation.

We focus now on significant overhead values. To name a few:
\(0.41\%\; \pm 7\cdot 10^{-2}\%\) for Eden CP and 64 tasks at 4 cores;
\(4.7\% \; \pm 0.72\%\) for GpH, 32 tasks, 8 cores;
\(0.34\% \; \pm 0.31\%\) for \ensuremath{\Conid{Par}} Monad at 4 cores with 64 tasks. The
worst significant overhead was in case of GpH with \(8\% \; \pm 6.9\%\)
at 16 cores with 32 tasks and input value \(2^{11213}-1\). In other
words, we notice no major slow-down through PArrows here.

For Sudoku the situation is slightly different. There is a minimal
significant (\(-1.4\% \; \pm 1.2\%\) at 8 cores) speed
\emph{improvement} with PArrows Eden CP version when compared with the
base Eden CP benchmark. However, with increasing number of cores the
error margin contains zero again: \(-1.6\% \; \pm 5.0\%\) at 16 cores.
The \ensuremath{\Conid{Par}} Monad shows a similar development, e.g.~with
\(-1.95\% \; \pm 0.64\%\) at 8 cores. The GpH version shows both a
significant speed improvement of \(-4.2\% \; \pm 0.26\%\) (for 16 cores)
with PArrows and a minor overhead of \(0.87\% \; \pm 0.70\%\) (4 cores).

The Gentleman multiplication with Eden CP shows a minor significant
overhead of \(2.6\% \; \pm 1.0\%\) at 8 cores and an insignificant
improvement at 16 cores.

Summarising, we observe a low (if significant at all) overhead, induced
by PArrows in the shared memory setting.

\hypertarget{distributed-memory}{%
\subsection{Distributed memory}\label{distributed-memory}}

\label{sec:benchmarksDistMem}

\hypertarget{speedup-1}{%
\subsubsection{Speedup}\label{speedup-1}}

The speedup of distributed memory Rabin--Miller benchmark with PArrows
and Eden showed an almost linear speedup excepting around 192 cores
where an unfortunate task distribution reduces performance. As seen in
Figure \ref{fig:rabinMillerDistSpeedup}, we reached a speedup of 213.4
with PArrrows at 256 cores (vs.~207.7 with pure Eden). Because of memory
limitations, the speedup of Jacobi sum test for large inputs (such as
\(2^{4253}-1\)) could be measured only in a massively distributed
setting. PArrows improved there from \(9193s\) (at 128 cores) to
\(1649s\) (at 256 cores). A scaled-down version with input
\(2^{3217}-1\) stagnates the speedup at about 11 for both PArrows and
Eden for more than 64 cores. There is apparently not enough work for
that many cores. The Gentleman test with input 4096 had an almost linear
speedup first, then plummeted between 128 and 224 cores, and recovered
at 256 cores with speedup of 129.

\begin{figure}
\centering
\includegraphics{src/img/rabinMillerDistSpeedup.pdf}
\caption{Speedup of the distributed Rabin--Miller benchmark using
PArrows with Eden.\label{fig:rabinMillerDistSpeedup}}
\end{figure}

\hypertarget{overhead-1}{%
\subsubsection{Overhead}\label{overhead-1}}

We use our mean overhead quality measure and the notion of significance
also for distributed memory benchmarks. The mean overhead of the
Rabin--Miller test in here ranges from \(0.29\%\) to \(-2.8\%\) (last
value in favour of PArrows), but these values are not significant with
error margins \(\pm 0.8\%\) and \(\pm 2.9\%\) correspondingly. A sole
significant (by a very low margin) overhead is \(0.35\% \; \pm 0.33\%\)
at 64 cores. We measured the mean overhead for Jacobi benchmark for an
input of \(2^{3217}-1\) for up to 256 cores. We reach the flattering
value \(-3.8\% \; \pm 0.93\%\) at 16 cores in favour of PArrows, it was
the sole significant overhead value. The value for 256 cores was
\(0.31\% \; \pm 0.39\%\). Mean overhead for distributed Gentleman
multiplication was also low. Significant values include
\(1.23\% \; \pm 1.20\%\) at 64 cores and \(2.4\% \; \pm 0.97\%\) at 256
cores. It took PArrows 64.2 seconds at 256 cores to complete the
benchmark.

Similar to the shared memory setting, PArrows only imply a very low
penalty with distributed memory that lies in lower single-percent digits
at most.

\newpage

\hypertarget{evaluation-of-results}{%
\section{Evaluation of results}\label{evaluation-of-results}}

\label{sec:benchmarksEvaluation}

PArrows performed in our benchmarks with little to no overhead. Tables
\ref{tbl:meanOverheadSharedMemory} and
\ref{tbl:meanOverHeadDistributedMemory} clarify this once more: The
PArrows-enabled versions trade blows with their vanilla counterparts
when comparing the means of the mean overheads over all different core
counts. If we combine these findings with the benefits of our DSL, the
minor overhead induced by PArrows is outweighed by their convenience and
usefulness to the user.

\begin{longtable}[]{@{}l@{}}
\caption{Overhead in the shared memory benchmarks. Bold marks values in
favour of PArrows. \label{tbl:meanOverheadSharedMemory}}\tabularnewline
\includegraphics{src/img/bestAndWorstBenchmarks1.pdf}\tabularnewline
\end{longtable}

\begin{longtable}[]{@{}l@{}}
\caption{Overhead in the distributed memory benchmarks. Bold marks
values in favour of PArrows.
\label{tbl:meanOverHeadDistributedMemory}}\tabularnewline
\includegraphics{src/img/bestAndWorstBenchmarks2.pdf}\tabularnewline
\end{longtable}

\hypertarget{outlook}{%
\chapter{Outlook}\label{outlook}}

\label{sec:outlook}

Finally, in this chapter we finish up this work on the PArrows DSL. In
Chapter \ref{sec:outlookContribs} we point our contributions. In Chapter
\ref{sec:outlookEval} we evaluate these on the basis of the goals we
have set ourselves in the Introduction. Chapter
\ref{sec:outlookConclusion} concludes the work and also points out
future work.

\hypertarget{contributions}{%
\section{Contributions}\label{contributions}}

\label{sec:outlookContribs}

In this thesis, we proposed an Arrow-based encoding for parallelism
based on a new Arrow combinator \ensuremath{\Varid{parEvalN}\mathbin{::}[\mskip1.5mu \Varid{arr}\;\Varid{a}\;\Varid{b}\mskip1.5mu]\to \Varid{arr}\;[\mskip1.5mu \Varid{a}\mskip1.5mu]\;[\mskip1.5mu \Varid{b}\mskip1.5mu]}.
A parallel Arrow is still an Arrow, hence the resulting parallel Arrow
can still be used in the same way as a potential sequential version. We
evaluated the expressive power of such a formalism in the context of
parallel programming.

One big advantage of this specific approach is that we do not have to
introduce any new types, facilitating composability (Chapter
\ref{sec:parallel-arrows}). These PArrow programs can readily exploit
multiple parallel language implementations. We demonstrated the use of
GpH, a \ensuremath{\Conid{Par}} Monad, and Eden. We did not re-implement all the parallel
internals, as this functionality is hosted in the \ensuremath{\Conid{ArrowParallel}} type
class, which abstracts all parallel implementation logic. The
implementations can easily be swapped, so we are not bound to any
specific one.

Next, we extended the PArrows formalism even further (Chapter
\ref{sec:further-development}). We used \ensuremath{\Conid{Future}}s to enable direct
communication of data between nodes in a distributed memory setting
similar to Eden's Remote Data (Dieterle et al.,
\protect\hyperlink{ref-Dieterle2010}{2010}\protect\hyperlink{ref-Dieterle2010}{b}).
Direct communication is useful in a distributed memory setting because
it allows for inter-node communication without blocking the master-node
(Chapter \ref{sec:futures}). Subsequently, we demonstrated the
expressiveness of PArrows by using them to define common algorithmic
skeletons (Chapters \ref{sec:skeletons}, \ref{sec:topology-skeletons}),
and by using these skeletons to implement four benchmarks (Chapter
\ref{sec:benchmarks}).

We also developed an experimental Cloud Haskell backend (Chapter
\ref{sec:cloudHaskellExperiment}) as a possible PArrows backend with
support for the recent trends in cloud computing. This in an early proof
of concept stage at the moment.

Finally, we practically demonstrated that Arrow parallelism has a low
performance overhead compared with existing approaches, with only some
negligible performance hits (Chapter \ref{sec:benchmarks}).

\hypertarget{evaluation-of-results-1}{%
\section{Evaluation of results}\label{evaluation-of-results-1}}

\label{sec:outlookEval}

In the Introduction, we have set ourselves goals for our DSL. We wanted:

\begin{itemize}
\tightlist
\item
  a DSL that allows us to parallelise arbitrary Arrow types
\item
  to tame the zoo of parallel Haskells.
\item
  low performance penalty
\item
  generality by being able to switch implementations at will
\end{itemize}

We will now go into detail on whether we have met these requirements.

\hypertarget{parallelizing-arbitrary-arrow-types}{%
\subsubsection{Parallelizing arbitrary Arrow
Types}\label{parallelizing-arbitrary-arrow-types}}

Our PArrows API generally succeeded in providing parallelism for
arbitrary Arrow types, but only those which fulfill certain type
classes. There are two restrictions to that, though:

\begin{itemize}
\item
  Most of our API as well as the actual instances for our type class
  \ensuremath{\Conid{ArrowParallel}} in the case of GpH and the \ensuremath{\Conid{Par}} Monad require an
  \ensuremath{\Conid{ArrowChoice}} instance. Additionally, we require \ensuremath{\Conid{ArrowLoop}} for the
  looping skeletons. While this indeed restricts the set of suitable
  Arrows, it is not a restriction in which Arrows can be parallelised
  specifically, but more of a restriction the type of Arrow.
  Furthermore, both type classes are an integral part in the code that
  uses them. For example, without \ensuremath{\Conid{ArrowChoice}} we could not express the
  recursion with Arrows as without it we can not express the recursion
  anchor in e.g.~the definition of \ensuremath{\Varid{evalN}} (Figure \ref{fig:evalN}). The
  same goes for \ensuremath{\Conid{ArrowLoop}} as it is required in order to have the
  know-tying fix-point semantics we require for our topology skeletons.
  In fact, Hughes (\protect\hyperlink{ref-Hughes2005}{2005}) mentions
  this as well in writing
  \enquote{there is little interesting that can be done without
  more operations on Arrows than just composition}.
\item
  The Eden backend currently has no general implementation for
  \ensuremath{\Conid{ArrowParallel}}. As explained earlier in Chapter
  \ref{sec:parrows-Eden}, the reason for this is that Eden's \ensuremath{\Varid{spawnF}}
  only works on functions (\ensuremath{(\to )}). Because of this, we resorted to
  having manual implementations of \ensuremath{\Conid{ArrowParallel}} for every type. As
  noted in the same chapter, however, this seems to be no real issue as
  a possible general implementation is possible. Nevertheless, this has
  to be evaluated with more involved tests in the future.
\end{itemize}

Summarizing, we can say that we mostly succeeded in our goal to provide
support for parallelizing arbitrary Arrows as these two restrictions are
no big issues at all as we have explained here.

\hypertarget{taming-the-zoo-of-parallel-haskells}{%
\subsubsection{Taming the zoo of parallel
Haskells}\label{taming-the-zoo-of-parallel-haskells}}

In this thesis, we showed how we can tame at least the three parallel
Haskells we used as backends -- GpH, the \ensuremath{\Conid{Par}} Monad and Eden. We even
included the blue print for a new backend based upon Cloud Haskell.
Therefore, we are confident that other parallel Haskells can be used as
backends in our DSL, even if they require some special care -- e.g.~like
we have talked about in the case of HdpH, which heavily relied on
Template Haskell to work. This Template Haskell code was however
incompatible with PArrows and would need replacing were HdpH used as a
backend. With the PArrows DSL we are able to tame the zoo of parallel
Haskells.

\hypertarget{low-penalty-interface}{%
\subsubsection{Low Penalty interface}\label{low-penalty-interface}}

As our benchmarks in Chapter \ref{sec:benchmarks} show, our PArrows DSL
did only induce very low overhead -- i.e.~never more than
\(8\% \; \pm 6.9\%\) and typically under \(2\%\). Typically, the mean
over all cores of relative mean overhead was less than \(3.5\%\) and
less than \(0.8\%\) for all benchmarks with GpH and Eden, respectively.
As for the \ensuremath{\Conid{Par}} Monad, the mean of mean overheads was in favour of
PArrows in all benchmarks (Chapter \ref{sec:benchmarks}). The PArrows
DSL is a very low penalty interface when compared to the native
backends.

\hypertarget{generality-switching-implementations-at-will}{%
\subsubsection{Generality -- Switching Implementations at
will}\label{generality-switching-implementations-at-will}}

Because of the way we designed our central \ensuremath{\Conid{ArrowParallel}} type class in
Chapter \ref{sec:parallel-arrows-type-class}, we can truly switch
between backend implementations at will. Therefore programs are portable
between parallel Haskell implementations. The only thing that has to be
done when switching between implementations, is changing the import
statement. Implementation specifics such as different config types are
well hidden away from the user with the help of default configuration
instances, but can be accessed if required. The only thing that is not
covered by our interface is the need for specific transport logic for
distributed backends for non standard data types (like Eden or in the
experimental Cloud Haskell backend). However, these are easily
implemented with default instances (\ensuremath{\Conid{Trans}} in Eden) or Template Haskell
(Cloud Haskell).

The only difficulty we currently have in terms of PArrows' generality is
that the implementations differ in their behaviour of \ensuremath{\Varid{parEvalN}}. The
GpH, \ensuremath{\Conid{Par}} Monad and the experimental Cloud Haskell versions of
\ensuremath{\Varid{parEvalN}} do not work in a manner that is compatible with the
topological skeletons we showed in this thesis. Even though we provided
a work-around for the sake of compatibility with the \ensuremath{\Conid{ArrowLoopParallel}}
type class, this can only be seen as temporary. This problem has to be
investigated further, and fixed by standardizing the way \ensuremath{\Varid{parEvalN}}
behaves.

\hypertarget{summary}{%
\subsubsection{Summary}\label{summary}}

Summarizing we can say that we have fulfilled the requirements set in
the Introduction of this thesis. PArrows is a DSL that allows us to
parallelize arbitrary Arrow types that manages to tame the zoo of
parallel Haskells while having a low performance penalty and is general
by permitting to switch implementations at will. As described, there are
some problems, however, these will have to be amended in the future.

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

\label{sec:outlookConclusion}

Arrows are a generic concept that allows for powerful composition
combinators. To our knowledge, we are first to represent \emph{parallel}
computation with Arrows, and hence to show their usefulness for
composing parallel programs.

In our opinion, a way to parallelise Arrows directly with a DSL like
ours was overdue. They serve as a general interface to computation, so
why not also to \emph{parallel} computation? Equally importantly, the
DSL can serve as a interface for parallelism in general. For years, the
maintainers of the different APIs have been developing their own brand
of parallelism without caring about portability, which is a big concern
if an application has to be ported to another eco-system. This is
definitely not only a theoretical argument. In the real world, projects
can become deprecated or cause other incompatibilities -- the same goes
for parallel APIs. Also, sometimes parallel programs that were written
to be run on a single machine have to be ported to a distributed
environment. Now, if a program that is to be ported were based on our
\emph{shallow} DSL, these problems would basically be non-existent.
Programmers could simply change the backend and continue their work in
other areas that matter.

Even for programmers that do not care for the portability between APIs
in their programs the PArrows DSL can be of benefit because of the
generality of our approach. Because we use Arrows to build our DSL, we
achieve a common way of parallelising computations whether they are
simple (\ensuremath{\to }) or monadic functions (\ensuremath{\Varid{a}\to \Varid{m}\;\Varid{b}}) -- or yet another
different computation type. We can even say that PArrows is not only a
general way to paralellise Arrow-based computations, but also a general
way to parallelise \emph{computations} in general.

\hypertarget{future-work}{%
\subsubsection{Future work}\label{future-work}}

\label{sec:future-work}

Our PArrows DSL can be expanded to other task parallel Haskells as we
have seen in the Cloud Haskell experiment. It is a primary focus of
further development as it will also help investigating the biggest
problem of our DSL -- the difference in behaviour of \ensuremath{\Varid{parEvalN}} across
the backends. In Chapter
\ref{sec:CloudHaskellArrowParallelLimitsMitigation}, we already proposed
a possible fix for the Cloud Haskell backend. Fixing this specifically
for Cloud Haskell will help us in understanding the problem and enable
us to amend the GpH and \ensuremath{\Conid{Par}} Monad backends as well.

In other future work, we see a big potential in getting HdpH (Maier et
al., \protect\hyperlink{ref-Maier:2014:HDS:2775050.2633363}{2014}) to
work with our interface. Furthermore, additional Future-aware versions
of Arrow combinators can be defined. Existing combinators could also be
improved, for example more specialised versions of \ensuremath{\mathbin{>\!\!>\!\!>}} and \ensuremath{\mathbin{*\!*\!*}}
combinators are viable.

Another area of interest is the expansion expanding of both our skeleton
library as well as the number of skeleton-based parallel programs that
use our DSL. It would also be interesting to see a hybrid of PArrows and
Accelerate (McDonell et al.,
\protect\hyperlink{ref-McDonell:2015:TRC:2887747.2804313}{2015}).

\appendix

\hypertarget{appendix}{%
\chapter{Appendix}\label{appendix}}

Following are additional chapters with supplementary information for
this thesis. Next, Chapter \ref{app:profunctorArrows} explains how
specific Profunctors fit the Arrow type class. Chapter \ref{app:omitted}
covers omitted function definitions. Then, Chapter \ref{syntacticSugar}
explains syntactic sugar for PArrows. We give additional definitions for
the experimental Cloud Haskell backend in Chapter
\ref{sec:appendixCloudHaskell} and finish with the plots for the shared
memory backends and distributed memory backends in Chapters
\ref{sec:benchmarkSharedPlots} and \ref{sec:benchmarkDistPlots},
respectively.

\hypertarget{profunctor-arrows}{%
\section{Profunctor Arrows}\label{profunctor-arrows}}

\label{app:profunctorArrows}

In Figure \ref{fig:profunctorArrow} we show how specific Profunctors can
fit into the Arrow type class. This works because Arrows are strong
Monads in the bicategory \ensuremath{\Conid{Prof}} of Profunctors as shown by Asada
(\protect\hyperlink{ref-Asada:2010:ASM:1863597.1863607}{2010}). In
Standard GHC \ensuremath{(\mathbin{>\!\!>\!\!>})} has the type
\ensuremath{(\mathbin{>\!\!>\!\!>})\mathbin{::}\Conid{Category}\;\Varid{cat}\Rightarrow \Varid{cat}\;\Varid{a}\;\Varid{b}\to \Varid{cat}\;\Varid{b}\;\Varid{c}\to \Varid{cat}\;\Varid{a}\;\Varid{c}} and is
therefore not part of the \ensuremath{\Conid{Arrow}} type class like presented in this
thesis.\footnote{For additional information on the type classes used,
  see:
  \url{https://hackage.haskell.org/package/profunctors-5.2.1/docs/Data-Profunctor.html}
  and
  \url{https://hackage.haskell.org/package/base-4.9.1.0/docs/Control-Category.html}.}

\begin{figure}[h]
\centering
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{instance}\;(\Conid{Category}\;\Varid{p},{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Conid{Strong}\;\Varid{p})\Rightarrow {}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Conid{Arrow}\;\Varid{p}\;\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Varid{arr}\;\Varid{f}\mathrel{=}\Varid{dimap}\;\Varid{id}\;\Varid{f}\;\Varid{id}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Varid{first}\mathrel{=}\Varid{first'}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\mathbf{instance}\;(\Conid{Category}\;\Varid{p},{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Conid{Strong}\;\Varid{p},{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Conid{Costrong}\;\Varid{p})\Rightarrow {}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Conid{ArrowLoop}\;\Varid{p}\;\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Varid{loop}\mathrel{=}\Varid{loop'}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\mathbf{instance}\;(\Conid{Category}\;\Varid{p},{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Conid{Strong}\;\Varid{p},{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Conid{Choice}\;\Varid{p})\Rightarrow {}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Conid{ArrowChoice}\;\Varid{p}\;\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Varid{left}\mathrel{=}\Varid{left'}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\caption{Profunctors as Arrows.}\label{fig:profunctorArrow}\end{figure}

\hypertarget{additional-function-definitions}{%
\section{Additional function
definitions}\label{additional-function-definitions}}

\label{app:omitted}

We have omitted some function definitions in the main text for brevity,
and redeem this here.

We begin with Arrow versions of Eden's \ensuremath{\Varid{shuffle}}, \ensuremath{\Varid{unshuffle}} and the
definition of \ensuremath{\Varid{takeEach}} can be found in Figure
\ref{fig:edenshuffleetc}. Similarly, Figure
\ref{fig:edenlazyrightrotate} contains the definition of Arrow versions
of Eden's \ensuremath{\Varid{lazy}} and \ensuremath{\Varid{rightRotate}} utility functions. Figure
\ref{fig:lazyzip3etc} contains Eden's definition of \ensuremath{\Varid{lazyzip3}} together
with the utility functions \ensuremath{\Varid{uncurry3}} and \ensuremath{\Varid{threetotwo}}. The full
definition of \ensuremath{\Varid{farmChunk}} is in Figure \ref{fig:farmChunk}. Eden
definition of \ensuremath{\Varid{ring}} skeleton is in Figure \ref{fig:ringEden}. It
follows Loogen (\protect\hyperlink{ref-Loogen2012}{2012}).

\begin{figure}[t]
\centering
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{shuffle}\mathbin{::}(\Conid{Arrow}\;\Varid{arr})\Rightarrow \Varid{arr}\;[\mskip1.5mu [\mskip1.5mu \Varid{a}\mskip1.5mu]\mskip1.5mu]\;[\mskip1.5mu \Varid{a}\mskip1.5mu]{}\<[E]%
\\
\>[B]{}\Varid{shuffle}\mathrel{=}\Varid{arr}\;(\Varid{concat}\mathbin{\circ}\Varid{transpose}){}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{unshuffle}\mathbin{::}(\Conid{Arrow}\;\Varid{arr})\Rightarrow \Conid{Int}\to \Varid{arr}\;[\mskip1.5mu \Varid{a}\mskip1.5mu]\;[\mskip1.5mu [\mskip1.5mu \Varid{a}\mskip1.5mu]\mskip1.5mu]{}\<[E]%
\\
\>[B]{}\Varid{unshuffle}\;\Varid{n}\mathrel{=}\Varid{arr}\;(\lambda \Varid{xs}\to [\mskip1.5mu \Varid{takeEach}\;\Varid{n}\;(\Varid{drop}\;\Varid{i}\;\Varid{xs})\mid \Varid{i}\leftarrow [\mskip1.5mu \mathrm{0}\mathinner{\ldotp\ldotp}\Varid{n}\mathbin{-}\mathrm{1}\mskip1.5mu]\mskip1.5mu]){}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{takeEach}\mathbin{::}\Conid{Int}\to [\mskip1.5mu \Varid{a}\mskip1.5mu]\to [\mskip1.5mu \Varid{a}\mskip1.5mu]{}\<[E]%
\\
\>[B]{}\Varid{takeEach}\;\Varid{n}\;[\mskip1.5mu \mskip1.5mu]\mathrel{=}[\mskip1.5mu \mskip1.5mu]{}\<[E]%
\\
\>[B]{}\Varid{takeEach}\;\Varid{n}\;(\Varid{x}\mathbin{:}\Varid{xs})\mathrel{=}\Varid{x}\mathbin{:}\Varid{takeEach}\;\Varid{n}\;(\Varid{drop}\;(\Varid{n}\mathbin{-}\mathrm{1})\;\Varid{xs}){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\caption{\ensuremath{\Varid{shuffle}}, \ensuremath{\Varid{unshuffle}}, \ensuremath{\Varid{takeEach}} definition.}\label{fig:edenshuffleetc}\end{figure}

\begin{figure}[t]
\centering
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{lazy}\mathbin{::}(\Conid{Arrow}\;\Varid{arr})\Rightarrow \Varid{arr}\;[\mskip1.5mu \Varid{a}\mskip1.5mu]\;[\mskip1.5mu \Varid{a}\mskip1.5mu]{}\<[E]%
\\
\>[B]{}\Varid{lazy}\mathrel{=}\Varid{arr}\;(\lambda \mathord{\sim}(\Varid{x}\mathbin{:}\Varid{xs})\to \Varid{x}\mathbin{:}\Varid{lazy}\;\Varid{xs}){}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{rightRotate}\mathbin{::}(\Conid{Arrow}\;\Varid{arr})\Rightarrow \Varid{arr}\;[\mskip1.5mu \Varid{a}\mskip1.5mu]\;[\mskip1.5mu \Varid{a}\mskip1.5mu]{}\<[E]%
\\
\>[B]{}\Varid{rightRotate}\mathrel{=}\Varid{arr}\mathbin{\$}\lambda \Varid{list}\to \mathbf{case}\;\Varid{list}\;\mathbf{of}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}[\mskip1.5mu \mskip1.5mu]\to [\mskip1.5mu \mskip1.5mu]{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Varid{xs}\to \Varid{last}\;\Varid{xs}\mathbin{:}\Varid{init}\;\Varid{xs}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\caption{\ensuremath{\Varid{lazy}} and \ensuremath{\Varid{rightRotate}} definitions.}\label{fig:edenlazyrightrotate}\end{figure}

\begin{figure}[t]
\centering
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{lazyzip3}\mathbin{::}[\mskip1.5mu \Varid{a}\mskip1.5mu]\to [\mskip1.5mu \Varid{b}\mskip1.5mu]\to [\mskip1.5mu \Varid{c}\mskip1.5mu]\to [\mskip1.5mu (\Varid{a},\Varid{b},\Varid{c})\mskip1.5mu]{}\<[E]%
\\
\>[B]{}\Varid{lazyzip3}\;\Varid{as}\;\Varid{bs}\;\Varid{cs}\mathrel{=}\Varid{zip3}\;\Varid{as}\;(\Varid{lazy}\;\Varid{bs})\;(\Varid{lazy}\;\Varid{cs}){}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{uncurry3}\mathbin{::}(\Varid{a}\to \Varid{b}\to \Varid{c}\to \Varid{d})\to (\Varid{a},(\Varid{b},\Varid{c}))\to \Varid{d}{}\<[E]%
\\
\>[B]{}\Varid{uncurry3}\;\Varid{f}\;(\Varid{a},(\Varid{b},\Varid{c}))\mathrel{=}\Varid{f}\;\Varid{a}\;\Varid{b}\;\Varid{c}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{threetotwo}\mathbin{::}(\Conid{Arrow}\;\Varid{arr})\Rightarrow \Varid{arr}\;(\Varid{a},\Varid{b},\Varid{c})\;(\Varid{a},(\Varid{b},\Varid{c})){}\<[E]%
\\
\>[B]{}\Varid{threetotwo}\mathrel{=}\Varid{arr}\mathbin{\$}\lambda \mathord{\sim}(\Varid{a},\Varid{b},\Varid{c})\to (\Varid{a},(\Varid{b},\Varid{c})){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\caption{\ensuremath{\Varid{lazyzip3}}, \ensuremath{\Varid{uncurry3}} and \ensuremath{\Varid{threetotwo}} definitions.}\label{fig:lazyzip3etc}\end{figure}

\begin{figure}[t]
\centering
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{7}{@{}>{\hspre}l<{\hspost}@{}}%
\column{9}{@{}>{\hspre}l<{\hspost}@{}}%
\column{13}{@{}>{\hspre}l<{\hspost}@{}}%
\column{16}{@{}>{\hspre}l<{\hspost}@{}}%
\column{19}{@{}>{\hspre}l<{\hspost}@{}}%
\column{20}{@{}>{\hspre}l<{\hspost}@{}}%
\column{22}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{ringSimple}\mathbin{::}(\Conid{Trans}\;\Varid{i},\Conid{Trans}\;\Varid{o},\Conid{Trans}\;\Varid{r})\Rightarrow {}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}(\Varid{i}\to \Varid{r}\to (\Varid{o},\Varid{r}))\to [\mskip1.5mu \Varid{i}\mskip1.5mu]\to [\mskip1.5mu \Varid{o}\mskip1.5mu]{}\<[E]%
\\
\>[B]{}\Varid{ringSimple}\;\Varid{f}\;\Varid{is}\mathrel{=}{}\<[20]%
\>[20]{}\Varid{os}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{where}\;(\Varid{os},\Varid{ringOuts})\mathrel{=}\Varid{unzip}\;(\Varid{parMap}{}\<[E]%
\\
\>[3]{}\hsindent{10}{}\<[13]%
\>[13]{}(\Varid{toRD}\mathbin{\$}\Varid{uncurry}\;\Varid{f})\;(\Varid{zip}\;\Varid{is}\mathbin{\$}\Varid{lazy}\;\Varid{ringIns})){}\<[E]%
\\
\>[3]{}\hsindent{6}{}\<[9]%
\>[9]{}\Varid{ringIns}\mathrel{=}\Varid{rightRotate}\;\Varid{ringOuts}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{toRD}\mathbin{::}(\Conid{Trans}\;\Varid{i},\Conid{Trans}\;\Varid{o},\Conid{Trans}\;\Varid{r})\Rightarrow {}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}((\Varid{i},\Varid{r})\to (\Varid{o},\Varid{r}))\to ((\Varid{i},\Conid{RD}\;\Varid{r})\to (\Varid{o},\Conid{RD}\;\Varid{r})){}\<[E]%
\\
\>[B]{}\Varid{toRD}\;{}\<[7]%
\>[7]{}\Varid{f}\;(\Varid{i},\Varid{ringIn}){}\<[22]%
\>[22]{}\mathrel{=}(\Varid{o},\Varid{release}\;\Varid{ringOut}){}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{where}\;(\Varid{o},\Varid{ringOut})\mathrel{=}\Varid{f}\;(\Varid{i},\Varid{fetch}\;\Varid{ringIn}){}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{rightRotate}{}\<[16]%
\>[16]{}\mathbin{::}[\mskip1.5mu \Varid{a}\mskip1.5mu]\to [\mskip1.5mu \Varid{a}\mskip1.5mu]{}\<[E]%
\\
\>[B]{}\Varid{rightRotate}\;[\mskip1.5mu \mskip1.5mu]\mathrel{=}{}\<[19]%
\>[19]{}[\mskip1.5mu \mskip1.5mu]{}\<[E]%
\\
\>[B]{}\Varid{rightRotate}\;\Varid{xs}\mathrel{=}{}\<[19]%
\>[19]{}\Varid{last}\;\Varid{xs}\mathbin{:}\Varid{init}\;\Varid{xs}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{lazy}\mathbin{::}[\mskip1.5mu \Varid{a}\mskip1.5mu]\to [\mskip1.5mu \Varid{a}\mskip1.5mu]{}\<[E]%
\\
\>[B]{}\Varid{lazy}\mathord{\sim}(\Varid{x}\mathbin{:}\Varid{xs})\mathrel{=}\Varid{x}\mathbin{:}\Varid{lazy}\;\Varid{xs}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\caption{Eden's definition of the \ensuremath{\Varid{ring}} skeleton.}\label{fig:ringEden}\end{figure}

Furthermore, Figure \ref{fig:torus_example_rest} contains the omitted
definitions required for parallel matrix multiplication with the \ensuremath{\Varid{torus}}
skeleton. They are: \ensuremath{\Varid{prMM}} (sequential matrix multiplication),
\ensuremath{\Varid{splitMatrix}} (which splits a matrix into chunks), \ensuremath{\Varid{staggerHorizontally}}
and \ensuremath{\Varid{staggerVertically}} (to pre-rotate the matrices), and lastly
\ensuremath{\Varid{matAdd}}, which calculates \(A + B\) for two matrices \(A\) and \(B\).

\begin{figure}[t]
\centering
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{9}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{prMM}\mathbin{::}\Conid{Matrix}\to \Conid{Matrix}\to \Conid{Matrix}{}\<[E]%
\\
\>[B]{}\Varid{prMM}\;\Varid{m1}\;\Varid{m2}\mathrel{=}\Varid{prMMTr}\;\Varid{m1}\;(\Varid{transpose}\;\Varid{m2}){}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{where}{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}\Varid{prMMTr}\;\Varid{m1'}\;\Varid{m2'}\mathrel{=}{}\<[E]%
\\
\>[5]{}\hsindent{4}{}\<[9]%
\>[9]{}[\mskip1.5mu [\mskip1.5mu \Varid{sum}\;(\Varid{zipWith}\;(\mathbin{*})\;\Varid{row}\;\Varid{col})\mid \Varid{col}\leftarrow \Varid{m2'}\mskip1.5mu]\mid \Varid{row}\leftarrow \Varid{m1'}\mskip1.5mu]{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{splitMatrix}\mathbin{::}\Conid{Int}\to \Conid{Matrix}\to [\mskip1.5mu [\mskip1.5mu \Conid{Matrix}\mskip1.5mu]\mskip1.5mu]{}\<[E]%
\\
\>[B]{}\Varid{splitMatrix}\;\Varid{size}\;\Varid{matrix}\mathrel{=}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{map}\;(\Varid{transpose}\mathbin{\circ}\Varid{map}\;(\Varid{chunksOf}\;\Varid{size}))\mathbin{\$}\Varid{chunksOf}\;\Varid{size}\mathbin{\$}\Varid{matrix}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{staggerHorizontally}\mathbin{::}[\mskip1.5mu [\mskip1.5mu \Varid{a}\mskip1.5mu]\mskip1.5mu]\to [\mskip1.5mu [\mskip1.5mu \Varid{a}\mskip1.5mu]\mskip1.5mu]{}\<[E]%
\\
\>[B]{}\Varid{staggerHorizontally}\;\Varid{matrix}\mathrel{=}\Varid{zipWith}\;\Varid{leftRotate}\;[\mskip1.5mu \mathrm{0}\mathinner{\ldotp\ldotp}\mskip1.5mu]\;\Varid{matrix}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{staggerVertically}\mathbin{::}[\mskip1.5mu [\mskip1.5mu \Varid{a}\mskip1.5mu]\mskip1.5mu]\to [\mskip1.5mu [\mskip1.5mu \Varid{a}\mskip1.5mu]\mskip1.5mu]{}\<[E]%
\\
\>[B]{}\Varid{staggerVertically}\;\Varid{matrix}\mathrel{=}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{transpose}\mathbin{\$}\Varid{zipWith}\;\Varid{leftRotate}\;[\mskip1.5mu \mathrm{0}\mathinner{\ldotp\ldotp}\mskip1.5mu]\;(\Varid{transpose}\;\Varid{matrix}){}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{leftRotate}\mathbin{::}\Conid{Int}\to [\mskip1.5mu \Varid{a}\mskip1.5mu]\to [\mskip1.5mu \Varid{a}\mskip1.5mu]{}\<[E]%
\\
\>[B]{}\Varid{leftRotate}\;\Varid{i}\;\Varid{xs}\mathrel{=}\Varid{xs2}\plus \Varid{xs1}\;\mathbf{where}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}(\Varid{xs1},\Varid{xs2})\mathrel{=}\Varid{splitAt}\;\Varid{i}\;\Varid{xs}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{matAdd}\mathrel{=}\Varid{chunksOf}\;(\Varid{dimX}\;\Varid{x})\mathbin{\$}\Varid{zipWith}\;(\mathbin{+})\;(\Varid{concat}\;\Varid{x})\;(\Varid{concat}\;\Varid{y}){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\caption{\ensuremath{\Varid{prMMTr}}, \ensuremath{\Varid{splitMatrix}}, \ensuremath{\Varid{staggerHorizontally}}, \ensuremath{\Varid{staggerVertically}} and \ensuremath{\Varid{matAdd}} definition.}\label{fig:torus_example_rest}\end{figure}

\hypertarget{syntactic-sugar}{%
\section{Syntactic sugar}\label{syntactic-sugar}}

\label{syntacticSugar}

Next, we also give the definitions for some syntactic sugar for PArrows,
namely \ensuremath{\mathbin{\mid\!\!*\!*\!*\!\!\mid}} and \ensuremath{\mathbin{\mid\!\!\&\!\&\!\&\!\!\mid}}.

For basic Arrows, we have the \ensuremath{\mathbin{*\!*\!*}} combinator. (Figure
\ref{fig:syntacticSugarArrows}) It allows us to combine two Arrows
\ensuremath{\Varid{arr}\;\Varid{a}\;\Varid{b}} and \ensuremath{\Varid{arr}\;\Varid{c}\;\Varid{d}} into an Arrow \ensuremath{\Varid{arr}\;(\Varid{a},\Varid{c})\;(\Varid{b},\Varid{d})} which does
both computations at once. This can easily be translated into a parallel
version \ensuremath{\mathbin{\mid\!\!*\!*\!*\!\!\mid}} with the use of \ensuremath{\Varid{parEval2}}, but for this we require a
backend which has an implementation that does not require any
configuration:\footnote{Hence the \ensuremath{()} as the \ensuremath{\Varid{conf}} parameter.}


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}(\mathbin{\mid\!\!*\!*\!*\!\!\mid})\mathbin{::}(\Conid{ArrowChoice}\;\Varid{arr},\Conid{ArrowParallel}\;\Varid{arr}\;(\Conid{Either}\;\Varid{a}\;\Varid{c})\;(\Conid{Either}\;\Varid{b}\;\Varid{d})\;()))\Rightarrow {}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{arr}\;\Varid{a}\;\Varid{b}\to \Varid{arr}\;\Varid{c}\;\Varid{d}\to \Varid{arr}\;(\Varid{a},\Varid{c})\;(\Varid{b},\Varid{d}){}\<[E]%
\\
\>[B]{}(\mathbin{\mid\!\!*\!*\!*\!\!\mid})\mathrel{=}\Varid{parEval2}\;(){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

We define the parallel \ensuremath{\mathbin{\mid\!\!\&\!\&\!\&\!\!\mid}} in a similar manner to its sequential
pendant \ensuremath{\mathbin{\&\!\&\!\&}} (Figure \ref{fig:syntacticSugarArrows}):


\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}(\mathbin{\mid\!\!\&\!\&\!\&\!\!\mid})\mathbin{::}(\Conid{ArrowChoice}\;\Varid{arr},\Conid{ArrowParallel}\;\Varid{arr}\;(\Conid{Either}\;\Varid{a}\;\Varid{a})\;(\Conid{Either}\;\Varid{b}\;\Varid{c})\;())\Rightarrow {}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{arr}\;\Varid{a}\;\Varid{b}\to \Varid{arr}\;\Varid{a}\;\Varid{c}\to \Varid{arr}\;\Varid{a}\;(\Varid{b},\Varid{c}){}\<[E]%
\\
\>[B]{}(\mathbin{\mid\!\!\&\!\&\!\&\!\!\mid})\;\Varid{f}\;\Varid{g}\mathrel{=}(\Varid{arr}\mathbin{\$}\lambda \Varid{a}\to (\Varid{a},\Varid{a}))\mathbin{>\!\!>\!\!>}\Varid{f}\mathbin{\mid\!\!*\!*\!*\!\!\mid}\Varid{g}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\vspace{-2\baselineskip}

\newpage

\hypertarget{experimental-cloud-haskell-backend-code}{%
\section{Experimental Cloud Haskell backend
code}\label{experimental-cloud-haskell-backend-code}}

\label{sec:appendixCloudHaskell}

Finally, we include the Template Haskell based code generator that makes
the experimental Cloud Haskell backend easier to use. As an example, we
also give a version of the main Sudoku benchmark program.

The code generator can be found in Figure \ref{fig:evalGen}. Here, if we
enclose this in a Haskell module, the functions \ensuremath{\Varid{mkEvalTasks}} (to
generate the \ensuremath{\Varid{evalTask}}s for the specific types), \ensuremath{\Varid{mkRemotables}} (to
mark the evaluation tasks as remotable in Cloud Haskell) and
\ensuremath{\Varid{mkEvaluatables}} (to create the \ensuremath{\Conid{Evaluatable}} instance) are the ones
exposed to the user.

\begin{figure}[th]
\centering
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{9}{@{}>{\hspre}l<{\hspost}@{}}%
\column{13}{@{}>{\hspre}l<{\hspost}@{}}%
\column{17}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\Varid{nested}\mathbin{::}\Conid{Type}\to \Conid{Type}\to \Conid{Type}{}\<[E]%
\\
\>[B]{}\Varid{nested}\;\Varid{a}\;\Varid{b}\mathrel{=}\Varid{a}\mathbin{`\Conid{AppT}`}(\Conid{ParensT}\;\Varid{b}){}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{tuple2}\mathbin{::}\Conid{Type}\to \Conid{Type}\to \Conid{Type}{}\<[E]%
\\
\>[B]{}\Varid{tuple2}\;\Varid{a}\;\Varid{b}\mathrel{=}(\Conid{TupleT}\;\mathrm{2}\mathbin{`\Conid{AppT}`}\Varid{a})\mathbin{`\Conid{AppT}`}\Varid{b}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{fn}\mathbin{::}\Conid{Type}\to \Conid{Type}\to \Conid{Type}{}\<[E]%
\\
\>[B]{}\Varid{fn}\;\Varid{a}\;\Varid{b}\mathrel{=}(\Conid{ArrowT}\mathbin{`\Conid{AppT}`}\Varid{a})\mathbin{`\Conid{AppT}`}\Varid{b}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{nameToFnName}\mathbin{::}\Conid{Name}\to \Conid{Name}{}\<[E]%
\\
\>[B]{}\Varid{nameToFnName}\;(\Conid{Name}\;(\Conid{OccName}\;\Varid{str})\;\anonymous )\mathrel{=}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{mkName}\mathbin{\$}(\text{\tt \char34 \char95 \char95 \char34}\plus \Varid{str}\plus \text{\tt \char34 \char95 evalTaskImpl\char34}){}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{evalTaskFn}\mathbin{::}\Conid{Name}\to \Conid{Name}\to \Conid{Q}\;[\mskip1.5mu \Conid{Dec}\mskip1.5mu]{}\<[E]%
\\
\>[B]{}\Varid{evalTaskFn}\;\Varid{typeName}\;\Varid{fnName}\mathrel{=}\mathbf{do}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\mathbf{let}\;\Varid{sendPort}\mathrel{=}\Conid{ConT}\mathbin{\$}\Varid{mkName}\;\text{\tt \char34 SendPort\char34}{}\<[E]%
\\
\>[5]{}\hsindent{4}{}\<[9]%
\>[9]{}\Varid{thunk}\mathrel{=}\Conid{ConT}\mathbin{\$}\Varid{mkName}\;\text{\tt \char34 Thunk\char34}{}\<[E]%
\\
\>[5]{}\hsindent{4}{}\<[9]%
\>[9]{}\Varid{process}\mathrel{=}\Conid{ConT}\mathbin{\$}\Varid{mkName}\;\text{\tt \char34 Process\char34}{}\<[E]%
\\
\>[5]{}\hsindent{4}{}\<[9]%
\>[9]{}\Varid{firstTup}\mathrel{=}(\Varid{sendPort}\mathbin{`\Varid{nested}`}(\Varid{sendPort}\mathbin{`\Varid{nested}`}{}\<[E]%
\\
\>[9]{}\hsindent{4}{}\<[13]%
\>[13]{}(\Varid{thunk}\mathbin{`\Varid{nested}`}(\Conid{ConT}\;\Varid{typeName})))){}\<[E]%
\\
\>[5]{}\hsindent{4}{}\<[9]%
\>[9]{}\Varid{secondTup}\mathrel{=}\Varid{sendPort}\mathbin{`\Varid{nested}`}(\Conid{ConT}\;\Varid{typeName}){}\<[E]%
\\
\>[5]{}\hsindent{4}{}\<[9]%
\>[9]{}\Varid{procNil}\mathrel{=}\Varid{process}\mathbin{`\Conid{AppT}`}(\Conid{TupleT}\;\mathrm{0}){}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{return}\;[\mskip1.5mu {}\<[E]%
\\
\>[5]{}\hsindent{8}{}\<[13]%
\>[13]{}\Conid{SigD}\;\Varid{fnName}\;((\Varid{firstTup}\mathbin{`\Varid{tuple2}`}\Varid{secondTup})\mathbin{`\Varid{fn}`}\Varid{procNil}),{}\<[E]%
\\
\>[5]{}\hsindent{8}{}\<[13]%
\>[13]{}\Conid{FunD}\;\Varid{fnName}\;{}\<[E]%
\\
\>[13]{}\hsindent{4}{}\<[17]%
\>[17]{}[\mskip1.5mu \Conid{Clause}\;[\mskip1.5mu \mskip1.5mu]\;(\Conid{NormalB}\;(\Conid{VarE}\mathbin{\$}\Varid{mkName}\;\text{\tt \char34 evalTaskBase\char34}))\;[\mskip1.5mu \mskip1.5mu]\mskip1.5mu]{}\<[E]%
\\
\>[5]{}\hsindent{4}{}\<[9]%
\>[9]{}\mskip1.5mu]{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{evaluatableInstance}\mathbin{::}\Conid{Name}\to \Conid{Name}\to \Conid{Q}\;[\mskip1.5mu \Conid{Dec}\mskip1.5mu]{}\<[E]%
\\
\>[B]{}\Varid{evaluatableInstance}\;\Varid{typeName}\;\Varid{fnName}\mathrel{=}\mathbf{do}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\mathbf{let}\;\Varid{evaluatable}\mathrel{=}\Conid{ConT}\mathbin{\$}\Varid{mkName}\;\text{\tt \char34 Evaluatable\char34}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{closure}\leftarrow \Varid{mkClosure}\;\Varid{fnName}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{return}\;[\mskip1.5mu {}\<[E]%
\\
\>[5]{}\hsindent{8}{}\<[13]%
\>[13]{}\Conid{InstanceD}\;(\Conid{Nothing})\;[\mskip1.5mu \mskip1.5mu]\;(\Varid{evaluatable}\mathbin{`\Varid{nested}`}\Conid{ConT}\;\Varid{typeName})\;[\mskip1.5mu {}\<[E]%
\\
\>[13]{}\hsindent{4}{}\<[17]%
\>[17]{}\Conid{FunD}\;(\Varid{mkName}\;\text{\tt \char34 evalTask\char34})\;[\mskip1.5mu \Conid{Clause}\;[\mskip1.5mu \mskip1.5mu]\;(\Conid{NormalB}\;\Varid{closure})\;[\mskip1.5mu \mskip1.5mu]\mskip1.5mu]{}\<[E]%
\\
\>[5]{}\hsindent{8}{}\<[13]%
\>[13]{}\mskip1.5mu]{}\<[E]%
\\
\>[5]{}\hsindent{4}{}\<[9]%
\>[9]{}\mskip1.5mu]{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{mkEvalTasks}\mathbin{::}[\mskip1.5mu \Conid{Name}\mskip1.5mu]\to \Conid{Q}\;[\mskip1.5mu \Conid{Dec}\mskip1.5mu]{}\<[E]%
\\
\>[B]{}\Varid{mkEvalTasks}\;\Varid{names}\mathrel{=}\mathbf{do}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\mathbf{let}\;\Varid{fnNames}\mathrel{=}\Varid{map}\;\Varid{nameToFnName}\;\Varid{names}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}(\Varid{mapM}\;(\Varid{uncurry}\;\Varid{evalTaskFn})\;{}\<[E]%
\\
\>[5]{}\hsindent{4}{}\<[9]%
\>[9]{}(\Varid{zipWith}\;(,)\;\Varid{names}\;\Varid{fnNames}))\bind (\Varid{return}\mathbin{\circ}\Varid{concat}){}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{mkRemotables}\mathbin{::}[\mskip1.5mu \Conid{Name}\mskip1.5mu]\to \Conid{Q}\;[\mskip1.5mu \Conid{Dec}\mskip1.5mu]{}\<[E]%
\\
\>[B]{}\Varid{mkRemotables}\;\Varid{names}\mathrel{=}\mathbf{do}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\mathbf{let}\;\Varid{fnNames}\mathrel{=}\Varid{map}\;\Varid{nameToFnName}\;\Varid{names}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\Varid{remotable}\;\Varid{fnNames}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{mkEvaluatables}\mathbin{::}[\mskip1.5mu \Conid{Name}\mskip1.5mu]\to \Conid{Q}\;[\mskip1.5mu \Conid{Dec}\mskip1.5mu]{}\<[E]%
\\
\>[B]{}\Varid{mkEvaluatables}\;\Varid{names}\mathrel{=}\mathbf{do}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}\mathbf{let}\;\Varid{fnNames}\mathrel{=}\Varid{map}\;\Varid{nameToFnName}\;\Varid{names}{}\<[E]%
\\
\>[B]{}\hsindent{5}{}\<[5]%
\>[5]{}(\Varid{mapM}\;(\Varid{uncurry}\;\Varid{evaluatableInstance})\;{}\<[E]%
\\
\>[5]{}\hsindent{4}{}\<[9]%
\>[9]{}(\Varid{zipWith}\;(,)\;\Varid{names}\;\Varid{fnNames}))\bind (\Varid{return}\mathbin{\circ}\Varid{concat}){}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\caption{The Template Haskell code generator for the Cloud Haskell backend.}\label{fig:evalGen}\end{figure}

A version of the main Sudoku benchmark program that uses this can be
found in Figure \ref{fig:sudokuCloudHaskell}\footnote{For the full code,
  see the GitHub repository at
  \url{https://github.com/s4ke/Parrows/blob/e1ab76018448d9d4ca3ed48ef1f0c5be26ae34ab/CloudHaskell/testing/Test.hs}.}.
We have to write type aliases for \ensuremath{\Conid{Maybe}\;\Conid{Grid}} (\ensuremath{\Conid{MaybeGrid}}) and
\ensuremath{[\mskip1.5mu \Conid{Maybe}\;\Conid{Grid}\mskip1.5mu]} (\ensuremath{\Conid{MaybeGridList}}). We can then use these to generate the
code required to to evaluate these types in the Cloud Haskell backend
with. In the \ensuremath{\Varid{main}} program we have two cases: a) the program is started
in master mode and starts the computation, b) the program is started in
slave mode and waits for computation requests. In order to launch this
program and have speedup as well, we have to start slave nodes for each
cpu core with commands like \enquote{<executable> slave 127.0.0.1 8000}
where the last parameter determines the port the slave will listen to
and wait for requests on. Similarly, a single master node can be started
with \enquote{<executable> master 127.0.0.1 7999} where, once again, the
last parameter determines the communication port.

\begin{figure}[th]
\centering
\begin{hscode}\SaveRestoreHook
\column{B}{@{}>{\hspre}l<{\hspost}@{}}%
\column{3}{@{}>{\hspre}l<{\hspost}@{}}%
\column{5}{@{}>{\hspre}l<{\hspost}@{}}%
\column{7}{@{}>{\hspre}l<{\hspost}@{}}%
\column{E}{@{}>{\hspre}l<{\hspost}@{}}%
\>[B]{}\mathbf{type}\;\Conid{MaybeGrid}\mathrel{=}\Conid{Maybe}\;\Conid{Grid}{}\<[E]%
\\
\>[B]{}\mathbf{type}\;\Conid{MaybeGridList}\mathrel{=}[\mskip1.5mu \Conid{Maybe}\;\Conid{Grid}\mskip1.5mu]{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\mbox{\onelinecomment  remotable declaration for all eval tasks}{}\<[E]%
\\
\>[B]{}\mathbin{\$}(\Varid{mkEvalTasks}\;[\mskip1.5mu \mathbin{\mathtt{\textquotesingle\textquotesingle}\!\!}\;\Conid{MaybeGrid},\mathbin{\mathtt{\textquotesingle\textquotesingle}\!\!}\;\Conid{MaybeGridList}\mskip1.5mu]){}\<[E]%
\\
\>[B]{}\mathbin{\$}(\Varid{mkRemotables}\;[\mskip1.5mu \mathbin{\mathtt{\textquotesingle\textquotesingle}\!\!}\;\Conid{MaybeGrid},\mathbin{\mathtt{\textquotesingle\textquotesingle}\!\!}\;\Conid{MaybeGridList}\mskip1.5mu]){}\<[E]%
\\
\>[B]{}\mathbin{\$}(\Varid{mkEvaluatables}\;[\mskip1.5mu \mathbin{\mathtt{\textquotesingle\textquotesingle}\!\!}\;\Conid{MaybeGrid},\mathbin{\mathtt{\textquotesingle\textquotesingle}\!\!}\;\Conid{MaybeGridList}\mskip1.5mu]){}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{myRemoteTable}\mathbin{::}\Conid{RemoteTable}{}\<[E]%
\\
\>[B]{}\Varid{myRemoteTable}\mathrel{=}\mathbin{\Conid{Main}.\char95 \char95 }\Varid{remoteTable}\;\Varid{initRemoteTable}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\Varid{main}\mathbin{::}\Conid{IO}\;(){}\<[E]%
\\
\>[B]{}\Varid{main}\mathrel{=}\mathbf{do}{}\<[E]%
\\
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\Varid{args}\leftarrow \Varid{getArgs}{}\<[E]%
\\[\blanklineskip]%
\>[B]{}\hsindent{3}{}\<[3]%
\>[3]{}\mathbf{case}\;\Varid{args}\;\mathbf{of}{}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}[\mskip1.5mu \text{\tt \char34 master\char34},\Varid{host},\Varid{port}\mskip1.5mu]\to \mathbf{do}{}\<[E]%
\\
\>[5]{}\hsindent{2}{}\<[7]%
\>[7]{}\Varid{conf}\leftarrow \Varid{startBackend}\;\Varid{myRemoteTable}\;\Conid{Master}\;\Varid{host}\;\Varid{port}{}\<[E]%
\\
\>[5]{}\hsindent{2}{}\<[7]%
\>[7]{}\Varid{readMVar}\;(\Varid{workers}\;\Varid{conf})\bind \Varid{print}{}\<[E]%
\\[\blanklineskip]%
\>[5]{}\hsindent{2}{}\<[7]%
\>[7]{}\Varid{grids}\leftarrow \Varid{fmap}\;\Varid{lines}\mathbin{\$}\Varid{readFile}\;\text{\tt \char34 sudoku.txt\char34}{}\<[E]%
\\[\blanklineskip]%
\>[5]{}\hsindent{2}{}\<[7]%
\>[7]{}\Varid{print}\;(\Varid{length}\;(\Varid{filter}\;\Varid{isJust}\;(\Varid{farm}\;\Varid{conf}\;\mathrm{4}\;\Varid{solve}\;\Varid{grids}))){}\<[E]%
\\
\>[3]{}\hsindent{2}{}\<[5]%
\>[5]{}[\mskip1.5mu \text{\tt \char34 slave\char34},\Varid{host},\Varid{port}\mskip1.5mu]\to \mathbf{do}{}\<[E]%
\\
\>[5]{}\hsindent{2}{}\<[7]%
\>[7]{}\Varid{startBackend}\;\Varid{myRemoteTable}\;\Conid{Slave}\;\Varid{host}\;\Varid{port}{}\<[E]%
\\
\>[5]{}\hsindent{2}{}\<[7]%
\>[7]{}\Varid{print}\;\text{\tt \char34 slave~shutdown.\char34}{}\<[E]%
\ColumnHook
\end{hscode}\resethooks
\caption{The Template Haskell version of the Sudoku benchmark program.}\label{fig:sudokuCloudHaskell}\end{figure}

\hypertarget{plots-for-the-shared-memory-benchmarks}{%
\section{Plots for the shared memory
benchmarks}\label{plots-for-the-shared-memory-benchmarks}}

\label{sec:benchmarkSharedPlots}

Following are all remaining plots for the shared memory benchmarks we
conducted for this thesis.

\vfill

\begin{figure}[h]
\centering
\includegraphics{src/img/perfSMRM32.pdf}
\caption{Parallel speedup of shared-memory Rabin--Miller test
\enquote{11213 32}.\label{fig:perfSMRM32}}
\end{figure}

\vfill

\begin{figure}[h]
\centering
\includegraphics{src/img/perfSMRM64.pdf}
\caption{Parallel speedup of shared-memory Rabin--Miller test
\enquote{11213 64}.\label{fig:perfSMRM64}}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics{src/img/overSMRM32Eden.pdf}
\caption{Mean overhead for shared-memory Rabin---Miller test
\enquote{11213 32} vs Eden CP.\label{fig:overSMRM32Eden}}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics{src/img/overSMRM32GpH.pdf}
\caption{Mean overhead for shared-memory Rabin---Miller test
\enquote{11213 32} vs GpH.\label{fig:overSMRM32GpH}}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics{src/img/overSMRM32Par.pdf}
\caption{Mean overhead for shared-memory Rabin---Miller test
\enquote{11213 32} vs \ensuremath{\Conid{Par}} monad.\label{fig:overSMRM32Par}}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics{src/img/overSMRM64Eden.pdf}
\caption{Mean overhead for shared-memory Rabin---Miller test
\enquote{11213 64} vs Eden CP.\label{fig:overSMRM64Eden}}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics{src/img/overSMRM64GpH.pdf}
\caption{Mean overhead for shared-memory Rabin---Miller test
\enquote{11213 64} vs GpH.\label{fig:overSMRM64GpH}}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics{src/img/overSMRM64Par.pdf}
\caption{Mean overhead for shared-memory Rabin---Miller test
\enquote{11213 64} vs \ensuremath{\Conid{Par}} Monad.\label{fig:overSMRM64Par}}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics{src/img/perfSMSudoku.pdf}
\caption{Parallel speedup of shared-memory Sudoku
\enquote{1000}.\label{fig:perfSMSudoku}}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics{src/img/overSMSudokuEden.pdf}
\caption{Mean overhead for shared-memory Sudoku \enquote{1000} vs Eden
CP.\label{fig:overSMSudokuEden}}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics{src/img/overSMSudokuGpH.pdf}
\caption{Mean overhead for shared-memory Sudoku \enquote{1000} vs
GpH.\label{fig:overSMSudokuGpH}}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics{src/img/overSMSudokuPar.pdf}
\caption{Mean overhead for shared-memory Sudoku \enquote{1000} vs \ensuremath{\Conid{Par}}
Monad.\label{fig:overSMSudokuPar}}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics{src/img/perfSMTorus.pdf}
\caption{Parallel speedup of shared-memory Gentleman
\enquote{512}.\label{fig:perfSMTorus}}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics{src/img/overSMTorusEden.pdf}
\caption{Mean overhead for shared-memory speedup of Gentleman
\enquote{512} vs Eden CP.\label{fig:overSMTorusEden}}
\end{figure}

\hypertarget{plots-for-the-distributed-memory-benchmarks}{%
\section{Plots for the distributed memory
benchmarks}\label{plots-for-the-distributed-memory-benchmarks}}

\label{sec:benchmarkDistPlots}

Just like for the shared memory benchmarks, we here depict all the
remaining plots for the distributed memory benchmarks.

\vfill

\begin{figure}[h]
\centering
\includegraphics{src/img/perfDistRM256.pdf}
\caption{Parallel speedup of distributed-memory Rabin---Miller test
\enquote{44497 256}.\label{fig:perfDistRM256}}
\end{figure}

\vfill

\begin{figure}[h]
\centering
\includegraphics{src/img/overDistRM256Eden.pdf}
\caption{Mean overhead for distributed-memory Rabin---Miller test
\enquote{44497 256} vs Eden.\label{fig:overDistRM256Eden}}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics{src/img/perfDistJacobi.pdf}
\caption{Parallel speedup of distributed-memory Jacobi sum test
\enquote{3217}.\label{fig:perfDistJacobi}}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics{src/img/overDistJacobiEden.pdf}
\caption{Mean overhead for distributed-memory Jacobi sum test
\enquote{3217} vs Eden.\label{fig:overDistJacobiEden}}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics{src/img/perfDistTorus.pdf}
\caption{Parallel speedup of distributed-memory Gentleman
\enquote{4096}.\label{fig:perfDistTorus}}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics{src/img/overDistTorusEden.pdf}
\caption{Mean overhead for distributed-memory Gentleman \enquote{4096}
vs Eden.\label{fig:overDistTorusEden}}
\end{figure}

\hypertarget{references}{%
\chapter*{References}\label{references}}
\addcontentsline{toc}{chapter}{References}

\hypertarget{refs}{}
\leavevmode\hypertarget{ref-Acar:2000:DLW:341800.341801}{}%
Acar, U. A., Blelloch, G. E. and Blumofe, R. D.: The data locality of
work stealing, in Proceedings of the 12Annual acm symposium on parallel
algorithms and architectures, pp. 1--12, ACM., 2000.

\leavevmode\hypertarget{ref-achten2004arrows}{}%
Achten, P., Eekelen, M. C. van, Plasmeijer, M. and Weelden, A. van:
Arrows for generic graphical editor components, Nijmegen Institute for
Computing; Information Sciences, Faculty of Science, University of
Nijmegen, The Netherlands. {[}online{]} Available from:
\url{ftp://ftp.cs.ru.nl/pub/Clean/papers/2004/achp2004-ArrowGECs.pdf},
2004.

\leavevmode\hypertarget{ref-achten2007arrow}{}%
Achten, P., Eekelen, M. van, Mol, M. de and Plasmeijer, R.: An arrow
based semantics for interactive applications, in Draft proceedings of
the symposium on trends in functional programming., 2007.

\leavevmode\hypertarget{ref-Alimarine:2005:BAA:1088348.1088357}{}%
Alimarine, A., Smetsers, S., Weelden, A. van, Eekelen, M. van and
Plasmeijer, R.: There and back again: Arrows for invertible programming,
in Proceedings of the 2005 ACM SIGPLAN workshop on Haskell, pp. 86--97,
ACM., 2005.

\leavevmode\hypertarget{ref-Aljabri:2013:DIG:2620678.2620682}{}%
Aljabri, M., Loidl, H.-W. and Trinder, P. W.: The design and
implementation of GUMSMP: A multilevel parallel haskell implementation,
in Proceedings of the 25Symposium on implementation and application of
functional languages, pp. 37:37--37:48, ACM., 2014.

\leavevmode\hypertarget{ref-Aljabri2015}{}%
Aljabri, M., Loidl, H.-W. and Trinder, P.: Balancing shared and
distributed heaps on NUMA architectures, in 15International symposium on
trends in functional programming, revised selected papers, edited by J.
Hage and J. McCarthy, pp. 1--17, Springer., 2015.

\leavevmode\hypertarget{ref-AlGo03a}{}%
Alt, M. and Gorlatch, S.: Future-Based RMI: Optimizing compositions of
remote method calls on the Grid, in Euro-par~2003, edited by H. Kosch,
L. Böszörményi, and H. Hellwagner, pp. 682--693, Springer-Verlag., 2003.

\leavevmode\hypertarget{ref-Asada:2010:ASM:1863597.1863607}{}%
Asada, K.: Arrows are strong monads, in Proceedings of the third ACM
SIGPLAN workshop on mathematically structured functional programming,
pp. 33--42, ACM, New York, NY, USA., 2010.

\leavevmode\hypertarget{ref-aswad2009low}{}%
Aswad, M., Trinder, P., Al Zain, A., Michaelson, G. and Berthold, J.:
Low pain vs no pain multi-core Haskells, in Trends in functional
programming, pp. 49--64., 2009.

\leavevmode\hypertarget{ref-ATKEY201119}{}%
Atkey, R.: What is a categorical model of arrows?, Electronic Notes in
Theoretical Computer Science, 229(5), 19--37,
doi:\href{https://doi.org/10.1016/j.entcs.2011.02.014}{10.1016/j.entcs.2011.02.014},
2011.

\leavevmode\hypertarget{ref-berthold_loidl_hammond_2016}{}%
Berthold, H.-W. A. H., Jost And Loidl: PAEAN: Portable and scalable
runtime support for parallel Haskell dialects, Journal of Functional
Programming, 26,\\
doi:\href{https://doi.org/10.1017/S0956796816000010}{10.1017/S0956796816000010},
2016.

\leavevmode\hypertarget{ref-JostThesis}{}%
Berthold, J.: Explicit and implicit parallel functional programming ---
concepts and implementation, PhD thesis, Philipps-Universität Marburg.,
2008.

\leavevmode\hypertarget{ref-Eden:PARCO05}{}%
Berthold, J. and Loogen, R.: Skeletons for recursively unfolding process
topologies, in Parallel computing: Current \& future issues of high-end
computing, parco 2005, malaga, spain, edited by G. R. Joubert, W. E.
Nagel, F. J. Peters, O. G. Plata, P. Tirado, and E. L. Zapata, Central
Institute for Applied Mathematics, Jülich, Germany., 2006.

\leavevmode\hypertarget{ref-Berthold2007a}{}%
Berthold, J. and Loogen, R.: Visualizing Parallel Functional Program
Executions: Case Studies with the Eden Trace Viewer, in ParCo '07.
parallel computing: Architectures, algorithms and applications, IOS
Press., 2007.

\leavevmode\hypertarget{ref-PADL08HMWS}{}%
Berthold, J., Dieterle, M., Loogen, R. and Priebe, S.: Hierarchical
master-worker skeletons, in Practical aspects of declarative languages
(padl'08), edited by D. S. Warren and P. Hudak, Springer-Verlag, San
Francisco (CA), USA., 2008.

\leavevmode\hypertarget{ref-arcs-dc}{}%
Berthold, J., Dieterle, M., Lobachev, O. and Loogen, R.: Distributed
Memory Programming on Many-Cores -- A Case Study Using Eden
Divide-\&-Conquer Skeletons, in Workshop on many-cores at arcs '09 -- 22
international conference on architecture of computing systems 2009,
edited by K.-E. Großpitsch, A. Henkersdorf, S. Uhrig, T. Ungerer, and J.
Hähner, pp. 47--55, VDE-Verlag., 2009a.

\leavevmode\hypertarget{ref-Berthold2009-mr}{}%
Berthold, J., Dieterle, M. and Loogen, R.: Implementing parallel Google
map-reduce in Eden, in Euro-par 2009 parallel processing, edited by H.
Sips, D. Epema, and H.-X. Lin, pp. 990--1002, Springer Berlin
Heidelberg., 2009b.

\leavevmode\hypertarget{ref-Berthold2009-fft}{}%
Berthold, J., Dieterle, M., Lobachev, O. and Loogen, R.: Parallel FFT
with Eden skeletons, in 10International conference on parallel computing
technologies, edited by V. Malyshkin, pp. 73--83, Springer., 2009c.

\leavevmode\hypertarget{ref-Bischof2002}{}%
Bischof, H. and Gorlatch, S.: Double-scan: Introducing and implementing
a new data-parallel skeleton, in Parallel processing, edited by B.
Monien and R. Feldmann, pp. 640--647, Springer., 2002.

\leavevmode\hypertarget{ref-Blumofe:1999:SMC:324133.324234}{}%
Blumofe, R. D. and Leiserson, C. E.: Scheduling multithreaded
computations by work stealing, J. ACM, 46(5), 720--748,
doi:\href{https://doi.org/10.1145/324133.324234}{10.1145/324133.324234},
1999.

\leavevmode\hypertarget{ref-botorog1996efficient}{}%
Botorog, G. H. and Kuchen, H.: Euro-Par'96 Parallel Processing, pp.
718--731, Springer-Verlag., 1996.

\leavevmode\hypertarget{ref-PArrowsPaper}{}%
Braun, M., Lobachev, O. and Trinder, P.: Arrows for parallel
computation, CoRR, abs/1801.02216 {[}online{]} Available from:
\url{http://arxiv.org/abs/1801.02216}, 2018.

\leavevmode\hypertarget{ref-brown2010ever}{}%
Brown, C. and Hammond, K.: Ever-decreasing circles: A skeleton for
parallel orbit calculations in Eden, 2010.

\leavevmode\hypertarget{ref-BUONO20102095}{}%
Buono, D., Danelutto, M. and Lametti, S.: Map, reduce and mapreduce, the
skeleton way, Procedia Computer Science, 1(1), 2095--2103,\\
doi:\href{https://doi.org/https://doi.org/10.1016/j.procs.2010.04.234}{https://doi.org/10.1016/j.procs.2010.04.234},
2010.

\leavevmode\hypertarget{ref-Chakravarty:2011:AHA:1926354.1926358}{}%
Chakravarty, M. M., Keller, G., Lee, S., McDonell, T. L. and Grover, V.:
Accelerating Haskell array codes with multicore GPUs, in Proceedings of
the 6Workshop on declarative aspects of multicore programming, pp.
3--14, ACM., 2011.

\leavevmode\hypertarget{ref-Chakravarty2007}{}%
Chakravarty, M. M. T., Leshchinskiy, R., Jones, S. L. Peyton, Keller, G.
and Marlow, S.: Data Parallel Haskell: A status report, in DAMP '07, pp.
10--18, ACM Press., 2007.

\leavevmode\hypertarget{ref-Chase:2005:DCW:1073970.1073974}{}%
Chase, D. and Lev, Y.: Dynamic circular work-stealing deque, in
Proceedings of the 17Annual acm symposium on parallelism in algorithms
and architectures, pp. 21--28, ACM., 2005.

\leavevmode\hypertarget{ref-CMCK14}{}%
Clifton-Everest, R., McDonell, T. L., Chakravarty, M. M. T. and Keller,
G.: Embedding Foreign Code, in PADL '14: The 16th international
symposium on practical aspects of declarative languages,
Springer-Verlag., 2014.

\leavevmode\hypertarget{ref-Cole1989}{}%
Cole, M. I.: Algorithmic skeletons: Structured management of parallel
computation, in Research monographs in parallel and distributed
computing, Pitman., 1989.

\leavevmode\hypertarget{ref-Czaplicki:2013:AFR:2499370.2462161}{}%
Czaplicki, E. and Chong, S.: Asynchronous functional reactive
programming for guis, SIGPLAN Not., 48(6), 411--422,
doi:\href{https://doi.org/10.1145/2499370.2462161}{10.1145/2499370.2462161},
2013.

\leavevmode\hypertarget{ref-Dagand:2009:ORD:1481861.1481870}{}%
Dagand, P.-É., Kostić, D. and Kuncak, V.: Opis: Reliable distributed
systems in OCaml, in Proceedings of the 4International workshop on types
in language design and implementation, pp. 65--78, ACM., 2009.

\leavevmode\hypertarget{ref-DANELUTTO1992205}{}%
Danelutto, M., Meglio, R. D., Orlando, S., Pelagatti, S. and Vanneschi,
M.: A methodology for the development and the support of massively
parallel programs, Future Generation Computer Systems, 8(1), 205--220,
doi:\href{https://doi.org/10.1016/0167-739X(92)90040-I}{10.1016/0167-739X(92)90040-I},
1992.

\leavevmode\hypertarget{ref-darlington1993parallel}{}%
Darlington, J., Field, A., Harrison, P., Kelly, P., Sharp, D., Wu, Q.
and While, R.: Parallel programming using skeleton functions, 146--160,
1993.

\leavevmode\hypertarget{ref-Dastgeer:2011:ASM:1984693.1984697}{}%
Dastgeer, U., Enmyren, J. and Kessler, C. W.: Auto-tuning SkePU: A
multi-backend skeleton programming framework for multi-GPU systems, in
Proceedings of the 4International workshop on multicore software
engineering, pp. 25--32, ACM., 2011.

\leavevmode\hypertarget{ref-Dean:2008:MSD:1327452.1327492}{}%
Dean, J. and Ghemawat, S.: MapReduce: Simplified data processing on
large clusters, Communications of the ACM, 51(1), 107--113,\\
doi:\href{https://doi.org/http://doi.acm.org/10.1145/1327452.1327492}{http://doi.acm.org/10.1145/1327452.1327492},
2008.

\leavevmode\hypertarget{ref-Dean:2010:MFD:1629175.1629198}{}%
Dean, J. and Ghemawat, S.: MapReduce: A flexible data processing tool,
Communications of the ACM, 53(1), 72--77,
doi:\href{https://doi.org/http://doi.acm.org/10.1145/1629175.1629198}{http://doi.acm.org/10.1145/1629175.1629198},
2010.

\leavevmode\hypertarget{ref-dieterle2010skeleton}{}%
Dieterle, M., Berthold, J. and Loogen, R.: A skeleton for distributed
work pools in Eden, in 10International symposium on functional and logic
programming, edited by M. Blume, N. Kobayashi, and G. Vidal, pp.
337--353, Springer., 2010a.

\leavevmode\hypertarget{ref-Dieterle2010}{}%
Dieterle, M., Horstmeyer, T. and Loogen, R.: Skeleton composition using
remote data, in 12International symposium on practical aspects of
declarative languages, vol. 5937, edited by M. Carro and R. Peña, pp.
73--87, Springer-Verlag., 2010b.

\leavevmode\hypertarget{ref-Dieterle2013}{}%
Dieterle, M., Horstmeyer, T., Berthold, J. and Loogen, R.: Iterating
skeletons, in 24International symposium on implementation and
application of functional languages, revised selected papers, edited by
R. Hinze, pp. 18--36, Springer., 2013.

\leavevmode\hypertarget{ref-dieterle_horstmeyer_loogen_berthold_2016}{}%
Dieterle, M., Horstmeyer, T., Loogen, R. and Berthold, J.: Skeleton
composition versus stable process systems in Eden, Journal of Functional
Programming, 26,
doi:\href{https://doi.org/10.1017/S0956796816000083}{10.1017/S0956796816000083},
2016.

\leavevmode\hypertarget{ref-Dinan:2009:SWS:1654059.1654113}{}%
Dinan, J., Larkins, D. B., Sadayappan, P., Krishnamoorthy, S. and
Nieplocha, J.: Scalable work stealing, in Proceedings of the conference
on high performance computing networking, storage and analysis, pp.
53:1--53:11, ACM., 2009.

\leavevmode\hypertarget{ref-delaEncina2011}{}%
Encina, A. de la, Hidalgo-Herrero, M., Rabanal, P. and Rubio, F.: A
parallel skeleton for genetic algorithms, in Advances in computational
intelligence: 11International work-conference on artificial neural
networks, edited by J. Cabestany, I. Rojas, and G. Joya, pp. 388--395,
Springer., 2011.

\leavevmode\hypertarget{ref-Epstein:2011:THC:2096148.2034690}{}%
Epstein, J., Black, A. P. and Peyton-Jones, S.: Towards haskell in the
cloud, SIGPLAN Not., 46(12), 118--129,
doi:\href{https://doi.org/10.1145/2096148.2034690}{10.1145/2096148.2034690},
2011.

\leavevmode\hypertarget{ref-Foltzer:2012:MPC:2398856.2364562}{}%
Foltzer, A., Kulkarni, A., Swords, R., Sasidharan, S., Jiang, E. and
Newton, R.: A meta-scheduler for the Par-monad: Composable scheduling
for the heterogeneous cloud, SIGPLAN Not., 47(9), 235--246,
doi:\href{https://doi.org/10.1145/2398856.2364562}{10.1145/2398856.2364562},
2012.

\leavevmode\hypertarget{ref-Geimer2010}{}%
Geimer, M., Wolf, F., Wylie, B. J. N., Ábrahám, E., Becker, D. and Mohr,
B.: The Scalasca performance toolset architecture, Concurrency and
Computation: Practice and Experience, 22(6), 2010.

\leavevmode\hypertarget{ref-Gentleman1978}{}%
Gentleman, W. M.: Some complexity results for matrix computations on
parallel processors, Journal of the ACM, 25(1), 112--115,
doi:\href{https://doi.org/10.1145/322047.322057}{10.1145/322047.322057},
1978.

\leavevmode\hypertarget{ref-Gorlatch1998}{}%
Gorlatch, S.: Programming with divide-and-conquer skeletons: A case
study of FFT, Journal of Supercomputing, 12(1-2), 85--97, 1998.

\leavevmode\hypertarget{ref-Gorlatch}{}%
Gorlatch, S. and Bischof, H.: A generic MPI implementation for a
data-parallel skeleton: Formal derivation and application to FFT,
Parallel Processing Letters, 8(4), 1998.

\leavevmode\hypertarget{ref-doi:10.1142ux2fS0129626403001380}{}%
Hammond, K., Berthold, J. and Loogen, R.: Automatic skeletons in
Template Haskell, Parallel Processing Letters, 13(03), 413--424,
doi:\href{https://doi.org/10.1142/S0129626403001380}{10.1142/S0129626403001380},
2003.

\leavevmode\hypertarget{ref-harris2007parallel}{}%
Harris, M., Sengupta, S. and Owens, J. D.: Parallel prefix sum (scan)
with CUDA, GPU gems, 3(39), 851--876, 2007.

\leavevmode\hypertarget{ref-Harris:2005:CMT:1065944.1065952}{}%
Harris, T., Marlow, S., Peyton Jones, S. and Herlihy, M.: Composable
memory transactions, in Proceedings of the 10ACM SIGPLAN symposium on
principles and practice of parallel programming, pp. 48--60, ACM., 2005.

\leavevmode\hypertarget{ref-Hey1990185}{}%
Hey, A. J. G.: Experiments in MIMD parallelism, Future Generation
Computer Systems, 6(3), 185--196, 1990.

\leavevmode\hypertarget{ref-Hippold2006}{}%
Hippold, J. and Rünger, G.: Task pool teams: A hybrid programming
environment for irregular algorithms on SMP clusters, Concurrency and
Computation: Practice and Experience, 18, 1575--1594, 2006.

\leavevmode\hypertarget{ref-Horstmeyer2013}{}%
Horstmeyer, T. and Loogen, R.: Graph-based communication in Eden,
Higher-Order and Symbolic Computation, 26(1), 3--28,
doi:\href{https://doi.org/10.1007/s10990-014-9101-y}{10.1007/s10990-014-9101-y},
2013.

\leavevmode\hypertarget{ref-Huang2007}{}%
Huang, L., Hudak, P. and Peterson, J.: HPorter: Using arrows to compose
parallel processes, in Practical aspects of declarative languages: 9th
international symposium, padl 2007, nice, france, january 14-15, 2007.
proceedings, edited by M. Hanus, pp. 275--289, Springer Berlin
Heidelberg, Berlin, Heidelberg., 2007.

\leavevmode\hypertarget{ref-Hudak2003}{}%
Hudak, P., Courtney, A., Nilsson, H. and Peterson, J.: Arrows, robots,
and functional reactive programming, in 4International school on
advanced functional programming, edited by J. Jeuring and S. L. Peyton
Jones, pp. 159--187, Springer., 2003.

\leavevmode\hypertarget{ref-Hughes:1990:WFP:119830.119832}{}%
Hughes, J.: Research topics in functional programming, edited by D. A.
Turner, pp. 17--42, Addison-Wesley Longman Publishing Co., Inc., Boston,
MA, USA. {[}online{]} Available from:
\url{http://dl.acm.org/citation.cfm?id=119830.119832}, 1990.

\leavevmode\hypertarget{ref-HughesArrows}{}%
Hughes, J.: Generalising monads to arrows, Science of Computer
Programming, 37(1--3), 67--111,
doi:\href{https://doi.org/10.1016/S0167-6423(99)00023-4}{10.1016/S0167-6423(99)00023-4},
2000.

\leavevmode\hypertarget{ref-Hughes2005}{}%
Hughes, J.: Programming with arrows, in 5International school on
advanced functional programming, edited by V. Vene and T. Uustalu, pp.
73--129, Springer., 2005.

\leavevmode\hypertarget{ref-jacobs_heunen_hasuo_2009}{}%
Jacobs, C. A. H., Bart And Heunen: Categorical semantics for arrows,
Journal of Functional Programming, 19(3-4), 403--438,
doi:\href{https://doi.org/10.1017/S0956796809007308}{10.1017/S0956796809007308},
2009.

\leavevmode\hypertarget{ref-janjic2013space}{}%
Janjic, V., Brown, C. M., Neunhoeffer, M., Hammond, K., Linton, S. A.
and Loidl, H.-W.: Space exploration using parallel orbits: A study in
parallel symbolic computing, Parallel Computing, 2013.

\leavevmode\hypertarget{ref-5361825}{}%
Karasawa, Y. and Iwasaki, H.: A parallel skeleton library for multi-core
clusters, in International conference on parallel processing 2009, pp.
84--91., 2009.

\leavevmode\hypertarget{ref-Keller:2010:RSP:1932681.1863582}{}%
Keller, G., Chakravarty, M. M., Leshchinskiy, R., Peyton Jones, S. and
Lippmeier, B.: Regular, shape-polymorphic, parallel arrays in haskell,
SIGPLAN Not., 45(9), 261--272,
doi:\href{https://doi.org/10.1145/1932681.1863582}{10.1145/1932681.1863582},
2010.

\leavevmode\hypertarget{ref-Kuchen2002}{}%
Kuchen, H.: A skeleton library, in Parallel processing, edited by B.
Monien and R. Feldmann, pp. 620--629, Springer., 2002.

\leavevmode\hypertarget{ref-Kuper:2014:TPE:2666356.2594312}{}%
Kuper, L., Todd, A., Tobin-Hochstadt, S. and Newton, R. R.: Taming the
parallel effect zoo: Extensible deterministic parallelism with LVish,
SIGPLAN Not., 49(6), 2--14,
doi:\href{https://doi.org/10.1145/2666356.2594312}{10.1145/2666356.2594312},
2014.

\leavevmode\hypertarget{ref-LAMMEL20081}{}%
Lämmel, R.: Google's mapreduce programming model --- revisited, Science
of Computer Programming, 70(1), 1--30,
doi:\href{https://doi.org/10.1016/j.scico.2007.07.001}{10.1016/j.scico.2007.07.001},
2008.

\leavevmode\hypertarget{ref-Lengauer1997}{}%
Lengauer, C., Gorlatch, S. and Herrmann, C.: The static parallelization
of loops and recursions, The Journal of Supercomputing, 11(4), 333--353,\\
doi:\href{https://doi.org/10.1023/A:1007904422322}{10.1023/A:1007904422322},
1997.

\leavevmode\hypertarget{ref-1648705}{}%
Li, P. and Zdancewic, S.: Encoding information flow in Haskell, in
19IEEE computer security foundations workshop, pp. 12--16., 2006.

\leavevmode\hypertarget{ref-LI20101974}{}%
Li, P. and Zdancewic, S.: Arrows for secure information flow,
Theoretical Computer Science, 411(19), 1974--1994,
doi:\href{https://doi.org/10.1016/j.tcs.2010.01.025}{10.1016/j.tcs.2010.01.025},
2010.

\leavevmode\hypertarget{ref-LINDLEY201197}{}%
Lindley, S., Wadler, P. and Yallop, J.: Idioms are oblivious, arrows are
meticulous, monads are promiscuous, Electronic Notes in Theoretical
Computer Science, 229(5), 97--117,
doi:\href{https://doi.org/10.1016/j.entcs.2011.02.018}{10.1016/j.entcs.2011.02.018},
2011.

\leavevmode\hypertarget{ref-scscp}{}%
Linton, S., Hammond, K., Konovalov, A., Al Zain, A. D., Trinder, P.,
Horn, P. and Roozemond, D.: Easy composition of symbolic computation
software: A new lingua franca for symbolic computation, in Proceedings
of the 2010 international symposium on symbolic and algebraic
computation, pp. 339--346, ACM Press., 2010.

\leavevmode\hypertarget{ref-Liu:2009:CCA:1631687.1596559}{}%
Liu, H., Cheng, E. and Hudak, P.: Causal commutative arrows and their
optimization, SIGPLAN Not., 44(9), 35--46,\\
doi:\href{https://doi.org/10.1145/1631687.1596559}{10.1145/1631687.1596559},
2009.

\leavevmode\hypertarget{ref-lobachev-phd}{}%
Lobachev, O.: Implementation and evaluation of algorithmic skeletons:
Parallelisation of computer algebra algorithms, PhD thesis,
Philipps-Universität Marburg., 2011.

\leavevmode\hypertarget{ref-Lobachev2012}{}%
Lobachev, O.: Parallel computation skeletons with premature termination
property, in 11International symposium on functional and logic
programming, edited by T. Schrijvers and P. Thiemann, pp. 197--212,
Springer., 2012.

\leavevmode\hypertarget{ref-Loogen2012}{}%
Loogen, R.: Eden -- parallel functional programming with Haskell, in
Central european functional programming school: 4th summer school, cefp
2011, budapest, hungary, june 14-24, 2011, revised selected papers,
edited by V. Zsók, Z. Horváth, and R. Plasmeijer, pp. 142--206,
Springer., 2012.

\leavevmode\hypertarget{ref-Eden:SkeletonBookChapter02}{}%
Loogen, R., Ortega-Mallén, Y., Peña, R., Priebe, S. and Rubio, F.:
Parallelism Abstractions in Eden, in Patterns and Skeletons for Parallel
and Distributed Computing, edited by F. A. Rabhi and S. Gorlatch, pp.
71--88, Springer-Verlag., 2003.

\leavevmode\hypertarget{ref-eden}{}%
Loogen, R., Ortega-Mallén, Y. and Peña-Marí, R.: Parallel Functional
Programming in Eden, Journal of Functional Programming, 15(3), 431--475,
2005.

\leavevmode\hypertarget{ref-Maier:2014:HDS:2775050.2633363}{}%
Maier, P., Stewart, R. and Trinder, P.: The HdpH DSLs for scalable
reliable computation, SIGPLAN Not., 49(12), 65--76,
doi:\href{https://doi.org/10.1145/2775050.2633363}{10.1145/2775050.2633363},
2014.

\leavevmode\hypertarget{ref-Mainland:2010:NEC:2088456.1863533}{}%
Mainland, G. and Morrisett, G.: Nikola: Embedding compiled GPU functions
in Haskell, SIGPLAN Not., 45(11), 67--78,
doi:\href{https://doi.org/10.1145/2088456.1863533}{10.1145/2088456.1863533},
2010.

\leavevmode\hypertarget{ref-marlow2013parallel}{}%
Marlow, S.: Parallel and concurrent programming in Haskell: Techniques
for multicore and multithreaded programming, ``O'Reilly Media, Inc.'',
2013.

\leavevmode\hypertarget{ref-Marlow2009}{}%
Marlow, S., Peyton Jones, S. and Singh, S.: Runtime support for
multicore Haskell, SIGPLAN Not., 44(9), 65--78, 2009.

\leavevmode\hypertarget{ref-par-monad}{}%
Marlow, S., Newton, R. and Peyton Jones, S.: A monad for deterministic
parallelism, SIGPLAN Not., 46(12), 71--82,
doi:\href{https://doi.org/10.1145/2096148.2034685}{10.1145/2096148.2034685},
2011.

\leavevmode\hypertarget{ref-McDonell:2015:TRC:2887747.2804313}{}%
McDonell, T. L., Chakravarty, M. M. T., Grover, V. and Newton, R. R.:
Type-safe runtime code generation: Accelerate to LLVM, SIGPLAN Not.,
50(12), 201--212,
doi:\href{https://doi.org/10.1145/2887747.2804313}{10.1145/2887747.2804313},
2015.

\leavevmode\hypertarget{ref-Michael:2009:IWS:1594835.1504186}{}%
Michael, M. M., Vechev, M. T. and Saraswat, V. A.: Idempotent work
stealing, SIGPLAN Not., 44(4), 45--54,
doi:\href{https://doi.org/10.1145/1594835.1504186}{10.1145/1594835.1504186},
2009.

\leavevmode\hypertarget{ref-learnyouahaskell}{}%
Michaelson, G.: Learn you a haskell for great good! A beginner's guide,
by miran lipovaca, no starch press, april 2011, isbn-10: 1593272839;
isbn-13: 978-1593272838, 376 pp., 23, 351--352, 2013.

\leavevmode\hypertarget{ref-vanNieuwpoort:2001:ELB:568014.379563}{}%
Nieuwpoort, R. V. van, Kielmann, T. and Bal, H. E.: Efficient load
balancing for wide-area divide-and-conquer applications, SIGPLAN Not.,
36(7), 34--43,\\
doi:\href{https://doi.org/10.1145/568014.379563}{10.1145/568014.379563},
2001.

\leavevmode\hypertarget{ref-Nilsson:2002:FRP:581690.581695}{}%
Nilsson, H., Courtney, A. and Peterson, J.: Functional reactive
programming, continued, in Proceedings of the 2002 ACM SIGPLAN workshop
on Haskell, pp. 51--64, ACM, New York, NY, USA., 2002.

\leavevmode\hypertarget{ref-4625841}{}%
Olivier, S. and Prins, J.: Scalable dynamic load balancing using UPC, in
37International conference on parallel processing, pp. 123--131., 2008.

\leavevmode\hypertarget{ref-Paterson:2001:NNA:507669.507664}{}%
Paterson, R.: A new notation for arrows, SIGPLAN Not., 36(10), 229--240,\\
doi:\href{https://doi.org/10.1145/507546.507664}{10.1145/507546.507664},
2001.

\leavevmode\hypertarget{ref-Eden:PPDP01}{}%
Peña, R. and Rubio, F.: Parallel Functional Programming at Two Levels of
Abstraction, in PPDP'01 --- intl.~conf.~on principles and practice of
declarative programming, pp. 187--198, Firenze, Italy, September 5--7.,
2001.

\leavevmode\hypertarget{ref-Perfumo:2008:LST:1366230.1366241}{}%
Perfumo, C., Sönmez, N., Stipic, S., Unsal, O., Cristal, A., Harris, T.
and Valero, M.: The limits of software transactional memory (STM):
Dissecting Haskell STM applications on a many-core environment, in
Proceedings of the 5Conference on computing frontiers, pp. 67--78, ACM,
Ischia, Italy., 2008.

\leavevmode\hypertarget{ref-Kuchen05}{}%
Poldner, M. and Kuchen, H.: Scalable farms, in PARCO, vol. 33, edited by
G. R. Joubert, W. E. Nagel, F. J. Peters, O. G. Plata, P. Tirado, and E.
L. Zapata, pp. 795--802, Central Institute for Applied Mathematics,
Jülich, Germany., 2005.

\leavevmode\hypertarget{ref-WPEuropar06}{}%
Priebe, S.: Dynamic task generation and transformation within a nestable
workpool skeleton, in Euro-par., 2006.

\leavevmode\hypertarget{ref-SkeletonBook}{}%
Rabhi, F. A. and Gorlatch, S., Eds.: Patterns and Skeletons for Parallel
and Distributed Computing, Springer-Verlag., 2003.

\leavevmode\hypertarget{ref-Rudolph:1991:SLB:113379.113401}{}%
Rudolph, L., Slivkin-Allalouf, M. and Upfal, E.: A simple load balancing
scheme for task allocation in parallel machines, in Proceedings of the
3Annual acm symposium on parallel algorithms and architectures, pp.
237--245, ACM., 1991.

\leavevmode\hypertarget{ref-Russo:2008:LLI:1411286.1411289}{}%
Russo, A., Claessen, K. and Hughes, J.: A library for light-weight
information-flow security in Haskell, in Proceedings of the 1ACM SIGPLAN
symposium on Haskell, pp. 13--24, ACM., 2008.

\leavevmode\hypertarget{ref-stewart_maier_trinder_2016}{}%
Stewart, P. A. T., Robert And Maier: Transparent fault tolerance for
scalable functional computation, Journal of Functional Programming, 26,\\
doi:\href{https://doi.org/10.1017/S095679681600006X}{10.1017/S095679681600006X},
2016.

\leavevmode\hypertarget{ref-obsidian-phd}{}%
Svensson, J.: Obsidian: GPU kernel programming in Haskell, PhD thesis,
Chalmers University of Technology., 2011.

\leavevmode\hypertarget{ref-Trinder1998a}{}%
Trinder, P., Hammond, K., Loidl, H.-W. and Peyton Jones, S.: Algorithm +
Strategy = Parallelism, J.~Funct.~Program., 8(1), 23--60 {[}online{]}
Available from:
\url{http://www.macs.hw.ac.uk//~dsg/gph/papers/ps/strategies.ps.gz},
1998.

\leavevmode\hypertarget{ref-Trinder1996}{}%
Trinder, P. W., Hammond, K., Mattson Jr., J. S., Partridge, A. S. and
Peyton Jones, S. L.: GUM: a Portable Parallel Implementation of Haskell,
in PLDI'96, ACM Press., 1996.

\leavevmode\hypertarget{ref-vizzotto_altenkirch_sabry_2006}{}%
Vizzotto, T. A. S., Juliana And Altenkirch: Structuring quantum effects:
Superoperators as arrows, Mathematical Structures in Computer Science,
16(3), 453--468,
doi:\href{https://doi.org/10.1017/S0960129506005287}{10.1017/S0960129506005287},
2006.

\leavevmode\hypertarget{ref-Wheeler2009}{}%
Wheeler, K. B. and Thain, D.: Visualizing massively multithreaded
applications with ThreadScope, Concurrency and Computation: Practice and
Experience, 22(1), 45--67, 2009.

\cleardoublepage

% !TEX root = ../thesis-example.tex
%
%************************************************
% Declaration
%************************************************
\pdfbookmark[0]{Declaration}{Declaration}
\chapter*{Declaration of independence}
\label{sec:declaration}
\thispagestyle{empty}

Hiermit versichere ich, {\thesisName}, dass ich die von mir vorgelegte Arbeit
\emph{\thesisTitle} selbstständig verfasst, keine anderen als die angegebenen
Quelle und Hilfsmittel verwendet und die  Arbeit  nicht  bereits  zur  Erlangung  eines
akademischen  Grades  eingereicht habe.

\smallskip

\noindent\textit{\thesisUniversityCity, \thesisDate}

\smallskip

\begin{flushright}
	\begin{minipage}{5cm}
		\rule{\textwidth}{1pt}
		\centering\thesisName
	\end{minipage}
\end{flushright}

%*****************************************
%*****************************************

\cleardoublepage

% --------------------------
% Back matter
% --------------------------

\addcontentsline{toc}{chapter}{\listfigurename}
\listoffigures
\cleardoublepage

\addcontentsline{toc}{chapter}{\listtablename}
\listoftables
\cleardoublepage

\pagestyle{empty}

\clearpage
\newpage
\mbox{}

% **************************************************
% End of Document CONTENT
% **************************************************

\end{document}
